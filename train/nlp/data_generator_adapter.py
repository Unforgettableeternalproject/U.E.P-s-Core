#!/usr/bin/env python3
"""
é©é…å™¨ï¼šå°‡æ–°çš„è³‡æ–™ç”Ÿæˆå™¨è¼¸å‡ºè½‰æ›ç‚ºæ¨™è¨»å·¥å…·æ ¼å¼
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from train.nlp.annotation_tool import AnnotationTool, TrainingExample, SegmentAnnotation

class DataGeneratorAdapter:
    """è³‡æ–™ç”Ÿæˆå™¨æ ¼å¼é©é…å™¨"""
    
    def __init__(self):
        self.annotation_tool = AnnotationTool()
    
    def convert_generated_to_annotation_format(self, generated_data: List[Dict[str, Any]]) -> List[TrainingExample]:
        """å°‡ç”Ÿæˆçš„è³‡æ–™è½‰æ›ç‚ºæ¨™è¨»å·¥å…·æ ¼å¼"""
        converted_examples = []
        
        for data in generated_data:
            # è½‰æ›åˆ†æ®µæ ¼å¼
            segments = []
            for seg in data['segments']:
                segment_annotation = SegmentAnnotation(
                    text=seg['text'],
                    label=seg['label'],
                    start=seg['start'],
                    end=seg['end'],
                    confidence=seg.get('confidence', 1.0),
                    annotator_notes=seg.get('annotator_notes', '')
                )
                segments.append(segment_annotation)
            
            # å‰µå»ºè¨“ç·´ç¯„ä¾‹
            example = TrainingExample(
                id=data['id'],
                text=data['text'],
                segments=segments,
                metadata=data['metadata']
            )
            
            converted_examples.append(example)
            
        return converted_examples
    
    def import_generated_dataset(self, jsonl_file: str) -> int:
        """å°å…¥ç”Ÿæˆçš„JSONLæ•¸æ“šé›†åˆ°æ¨™è¨»å·¥å…·"""
        imported_count = 0
        
        try:
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        try:
                            data = json.loads(line)
                            
                            # æ·»åŠ åˆ°æ¨™è¨»å·¥å…·
                            example = TrainingExample(
                                id=data['id'],
                                text=data['text'],
                                segments=[
                                    SegmentAnnotation(
                                        text=seg['text'],
                                        label=seg['label'],
                                        start=seg['start'],
                                        end=seg['end'],
                                        confidence=seg.get('confidence', 1.0),
                                        annotator_notes=seg.get('annotator_notes', '')
                                    ) for seg in data['segments']
                                ],
                                metadata=data['metadata']
                            )
                            
                            # æ·»åŠ åˆ°å·¥å…·
                            self.annotation_tool.examples.append(example)
                            imported_count += 1
                            
                        except json.JSONDecodeError as e:
                            print(f"âš ï¸  ç¬¬ {line_num} è¡ŒJSONè§£æéŒ¯èª¤: {e}")
                        except Exception as e:
                            print(f"âš ï¸  ç¬¬ {line_num} è¡Œè™•ç†éŒ¯èª¤: {e}")
            
            print(f"âœ… æˆåŠŸå°å…¥ {imported_count} å€‹è¨“ç·´ç¯„ä¾‹")
            
            # å°å‡ºç‚ºæ¨™æº–æ ¼å¼
            if imported_count > 0:
                self.annotation_tool.export_training_data(format_type="both")
                print("ğŸ“¤ å·²å°å‡ºç‚ºæ¨™æº–è¨“ç·´æ ¼å¼")
            
            return imported_count
            
        except FileNotFoundError:
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {jsonl_file}")
            return 0
        except Exception as e:
            print(f"âŒ å°å…¥å¤±æ•—: {e}")
            return 0
    
    def validate_generated_data(self, jsonl_file: str) -> Dict[str, Any]:
        """é©—è­‰ç”Ÿæˆçš„æ•¸æ“šè³ªé‡"""
        stats = {
            'total_examples': 0,
            'valid_examples': 0,
            'invalid_examples': 0,
            'label_distribution': {},
            'complexity_distribution': {'single': 0, 'double': 0, 'triple': 0},
            'errors': []
        }
        
        try:
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        stats['total_examples'] += 1
                        
                        try:
                            data = json.loads(line)
                            
                            # é©—è­‰å¿…è¦æ¬„ä½
                            required_fields = ['id', 'text', 'tokens', 'bio_labels', 'segments']
                            missing_fields = [field for field in required_fields if field not in data]
                            
                            if missing_fields:
                                stats['invalid_examples'] += 1
                                stats['errors'].append(f"ç¬¬ {line_num} è¡Œç¼ºå°‘æ¬„ä½: {missing_fields}")
                                continue
                            
                            # é©—è­‰BIOæ¨™ç±¤ä¸€è‡´æ€§
                            if len(data['tokens']) != len(data['bio_labels']):
                                stats['invalid_examples'] += 1
                                stats['errors'].append(f"ç¬¬ {line_num} è¡Œ: tokenså’Œbio_labelsé•·åº¦ä¸ä¸€è‡´")
                                continue
                            
                            # é©—è­‰åˆ†æ®µ
                            text = data['text']
                            segments = data['segments']
                            segment_valid = True
                            
                            for seg in segments:
                                start, end = seg['start'], seg['end']
                                if start >= end or end > len(text):
                                    stats['invalid_examples'] += 1
                                    stats['errors'].append(f"ç¬¬ {line_num} è¡Œ: ç„¡æ•ˆçš„åˆ†æ®µä½ç½® [{start}, {end}]")
                                    segment_valid = False
                                    break
                                
                                actual_text = text[start:end]
                                if actual_text != seg['text']:
                                    stats['invalid_examples'] += 1
                                    stats['errors'].append(f"ç¬¬ {line_num} è¡Œ: åˆ†æ®µæ–‡æœ¬ä¸åŒ¹é…")
                                    segment_valid = False
                                    break
                            
                            if not segment_valid:
                                continue
                            
                            # çµ±è¨ˆ
                            stats['valid_examples'] += 1
                            
                            # æ¨™ç±¤åˆ†ä½ˆ
                            for seg in segments:
                                label = seg['label']
                                stats['label_distribution'][label] = stats['label_distribution'].get(label, 0) + 1
                            
                            # è¤‡é›œåº¦åˆ†ä½ˆ
                            segment_count = len(segments)
                            if segment_count == 1:
                                stats['complexity_distribution']['single'] += 1
                            elif segment_count == 2:
                                stats['complexity_distribution']['double'] += 1
                            else:
                                stats['complexity_distribution']['triple'] += 1
                                
                        except json.JSONDecodeError as e:
                            stats['invalid_examples'] += 1
                            stats['errors'].append(f"ç¬¬ {line_num} è¡ŒJSONè§£æéŒ¯èª¤: {e}")
                        except Exception as e:
                            stats['invalid_examples'] += 1
                            stats['errors'].append(f"ç¬¬ {line_num} è¡Œè™•ç†éŒ¯èª¤: {e}")
        
        except Exception as e:
            stats['errors'].append(f"æª”æ¡ˆè®€å–éŒ¯èª¤: {e}")
        
        return stats

def test_adapter():
    """æ¸¬è©¦é©é…å™¨åŠŸèƒ½"""
    adapter = DataGeneratorAdapter()
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç”Ÿæˆçš„æ•¸æ“šæª”æ¡ˆ
    jsonl_file = "nlp_training_data.jsonl"
    
    if not Path(jsonl_file).exists():
        print("ğŸ” æœªæ‰¾åˆ°ç”Ÿæˆçš„æ•¸æ“šæª”æ¡ˆï¼Œå…ˆé‹è¡Œè³‡æ–™ç”Ÿæˆå™¨...")
        
        # é‹è¡Œè³‡æ–™ç”Ÿæˆå™¨
        try:
            exec(open('training_data_generator.py').read())
            print("âœ… è³‡æ–™ç”Ÿæˆå™¨é‹è¡Œå®Œæˆ")
        except Exception as e:
            print(f"âŒ è³‡æ–™ç”Ÿæˆå™¨é‹è¡Œå¤±æ•—: {e}")
            return
    
    # é©—è­‰æ•¸æ“š
    print("ğŸ” é©—è­‰ç”Ÿæˆçš„æ•¸æ“š...")
    stats = adapter.validate_generated_data(jsonl_file)
    
    print(f"\nğŸ“Š æ•¸æ“šé©—è­‰çµæœ:")
    print(f"   ç¸½ç¯„ä¾‹æ•¸: {stats['total_examples']}")
    print(f"   æœ‰æ•ˆç¯„ä¾‹: {stats['valid_examples']}")
    print(f"   ç„¡æ•ˆç¯„ä¾‹: {stats['invalid_examples']}")
    print(f"   æ¨™ç±¤åˆ†ä½ˆ: {stats['label_distribution']}")
    print(f"   è¤‡é›œåº¦åˆ†ä½ˆ: {stats['complexity_distribution']}")
    
    if stats['errors']:
        print(f"\nâš ï¸  ç™¼ç¾ {len(stats['errors'])} å€‹éŒ¯èª¤:")
        for i, error in enumerate(stats['errors'][:5], 1):  # åªé¡¯ç¤ºå‰5å€‹
            print(f"   {i}. {error}")
        if len(stats['errors']) > 5:
            print(f"   ... é‚„æœ‰ {len(stats['errors']) - 5} å€‹éŒ¯èª¤")
    
    # å°å…¥æ•¸æ“š
    if stats['valid_examples'] > 0:
        print(f"\nğŸ“¥ å°å…¥æœ‰æ•ˆæ•¸æ“šåˆ°æ¨™è¨»å·¥å…·...")
        imported_count = adapter.import_generated_dataset(jsonl_file)
        
        if imported_count > 0:
            print(f"ğŸ‰ æˆåŠŸè™•ç† {imported_count} å€‹è¨“ç·´ç¯„ä¾‹")
            print("ğŸ“ æ•¸æ“šå·²å°å‡ºç‚ºæ¨™æº–æ ¼å¼åˆ° train/nlp/data/annotated/")
        else:
            print("âŒ å°å…¥å¤±æ•—")
    else:
        print("âŒ æ²’æœ‰æœ‰æ•ˆæ•¸æ“šå¯å°å…¥")

if __name__ == "__main__":
    test_adapter()
