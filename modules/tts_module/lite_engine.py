"""
IndexTTS2 Lite Engine - ç²¾ç°¡æ¨è«–å¼•æ“

ç²¾ç°¡åŠŸèƒ½:
- âœ… åŠ è¼‰é æå–çš„è§’è‰²ç‰¹å¾µ
- âœ… æƒ…æ„Ÿå‘é‡æ§åˆ¶ (8ç¶­)
- âœ… æ–‡æœ¬è½‰èªéŸ³åˆæˆ
- âŒ ç§»é™¤: ç‰¹å¾µæå–ã€æƒ…æ„Ÿè­˜åˆ¥ã€BPEåˆ†è©ç­‰

ä½¿ç”¨æ–¹å¼:
    from indextts.lite_engine import IndexTTSLite
    
    # åˆå§‹åŒ–
    engine = IndexTTSLite(
        cfg_path="checkpoints/config.yaml",
        model_dir="checkpoints"
    )
    
    # åŠ è¼‰è§’è‰²
    engine.load_character("characters/uep-1.pt")
    
    # åˆæˆèªéŸ³
    engine.synthesize(
        text="Hello world",
        output_path="output.wav",
        emotion_vector=[0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # Happy
        max_emotion_strength=0.3  # ä¿ç•™ 70% åŸå§‹è²éŸ³
    )
"""

import os
import torch
import warnings
from pathlib import Path
from typing import Optional, List, Union

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

from omegaconf import OmegaConf
from huggingface_hub import hf_hub_download
import safetensors

# å°å…¥ç³»çµ±æ—¥èªŒå·¥å…·
try:
    from utils.debug_helper import debug_log, info_log, error_log
    _HAS_SYSTEM_LOG = True
except ImportError:
    _HAS_SYSTEM_LOG = False
    # Fallback: ä½¿ç”¨ print
    def debug_log(level, msg): print(msg)
    def info_log(msg): print(msg)
    def error_log(msg): print(f"ERROR: {msg}")

# ä½¿ç”¨çµ•å°å°å…¥ (ç•¶ä½œç‚ºè…³æœ¬é‹è¡Œæ™‚)
import sys
if __name__ == "__main__":
    sys.path.insert(0, os.path.dirname(__file__))

try:
    # å˜—è©¦ç›¸å°å°å…¥
    from .gpt.model_v2 import UnifiedVoice
    from .tts_utils.maskgct_utils import build_semantic_model, build_semantic_codec
    from .tts_utils.front import TextNormalizer, TextTokenizer
    from .s2mel.modules.commons import load_checkpoint2, MyModel
    from .s2mel.modules.bigvgan import bigvgan
except ImportError:
    # å›é€€åˆ°çµ•å°å°å…¥
    from gpt.model_v2 import UnifiedVoice
    from tts_utils.maskgct_utils import build_semantic_model, build_semantic_codec
    from tts_utils.front import TextNormalizer, TextTokenizer
    from s2mel.modules.commons import load_checkpoint2, MyModel
    from s2mel.modules.bigvgan import bigvgan


class IndexTTSLite:
    """ç²¾ç°¡ç‰ˆ IndexTTS2 æ¨è«–å¼•æ“"""
    
    def __init__(
        self,
        cfg_path: str = "checkpoints/config.yaml",
        model_dir: str = "checkpoints",
        use_fp16: bool = True,
        device: Optional[str] = None,
        use_cuda_kernel: bool = False
    ):
        """
        åˆå§‹åŒ–ç²¾ç°¡æ¨è«–å¼•æ“
        
        Args:
            cfg_path: é…ç½®æ–‡ä»¶è·¯å¾‘
            model_dir: æ¨¡å‹ç›®éŒ„
            use_fp16: æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦
            device: è¨­å‚™ (None=è‡ªå‹•æª¢æ¸¬)
            use_cuda_kernel: æ˜¯å¦ä½¿ç”¨ CUDA kernel (éœ€è¦ CUDA Toolkit)
        """
        self.model_dir = model_dir
        self.use_fp16 = use_fp16
        self.use_cuda_kernel = use_cuda_kernel
        
        # è¨­ç½®è¨­å‚™
        if device is None:
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                self.device = torch.device("mps")
            else:
                self.device = torch.device("cpu")
        else:
            self.device = torch.device(device)
        
        # åŠ è¼‰é…ç½®
        self.cfg = OmegaConf.load(cfg_path)
        
        # ç•¶å‰åŠ è¼‰çš„è§’è‰²ç‰¹å¾µ
        self.current_character = None
        self.character_features = None
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        info_log("âœ… IndexTTS Lite Engine åˆå§‹åŒ–å®Œæˆ!")
    
    def _init_models(self):
        """åˆå§‹åŒ–å¿…è¦çš„æ¨¡å‹çµ„ä»¶"""
        debug_log(2, "ğŸš€ åˆå§‹åŒ–æ¨¡å‹...")
        
        # 1. GPT æ¨¡å‹
        debug_log(2, "   [1/4] åŠ è¼‰ GPT æ¨¡å‹...")
        gpt_path = os.path.join(self.model_dir, self.cfg.gpt_checkpoint)
        # ä½¿ç”¨å­—å…¸å±•é–‹ä¾†å‚³éæ‰€æœ‰ GPT é…ç½®åƒæ•¸(åŒ…æ‹¬åµŒå¥—çš„ condition_module ç­‰)
        self.gpt = UnifiedVoice(**self.cfg.gpt).to(self.device)
        
        self.gpt.load_state_dict(torch.load(gpt_path, map_location=self.device, weights_only=True))
        
        if self.use_fp16:
            self.gpt.eval().half()
        else:
            self.gpt.eval()
        
        self.gpt.post_init_gpt2_config(use_deepspeed=False, kv_cache=True, half=self.use_fp16)
        debug_log(3, f"      âœ“ GPT åŠ è¼‰å®Œæˆ: {gpt_path}")
        
        # 2. Semantic Codec (ç”¨æ–¼ GPT è¼¸å‡ºè§£ç¢¼)
        debug_log(2, "   [2/4] åŠ è¼‰ Semantic Codec...")
        semantic_codec = build_semantic_codec(self.cfg.semantic_codec)
        semantic_code_ckpt = hf_hub_download("amphion/MaskGCT", filename="semantic_codec/model.safetensors")
        safetensors.torch.load_model(semantic_codec, semantic_code_ckpt)
        self.semantic_codec = semantic_codec.to(self.device)
        self.semantic_codec.eval()
        debug_log(3, f"      âœ“ Semantic Codec åŠ è¼‰å®Œæˆ")
        
        # 3. S2Mel æ¨¡å‹
        debug_log(2, "   [3/4] åŠ è¼‰ S2Mel æ¨¡å‹...")
        s2mel_path = os.path.join(self.model_dir, self.cfg.s2mel_checkpoint)
        s2mel = MyModel(**self.cfg.s2mel, use_gpt_latent=True)
        s2mel, _, _, _ = load_checkpoint2(s2mel, None, s2mel_path)
        self.s2mel = s2mel.to(self.device)
        # åˆå§‹åŒ– GPT-Fast cache (åƒè€ƒ infer_v2.py line 139)
        self.s2mel.models['cfm'].estimator.setup_caches(max_batch_size=1, max_seq_length=8192)
        self.s2mel.eval()
        debug_log(3, f"      âœ“ S2Mel åŠ è¼‰å®Œæˆ: {s2mel_path}")
        
        # 4. BigVGAN Vocoder
        debug_log(2, "   [4/4] åŠ è¼‰ BigVGAN Vocoder...")
        bigvgan_name = self.cfg.vocoder.name
        self.bigvgan = bigvgan.BigVGAN.from_pretrained(bigvgan_name, use_cuda_kernel=self.use_cuda_kernel)
        self.bigvgan = self.bigvgan.to(self.device)
        self.bigvgan.remove_weight_norm()
        self.bigvgan.eval()
        debug_log(3, f"      âœ“ BigVGAN åŠ è¼‰å®Œæˆ: {bigvgan_name}")
        
        # 5. æƒ…æ„Ÿå’Œèªªè©±äººçŸ©é™£ (ç”¨æ–¼ emo_vector æ˜ å°„)
        debug_log(2, "   [5/6] åŠ è¼‰æƒ…æ„Ÿå’Œèªªè©±äººçŸ©é™£...")
        emo_matrix = torch.load(os.path.join(self.model_dir, self.cfg.emo_matrix), map_location=self.device, weights_only=True)
        spk_matrix = torch.load(os.path.join(self.model_dir, self.cfg.spk_matrix), map_location=self.device, weights_only=True)
        
        self.emo_matrix = emo_matrix.to(self.device)
        self.spk_matrix = spk_matrix.to(self.device)
        self.emo_num = list(self.cfg.emo_num)
        
        # Split çŸ©é™£ (ç”¨æ–¼æŒ‰æƒ…æ„Ÿé¡åˆ¥ç´¢å¼•)
        self.emo_matrix = torch.split(self.emo_matrix, self.emo_num)
        self.spk_matrix = torch.split(self.spk_matrix, self.emo_num)
        debug_log(3, f"      âœ“ çŸ©é™£åŠ è¼‰å®Œæˆ: {self.cfg.emo_matrix}, {self.cfg.spk_matrix}")
        
        # 6. æ–‡æœ¬æ¨™æº–åŒ–å™¨å’Œ BPE Tokenizer (åƒè€ƒ infer_v2.py line 159-162)
        debug_log(2, "   [6/6] åŠ è¼‰æ–‡æœ¬è™•ç†...")
        bpe_path = os.path.join(self.model_dir, self.cfg.dataset["bpe_model"])
        self.text_normalizer = TextNormalizer()
        self.text_normalizer.load()
        self.tokenizer = TextTokenizer(bpe_path, self.text_normalizer)
        debug_log(3, f"      âœ“ æ–‡æœ¬è™•ç†å™¨å’Œ BPE æ¨¡å‹åŠ è¼‰å®Œæˆ: {bpe_path}")
    
    def load_character(self, character_path: Union[str, Path], verbose: bool = True):
        """
        åŠ è¼‰é æå–çš„è§’è‰²ç‰¹å¾µ
        
        Args:
            character_path: è§’è‰²ç‰¹å¾µæ–‡ä»¶è·¯å¾‘ (.pt)
            verbose: æ˜¯å¦æ‰“å°è©³ç´°ä¿¡æ¯
            
        Returns:
            bool: æ˜¯å¦åŠ è¼‰æˆåŠŸ
        """
        character_path = Path(character_path)
        
        if not character_path.exists():
            raise FileNotFoundError(f"è§’è‰²æ–‡ä»¶ä¸å­˜åœ¨: {character_path}")
        
        if verbose:
            debug_log(2, f"ğŸ“‚ åŠ è¼‰è§’è‰²: {character_path.name}")
        
        try:
            # åŠ è¼‰ç‰¹å¾µ
            features = torch.load(character_path, map_location=self.device, weights_only=False)
            
            # é©—è­‰å¿…è¦å­—æ®µ
            required_fields = ['spk_cond_emb', 'style', 'prompt_condition', 'ref_mel']
            missing_fields = [f for f in required_fields if f not in features]
            
            if missing_fields:
                raise ValueError(f"è§’è‰²æ–‡ä»¶ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
            
            # å¦‚æœä½¿ç”¨ fp16,å°‡æµ®é»æ•¸ç‰¹å¾µè½‰æ›ç‚º half
            if self.use_fp16:
                for key in ['spk_cond_emb', 'style', 'prompt_condition', 'ref_mel']:
                    if key in features and features[key].dtype in [torch.float32, torch.float64]:
                        features[key] = features[key].half()
            
            # æª¢æŸ¥æƒ…æ„Ÿç´¢å¼• (emo_indices)
            if 'emo_indices' not in features:
                if verbose:
                    debug_log(2, "   âš ï¸  è­¦å‘Š: æ­¤è§’è‰²æ–‡ä»¶æ²’æœ‰ emo_indices,å°‡ä½¿ç”¨å…¨é›¶å‘é‡")
                features['emo_indices'] = torch.zeros(8, dtype=torch.long).to(self.device)
            elif isinstance(features['emo_indices'], list):
                # å¦‚æœæ˜¯ list,è½‰æ›ç‚º tensor
                features['emo_indices'] = torch.tensor(features['emo_indices'], dtype=torch.long).to(self.device)
            
            self.character_features = features
            self.current_character = character_path.stem
            
            if verbose:
                info_log(f"   âœ“ è§’è‰² '{self.current_character}' åŠ è¼‰æˆåŠŸ!")
                
                # åŠ è¼‰ metadata (å¦‚æœå­˜åœ¨)
                metadata_path = character_path.with_suffix('.pt_metadata.json')
                if metadata_path.exists():
                    import json
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    debug_log(3, f"   ğŸ“‹ æå–æ™‚é–“: {metadata.get('extraction_time', 'N/A')}")
                    debug_log(3, f"   ğŸ“‹ éŸ³é »é•·åº¦: {metadata.get('audio_duration', 'N/A')}")
                    if 'emo_indices' in metadata:
                        debug_log(3, f"   ğŸ“‹ æƒ…æ„Ÿç´¢å¼•: {metadata['emo_indices']}")
            
            return True
            
        except Exception as e:
            error_log(f"âŒ åŠ è¼‰è§’è‰²å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def normalize_emotion_vector(
        self,
        emotion_vector: List[float],
        max_strength: float = 0.3,
        verbose: bool = False
    ) -> List[float]:
        """
        æ­¸ä¸€åŒ–æƒ…æ„Ÿå‘é‡,ç¢ºä¿ä¸æœƒè¦†è“‹åŸå§‹è²éŸ³ç‰¹å¾µ
        
        Args:
            emotion_vector: 8ç¶­æƒ…æ„Ÿå‘é‡ [happy, angry, sad, afraid, disgusted, melancholic, surprised, calm]
            max_strength: æœ€å¤§æƒ…æ„Ÿå¼·åº¦ (0-1),æ¨è–¦ 0.3 ä¿ç•™ 70% åŸå§‹è²éŸ³
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
            
        Returns:
            æ­¸ä¸€åŒ–å¾Œçš„æƒ…æ„Ÿå‘é‡
        """
        current_sum = sum(emotion_vector)
        
        if current_sum == 0 or current_sum <= max_strength:
            return emotion_vector
        
        # æ¯”ä¾‹å£“ç¸®
        scale_factor = max_strength / current_sum
        normalized = [v * scale_factor for v in emotion_vector]
        
        if verbose:
            debug_log(3, f"   ğŸ“Š æƒ…æ„Ÿæ­¸ä¸€åŒ–: {current_sum:.2f} â†’ {sum(normalized):.2f}")
            debug_log(3, f"      åŸå§‹è²éŸ³ä¿ç•™: {(1 - sum(normalized)) * 100:.0f}%")
        
        return normalized
    
    def synthesize_direct(
        self,
        text: str,
        output_path: str,
        emotion_vector: Optional[List[float]] = None,
        max_emotion_strength: float = 0.3,
        language: str = 'en',
        # GPT å„ªåŒ–åƒæ•¸
        num_beams: int = 1,
        do_sample: bool = True,
        temperature: float = 0.6,
        top_p: float = 0.9,
        top_k: int = 20,
        verbose: bool = True
    ) -> bool:
        """
        ã€å·²å»¢æ£„ã€‘ç›´æ¥æ‰‹å‹•èª¿ç”¨å„å€‹æ¨¡å‹æ­¥é©Ÿçš„åˆæˆæ–¹æ³•
        
        æ­¤æ–¹æ³•å­˜åœ¨å•é¡Œ:
        1. æ‰‹å‹•å¯¦ç¾å®¹æ˜“å‡ºéŒ¯
        2. å¯èƒ½è§¸ç™¼ CUDA kernel éŒ¯èª¤
        3. å¯èƒ½å°è‡´ç„¡é™å¾ªç’°
        
        è«‹ä½¿ç”¨ synthesize() æ–¹æ³•ä»£æ›¿!
        """
        raise DeprecationWarning("è«‹ä½¿ç”¨ synthesize() æ–¹æ³•,å®ƒä½¿ç”¨ IndexTTS2 çš„å…§éƒ¨é‚è¼¯æ›´ç©©å®š")
    
    def synthesize(
        self,
        text: str,
        output_path: str,
        emotion_vector: Optional[List[float]] = None,
        max_emotion_strength: float = 0.5,
        language: str = 'en',
        # GPT å„ªåŒ–åƒæ•¸
        num_beams: int = 1,
        do_sample: bool = True,
        temperature: float = 0.6,
        top_p: float = 0.9,
        top_k: int = 20,
        verbose: bool = True
    ) -> bool:
        """
        åˆæˆèªéŸ³ (ç¨ç«‹å¼•æ“å¯¦ç¾,åƒè€ƒ infer_v2.py é‚è¼¯)
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            output_path: è¼¸å‡ºéŸ³é »è·¯å¾‘
            emotion_vector: 8ç¶­æƒ…æ„Ÿå‘é‡ [happy, angry, sad, afraid, disgusted, melancholic, surprised, calm]
            max_emotion_strength: æœ€å¤§æƒ…æ„Ÿå¼·åº¦ (0.5=ä¿ç•™50%åŸè²)
            language: èªè¨€ ('en' æˆ– 'zh')
            num_beams: GPT beam search å¤§å° (1=æœ€å¿«)
            do_sample: æ˜¯å¦æ¡æ¨£
            temperature: æ¡æ¨£æº«åº¦
            top_p: Nucleus æ¡æ¨£
            top_k: Top-K æ¡æ¨£
            verbose: æ˜¯å¦æ‰“å°è©³ç´°ä¿¡æ¯
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if self.character_features is None:
            raise RuntimeError("è«‹å…ˆä½¿ç”¨ load_character() åŠ è¼‰è§’è‰²!")
        
        if verbose:
            debug_log(2, f"ğŸ™ï¸  åˆæˆèªéŸ³...")
            debug_log(3, f"   è§’è‰²: {self.current_character}")
            debug_log(3, f"   æ–‡æœ¬: {text}")
            debug_log(3, f"   æ–‡æœ¬é•·åº¦: {len(text)} å­—ç¬¦")
        
        # 1. æƒ…æ„Ÿå‘é‡è™•ç†
        if emotion_vector is None:
            emotion_vector = [0.0] * 8  # ä¸­æ€§
        
        if len(emotion_vector) != 8:
            raise ValueError("æƒ…æ„Ÿå‘é‡å¿…é ˆæ˜¯ 8 ç¶­")
        
        # æ­¸ä¸€åŒ–æƒ…æ„Ÿå‘é‡
        normalized_emotion = self.normalize_emotion_vector(emotion_vector, max_emotion_strength, verbose)
        
        try:
            # 2. æ–‡æœ¬è™•ç† (åƒè€ƒ infer_v2.py line 487-523)
            if verbose:
                debug_log(3, "   [1/4] æ–‡æœ¬è™•ç†...")
            
            # ä½¿ç”¨ BPE tokenizer é€²è¡Œæ­£ç¢ºçš„ tokenization
            text_tokens_list = self.tokenizer.tokenize(text)
            # ç°¡åŒ–ç‰ˆ:åªå–ç¬¬ä¸€å€‹ segment (å®Œæ•´ç‰ˆæ‡‰è©²å¾ªç’°è™•ç†æ‰€æœ‰ segments)
            segments = self.tokenizer.split_segments(text_tokens_list, max_text_tokens_per_segment=200)
            if len(segments) > 1 and verbose:
                debug_log(2, f"      âš ï¸  è­¦å‘Š: æ–‡æœ¬è¢«åˆ†å‰²ç‚º {len(segments)} æ®µ,åªè™•ç†ç¬¬ä¸€æ®µ")
            
            # è½‰æ›ç‚º token IDs
            text_tokens = self.tokenizer.convert_tokens_to_ids(segments[0])
            text_tokens = torch.tensor(text_tokens, dtype=torch.int32, device=self.device).unsqueeze(0)
            
            # 3. è™•ç†æƒ…æ„Ÿå‘é‡æ˜ å°„ (åƒè€ƒ infer_v2.py line 456-462)
            # ä½¿ç”¨é è¨ˆç®—çš„ emo_indices æˆ–å‹•æ…‹è¨ˆç®—
            if 'emo_indices' in self.character_features:
                # ä½¿ç”¨é è¨ˆç®—çš„ç´¢å¼•
                emo_indices = self.character_features['emo_indices']
            else:
                # å‹•æ…‹è¨ˆç®—æœ€ç›¸ä¼¼çš„ç´¢å¼• (find_most_similar_cosine)
                style = self.character_features['style']
                emo_indices = []
                for spk_mat in self.spk_matrix:
                    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
                    query_norm = torch.nn.functional.normalize(style.squeeze(0), p=2, dim=-1)
                    matrix_norm = torch.nn.functional.normalize(spk_mat, p=2, dim=-1)
                    similarities = torch.matmul(query_norm, matrix_norm.T)
                    most_similar_idx = torch.argmax(similarities).item()
                    emo_indices.append(most_similar_idx)
            
            # å¾ emo_matrix ä¸­ç²å–å°æ‡‰çš„æƒ…æ„Ÿå‘é‡ä¸¦åŠ æ¬Š
            weight_vector = torch.tensor(normalized_emotion, device=self.device)
            emo_matrix_selected = [emo_mat[idx].unsqueeze(0) for idx, emo_mat in zip(emo_indices, self.emo_matrix)]
            emo_matrix_cat = torch.cat(emo_matrix_selected, 0)  # [8, hidden_dim]
            emovec_mat = weight_vector.unsqueeze(1) * emo_matrix_cat  # [8, hidden_dim]
            emovec_mat = torch.sum(emovec_mat, 0).unsqueeze(0)  # [1, hidden_dim]
            
            # ç¢ºä¿ dtype èˆ‡æ¨¡å‹ä¸€è‡´
            if self.use_fp16:
                emovec_mat = emovec_mat.half()
            
            # 4. GPT ç”Ÿæˆèªç¾© tokens
            if verbose:
                debug_log(3, "   [2/4] GPT ç”Ÿæˆä¸­...")
            
            with torch.no_grad():
                # ä½¿ç”¨ autocast è™•ç† FP16 (åƒè€ƒ infer_v2.py line 534)
                dtype = torch.float16 if self.use_fp16 else None
                with torch.amp.autocast(self.device.type, enabled=dtype is not None, dtype=dtype):
                    # æº–å‚™ conditioning (åƒè€ƒ infer_v2.py line 419-452)
                    spk_cond_emb = self.character_features['spk_cond_emb']
                    # ç°¡åŒ–ç‰ˆ:ä½¿ç”¨ spk_cond_emb ä½œç‚º emo_cond_emb (å®Œæ•´ç‰ˆæ‡‰å¾æƒ…æ„Ÿåƒè€ƒéŸ³é »æå–)
                    emo_cond_emb = spk_cond_emb
                    
                    # Merge emovec (åƒè€ƒ infer_v2.py line 535-544)
                    emovec = self.gpt.merge_emovec(
                        spk_cond_emb,
                        emo_cond_emb,
                        torch.tensor([spk_cond_emb.shape[-1]], device=self.device),
                        torch.tensor([emo_cond_emb.shape[-1]], device=self.device),
                        alpha=1.0  # emo_alpha
                    )
                    
                    # å¦‚æœæœ‰æƒ…æ„Ÿå‘é‡,æ··åˆ emovec_mat
                    if emovec_mat is not None:
                        weight_sum = torch.sum(torch.tensor(normalized_emotion, device=self.device))
                        emovec = emovec_mat + (1 - weight_sum) * emovec
                    
                    # inference_speech è¿”å› (codes, speech_conditioning_latent)
                    codes, speech_conditioning_latent = self.gpt.inference_speech(
                        spk_cond_emb,
                        text_tokens,
                        emo_cond_emb,  # ä½¿ç”¨ emo_cond_emb (ä¸æ˜¯ spk_cond_emb!)
                        cond_lengths=torch.tensor([spk_cond_emb.shape[-1]], device=self.device),
                        emo_cond_lengths=torch.tensor([emo_cond_emb.shape[-1]], device=self.device),
                        emo_vec=emovec,  # ä½¿ç”¨mergeå¾Œçš„ emovec
                        num_return_sequences=1,
                        num_beams=num_beams,
                        do_sample=do_sample,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=top_k,
                        length_penalty=1.0,
                        repetition_penalty=1.2,
                        max_generate_length=self.cfg.gpt.max_mel_tokens
                    )
                    
                    # GPT forward ç²å– latent
                    use_speed = torch.zeros(1, device=self.device).long()
                    latent = self.gpt(
                        speech_conditioning_latent,
                        text_tokens,
                        torch.tensor([text_tokens.shape[-1]], device=self.device),
                        codes,
                        torch.tensor([codes.shape[-1]], device=self.device),
                        spk_cond_emb,
                        cond_mel_lengths=torch.tensor([spk_cond_emb.shape[-1]], device=self.device),
                        emo_cond_mel_lengths=torch.tensor([spk_cond_emb.shape[-1]], device=self.device),
                        emo_vec=emovec_mat,
                        use_speed=use_speed
                    )
            
            # æ‰¾åˆ°å¯¦éš›çš„ code é•·åº¦ (åƒè€ƒ infer_v2.py line 583-591)
            stop_mel_token = self.cfg.gpt.stop_mel_token
            code_lens = []
            for code in codes:
                if stop_mel_token not in code:
                    code_len = len(code)
                else:
                    len_ = (code == stop_mel_token).nonzero(as_tuple=False)[0] + 1
                    code_len = len_ - 1
                code_lens.append(code_len)
            
            codes = codes[:, :code_len]  # è£å‰ªåˆ°å¯¦éš›é•·åº¦
            code_lens = torch.LongTensor(code_lens).to(self.device)
            
            # 5. S2Mel ç”Ÿæˆ Mel é »è­œ
            if verbose:
                debug_log(3, "   [3/4] S2Mel ç”Ÿæˆä¸­...")
            
            with torch.no_grad():
                # ä½¿ç”¨ dtype=None çš„ autocast (åƒè€ƒ infer_v2.py line 617)
                dtype = None
                with torch.amp.autocast(self.device.type, enabled=dtype is not None, dtype=dtype):
                    latent = self.s2mel.models['gpt_layer'](latent)
                    S_infer = self.semantic_codec.quantizer.vq2emb(codes.unsqueeze(1))
                    S_infer = S_infer.transpose(1, 2)
                    
                    # è£å‰ª latent ä»¥åŒ¹é… S_infer çš„é•·åº¦
                    if latent.shape[1] > S_infer.shape[1]:
                        latent = latent[:, :S_infer.shape[1], :]
                    
                    S_infer = S_infer + latent
                    target_lengths = (code_lens * 1.72).long()
                
                cond = self.s2mel.models['length_regulator'](
                    S_infer,
                    ylens=target_lengths,
                    n_quantizers=3,
                    f0=None
                )[0]
                
                cat_condition = torch.cat([self.character_features['prompt_condition'], cond], dim=1)
                
                # CFM inference
                vc_target = self.s2mel.models['cfm'].inference(
                    cat_condition,
                    torch.LongTensor([cat_condition.size(1)]).to(self.device),
                    self.character_features['ref_mel'],
                    self.character_features['style'],
                    None,
                    n_timesteps=25,
                    inference_cfg_rate=0.7
                )
                
                # ç§»é™¤åƒè€ƒéŸ³é »éƒ¨åˆ†
                mel = vc_target[:, :, self.character_features['ref_mel'].size(-1):]
            
            # 6. BigVGAN ç”Ÿæˆæ³¢å½¢
            if verbose:
                debug_log(3, "   [4/4] BigVGAN ç”Ÿæˆä¸­...")
            
            with torch.no_grad():
                # BigVGAN éœ€è¦ Float32 è¼¸å…¥ (åƒè€ƒ infer_v2.py line 641)
                audio_output = self.bigvgan(mel.float()).squeeze(0).cpu()
            
            # 7. ä¿å­˜éŸ³é »
            import torchaudio
            torchaudio.save(
                output_path,
                audio_output,
                sample_rate=22050,
                encoding="PCM_S",
                bits_per_sample=16
            )
            
            if verbose:
                duration = audio_output.shape[-1] / 22050
                debug_log(2, f"   âœ“ åˆæˆå®Œæˆ!")
                debug_log(3, f"   ğŸ“ ä¿å­˜è‡³: {output_path}")
                debug_log(3, f"   â±ï¸  éŸ³é »æ™‚é•·: {duration:.2f}ç§’")
            
            return True
            
        except Exception as e:
            error_log(f"âŒ åˆæˆå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_current_character(self) -> Optional[str]:
        """ç²å–ç•¶å‰åŠ è¼‰çš„è§’è‰²åç¨±"""
        return self.current_character
    
    def is_character_loaded(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦å·²åŠ è¼‰è§’è‰²"""
        return self.character_features is not None