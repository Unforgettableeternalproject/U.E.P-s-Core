# modules/stt_module/stt_module.py
# STT Module Phase 3 - æŒçºŒèƒŒæ™¯ç›£è½ + å¯¦æ™‚èªéŸ³è­˜åˆ¥æ•´åˆ + NLPæ¨¡çµ„é€£æ¥

import threading
import queue
import time
import re
import numpy as np
import tempfile
import os
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# æ–°çš„æ ¸å¿ƒä¾è³´
import torch
import pyaudio
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline

from core.module_base import BaseModule
from utils.debug_helper import debug_log, info_log, error_log
from configs.config_loader import load_module_config
from core.schemas import STTModuleData, create_stt_data
from core.schema_adapter import STTSchemaAdapter
from .schemas import STTInput, STTOutput, ActivationMode, SpeakerInfo

# ç¨ç«‹æ¨¡çµ„
from .vad import VoiceActivityDetection
from .speaker_identification import SpeakerIdentification

def correct_stt(text):
    """STT çµæœä¿®æ­£ - ä¸»è¦é‡å°è‹±æ–‡è­˜åˆ¥å„ªåŒ–"""
    corrections = {
        # UEP ç›¸é—œä¿®æ­£
        "you ep": "UEP",
        "youpee": "UEP", 
        "uvp": "UEP",
        "u e p": "UEP",
        "u.e.p": "UEP",
        "uep": "UEP",
        "you e p": "UEP",
        "yu ep": "UEP",
        "yup": "UEP",
        
        # å¸¸è¦‹è‹±æ–‡ä¿®æ­£
        "cant": "can't",
        "wont": "won't",
        "dont": "don't",
        "isnt": "isn't",
        
        # èªéŸ³åŠ©æ‰‹å¸¸è¦‹èª¤è­˜åˆ¥
        "hey you ep": "hey UEP",
        "hello you ep": "hello UEP",
        "hi you ep": "hi UEP"
    }
    
    result = text.lower()
    for wrong, correct in corrections.items():
        result = result.replace(wrong, correct)
    
    # ä¿æŒåŸæœ‰å¤§å°å¯«æ ¼å¼ï¼Œä½†ç¢ºä¿ UEP æ˜¯å¤§å¯«
    if "uep" in result.lower():
        result = re.sub(r'\buep\b', 'UEP', result, flags=re.IGNORECASE)
    
    return result

class STTModule(BaseModule):
    def __init__(self, config=None, working_context_manager=None, result_callback=None):
        self.config = config or load_module_config("stt_module")
        
        # å·¥ä½œä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.working_context_manager = working_context_manager
        
        # çµæœå›èª¿å‡½æ•¸ï¼Œç”¨æ–¼å°‡è­˜åˆ¥çµæœç™¼é€çµ¦NLPæ¨¡çµ„
        self.result_callback = result_callback
        
        # åŸºæœ¬é…ç½®
        self.device_index = self.config.get("device_index", None)  # å…è¨±è‡ªå‹•é¸æ“‡éº¥å…‹é¢¨
        self.phrase_time_limit = self.config.get("phrase_time_limit", 5)
        self.sample_rate = 16000  # Whisper æ¨™æº–æ¡æ¨£ç‡
        
        # Transformers Whisper æ¨¡å‹é…ç½®
        self.whisper_model_id = self.config.get("whisper_model_id", "openai/whisper-large-v3")
        self.whisper_local_path = self.config.get("whisper_local_path", "models/stt/whisper/whisper-large-v3")
        self.use_local_model = self.config.get("use_local_model", True)
        
        # è¨­å‚™é…ç½®
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        
        # æ¨¡å‹çµ„ä»¶
        self.model = None
        self.processor = None
        self.pipe = None
        
        # èªªè©±äººè­˜åˆ¥æ¨¡å¼é…ç½® (ç¾åœ¨ä½¿ç”¨çµ±ä¸€ç³»çµ±ï¼Œä½†ä¿ç•™æ­¤è®Šæ•¸ä»¥å…¼å®¹ç¾æœ‰é…ç½®)
        self.speaker_recognition_mode = self.config.get("speaker_recognition_mode", "unified")
        
        # ç¨ç«‹æ¨¡çµ„
        self.vad_module = VoiceActivityDetection(self.sample_rate)
        self.speaker_module = SpeakerIdentification(config=self.config)  # å¢å¼·ç‰ˆèªè€…è­˜åˆ¥ç³»çµ±
        
        # PyAudio é…ç½®
        self.pyaudio_instance = None
        self.audio_stream = None
        self.pa_config = {
            "format": pyaudio.paInt16,
            "channels": 1,
            "rate": self.sample_rate,
            "frames_per_buffer": 1024,
        }
        
        # ç•¶å‰ç‹€æ…‹
        self._current_mode = ActivationMode.MANUAL
        self._listening_active = False
        
        info_log("[STT] Transformers Whisper + pyannote æ¶æ§‹æ¨¡çµ„åˆå§‹åŒ–å®Œæˆ")

    def debug(self):
        debug_log(1, "[STT] Debug æ¨¡å¼å•Ÿç”¨")
        debug_log(2, f"[STT] åŸºæœ¬è¨­å®š: è¨­å‚™={self.device_index}, æ¡æ¨£ç‡={self.sample_rate}")
        debug_log(2, f"[STT] æ¨¡å‹ ID: {self.whisper_model_id}")
        debug_log(2, f"[STT] æœ¬åœ°è·¯å¾‘: {self.whisper_local_path}")
        debug_log(2, f"[STT] ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {self.use_local_model}")
        debug_log(2, f"[STT] è¨ˆç®—è¨­å‚™: {self.device}, æ•¸æ“šé¡å‹: {self.torch_dtype}")
        debug_log(2, f"[STT] PyAudio é…ç½®: {self.pa_config}")
        debug_log(2, f"[STT] æ¨¡å¼: æŒçºŒèƒŒæ™¯ç›£è½ï¼Œå¯¦æ™‚å‚³é€çµæœçµ¦NLPæ¨¡çµ„")

    def initialize(self):
        debug_log(1, "[STT] åˆå§‹åŒ–ä¸­...")
        self.debug()

        try:
            # åˆå§‹åŒ– Transformers Whisper æ¨¡å‹
            model_path = None
            if self.use_local_model and os.path.exists(self.whisper_local_path):
                model_path = self.whisper_local_path
                info_log(f"[STT] ä½¿ç”¨æœ¬åœ° Transformers æ¨¡å‹: {model_path}")
            else:
                model_path = self.whisper_model_id
                info_log(f"[STT] ä½¿ç”¨é ç«¯ Transformers æ¨¡å‹: {model_path}")
            
            # è¼‰å…¥æ¨¡å‹
            info_log("[STT] è¼‰å…¥ Whisper æ¨¡å‹...")
            self.model = AutoModelForSpeechSeq2Seq.from_pretrained(
                model_path,
                torch_dtype=self.torch_dtype,
                low_cpu_mem_usage=True,
                use_safetensors=True
            )
            self.model.to(self.device)
            
            # è¼‰å…¥è™•ç†å™¨
            info_log("[STT] è¼‰å…¥è™•ç†å™¨...")
            if self.use_local_model and os.path.exists(self.whisper_local_path):
                self.processor = AutoProcessor.from_pretrained(self.whisper_local_path)
            else:
                self.processor = AutoProcessor.from_pretrained(self.whisper_model_id)
            
            # å‰µå»º pipeline
            info_log("[STT] å‰µå»ºèªéŸ³è­˜åˆ¥ pipeline...")
            self.pipe = pipeline(
                "automatic-speech-recognition",
                model=self.model,
                tokenizer=self.processor.tokenizer,
                feature_extractor=self.processor.feature_extractor,
                torch_dtype=self.torch_dtype,
                device=self.device,
            )
            
            info_log(f"[STT] Transformers Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ (è¨­å‚™: {self.device})")
            
            # åˆå§‹åŒ– PyAudio
            self.pyaudio_instance = pyaudio.PyAudio()
            info_log("[STT] PyAudio åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–æ–°çš„ç¨ç«‹æ¨¡çµ„
            info_log("[STT] åˆå§‹åŒ– VAD æ¨¡çµ„...")
            if not self.vad_module.initialize():
                error_log("[STT] VAD æ¨¡çµ„åˆå§‹åŒ–å¤±æ•—ï¼Œä½†ä¸å½±éŸ¿åŸºæœ¬ STT åŠŸèƒ½")
            
            info_log("[STT] åˆå§‹åŒ–èªªè©±äººè­˜åˆ¥æ¨¡çµ„...")
            if not self.speaker_module.initialize():
                info_log("[STT] èªªè©±äººè­˜åˆ¥æ¨¡çµ„ä½¿ç”¨ fallback æ¨¡å¼ï¼ŒåŸºæœ¬åŠŸèƒ½ä»å¯ä½¿ç”¨")
            else:
                info_log("[STT] èªªè©±äººè­˜åˆ¥æ¨¡çµ„åˆå§‹åŒ–æˆåŠŸ")
            
            # èªè€…è­˜åˆ¥å·²ç¶“åˆå§‹åŒ–å®Œç•¢
            
            # åˆ—å‡ºå¯ç”¨çš„éŸ³é »è¨­å‚™
            debug_log(3, "[STT] å¯ç”¨éŸ³é »è¨­å‚™ï¼š")
            for i in range(self.pyaudio_instance.get_device_count()):
                device_info = self.pyaudio_instance.get_device_info_by_index(i)
                if device_info['maxInputChannels'] > 0:
                    device_name = device_info['name']
                    debug_log(3, f"  è¨­å‚™ {i}: {device_name}")
            
            # è¨­ç½®åˆå§‹åŒ–å®Œæˆæ¨™èªŒ
            self.is_initialized = True
            
            return True
            
        except Exception as e:
            error_log(f"[STT] åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
            return False

    def handle(self, data: dict = {}) -> dict:
        """è™•ç† STT è«‹æ±‚"""
        try:
            # ä½¿ç”¨ schema adapter è½‰æ›è¼¸å…¥æ•¸æ“š
            schema_adapter = STTSchemaAdapter()
            adapted_input = schema_adapter.adapt_input(data)
            
            # è½‰æ›ç‚ºæ¨¡çµ„å…§éƒ¨ä½¿ç”¨çš„æ ¼å¼
            validated = STTInput(**adapted_input)
            debug_log(1, f"[STT] è™•ç†è«‹æ±‚: {validated.mode}")
            
            start_time = time.time()
            
            if validated.mode == ActivationMode.MANUAL:
                # æ‰‹å‹•æ¨¡å¼ï¼šç«‹å³éŒ„éŸ³è­˜åˆ¥
                result = self._manual_recognition(validated)
            elif validated.mode == ActivationMode.CONTINUOUS:
                # æŒçºŒèƒŒæ™¯ç›£è½æ¨¡å¼ï¼šæŒçºŒéŒ„éŸ³ä¸¦å¯¦æ™‚å‚³é€çµæœçµ¦NLP
                result = self._continuous_recognition(validated)
            else:
                # ä¸æ”¯æŒçš„æ¨¡å¼
                raw_result = STTOutput(
                    text="", 
                    confidence=0.0, 
                    error="ä¸æ”¯æŒçš„æ¨¡å¼",
                    activation_reason="ä¸æ”¯æŒçš„æ¨¡å¼"
                ).model_dump()
                # ä½¿ç”¨ schema adapter è½‰æ›è¼¸å‡ºæ•¸æ“š
                return schema_adapter.adapt_output(raw_result)
                
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            
            # å°‡çµæœè½‰æ›ç‚º STTOutput ç‰©ä»¶
            stt_output = STTOutput(**result)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è­˜åˆ¥å‡ºæ–‡æœ¬
            if not stt_output.text or not stt_output.text.strip():
                info_log("[STT] ğŸ”‡ æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹")
                # æ›´æ–°éŒ¯èª¤ä¿¡æ¯
                stt_output.error = "æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹"
                result["error"] = "æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹"
            else:
                # ç¢ºä¿ç•¶æœ‰è­˜åˆ¥æ–‡æœ¬æ™‚ï¼Œç§»é™¤å¯èƒ½çš„éŒ¯èª¤ä¿¡æ¯
                stt_output.error = None
                result["error"] = None
            
            # ä½¿ç”¨ STTOutput çš„æ–¹æ³•è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
            unified_data = stt_output.to_unified_format()
            
            # å°‡çµ±ä¸€æ ¼å¼è½‰æ›ç‚º API è¼¸å‡ºæ ¼å¼
            return schema_adapter.adapt_output(result)
            
        except Exception as e:
            error_log(f"[STT] è™•ç†å¤±æ•—: {str(e)}")
            return STTOutput(
                text="",
                confidence=0.0,
                error=f"è™•ç†å¤±æ•—: {str(e)}"
            ).model_dump()

    def _manual_recognition(self, input_data: STTInput) -> dict:
        """æ‰‹å‹•èªéŸ³è­˜åˆ¥ - ä½¿ç”¨ Transformers Whisper"""
        try:
            info_log("[STT] é–‹å§‹éŒ„éŸ³...")
            
            # ä½¿ç”¨ PyAudio ç›´æ¥éŒ„éŸ³
            duration = input_data.duration if input_data.duration else self.phrase_time_limit
            audio_data = self._record_audio(duration)
            
            if audio_data is None or len(audio_data) == 0:
                return STTOutput(
                    text="", 
                    confidence=0.0, 
                    error="éŒ„éŸ³å¤±æ•—æˆ–éŸ³é »ç‚ºç©º",
                    activation_reason="éŒ„éŸ³å¤±æ•—"
                ).model_dump()
            
            info_log("[STT] ä½¿ç”¨ Transformers Whisper é€²è¡ŒèªéŸ³è­˜åˆ¥...")
            
            # æ­£è¦åŒ–éŸ³é »æ•¸æ“šåˆ° [-1, 1] ç¯„åœ
            audio_float = audio_data.astype(np.float32) / 32768.0
            
            # ç”Ÿæˆåƒæ•¸é…ç½®
            generate_kwargs = {
                "max_new_tokens": 128,  # é™ä½åˆ°å®‰å…¨ç¯„åœ
                "num_beams": 1,
                "condition_on_prev_tokens": False,
                "compression_ratio_threshold": 1.35,
                "temperature": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
                "logprob_threshold": -1.0,
                "no_speech_threshold": 0.4,  # é™ä½é–¾å€¼ä»¥æé«˜æ•æ„Ÿåº¦
                "return_timestamps": True,
                "language": "en",  # ä½¿ç”¨æ¨™æº–ä»£ç¢¼
            }
            
            # æª¢æŸ¥éŸ³é »æ•¸æ“šæ˜¯å¦æœ‰èªéŸ³å…§å®¹
            if not self.vad_module.has_sufficient_speech(audio_data):
                info_log("[STT] VAD æª¢æ¸¬ï¼šæœªæª¢æ¸¬åˆ°è¶³å¤ èªéŸ³å…§å®¹ï¼Œä½†ä»å˜—è©¦è­˜åˆ¥")
            
            # ä½¿ç”¨ Transformers pipeline é€²è¡ŒèªéŸ³è­˜åˆ¥
            result = self.pipe(
                audio_float,
                generate_kwargs=generate_kwargs
            )
            
            text = result["text"].strip()
            text = correct_stt(text)
            confidence = self._calculate_transformers_confidence(result)
            
            # æª¢æŸ¥çµæœæ˜¯å¦ç‚ºç©º
            if not text or text.isspace():
                info_log("[STT] ğŸ”‡ æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹")
                return STTOutput(
                    text="",
                    confidence=0.0,
                    speaker_info=None,
                    activation_reason="manual",
                    error="æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹"
                ).model_dump()
            
            # èªªè©±äººè­˜åˆ¥ - æ ¹æ“šé…ç½®é¸æ“‡æ¨¡å¼
            speaker_info = None
            if input_data.enable_speaker_id:
                speaker_info = self._identify_speaker_with_mode(audio_data)
            
            # é¡¯ç¤ºè­˜åˆ¥çµæœ
            info_log(f"[STT] è­˜åˆ¥çµæœ: '{text}' (ä¿¡å¿ƒåº¦: {confidence:.2f})")
            
            return STTOutput(
                text=text,
                confidence=confidence,
                speaker_info=speaker_info,
                activation_reason="manual",
                error=None
            ).model_dump()
            
        except Exception as e:
            error_log(f"[STT] è­˜åˆ¥å¤±æ•—: {str(e)}")
            return STTOutput(
                text="", 
                confidence=0.0, 
                error=f"è­˜åˆ¥å¤±æ•—: {str(e)}",
                activation_reason="è­˜åˆ¥å¤±æ•—ï¼šæœªçŸ¥éŒ¯èª¤"
            ).model_dump()

    def _record_audio(self, duration: float) -> np.ndarray:
        """ä½¿ç”¨ PyAudio éŒ„è£½éŸ³é »"""
        try:
            # å‰µå»ºéŸ³é »æµ
            stream_params = {
                "format": self.pa_config["format"],
                "channels": self.pa_config["channels"],
                "rate": self.pa_config["rate"],
                "input": True,
                "frames_per_buffer": self.pa_config["frames_per_buffer"]
            }
            
            # åªæœ‰ç•¶è¨­å‚™ç´¢å¼•è¢«æ˜ç¢ºæŒ‡å®šæ™‚æ‰æ·»åŠ 
            if self.device_index is not None:
                stream_params["input_device_index"] = self.device_index
                
            stream = self.pyaudio_instance.open(**stream_params)
            
            frames = []
            frames_to_record = int(self.sample_rate * duration / self.pa_config["frames_per_buffer"])
            
            info_log(f"[STT] é–‹å§‹éŒ„éŸ³ {duration} ç§’...")
            for _ in range(frames_to_record):
                data = stream.read(self.pa_config["frames_per_buffer"])
                frames.append(data)
            
            stream.stop_stream()
            stream.close()
            
            # è½‰æ›ç‚º numpy æ•¸çµ„
            audio_data = b''.join(frames)
            audio_array = np.frombuffer(audio_data, dtype=np.int16)
            
            # ç°¡å–®çš„éŸ³é »å‰è™•ç†ï¼šæ­¸ä¸€åŒ–ä¸¦å¢å¼·
            if len(audio_array) > 0:
                # æª¢æŸ¥éŸ³é »æ˜¯å¦å…¨ç‚ºéœéŸ³
                if np.max(np.abs(audio_array)) > 0:
                    # æ­¸ä¸€åŒ–åˆ°æœ€å¤§æŒ¯å¹…çš„ 90%
                    norm_factor = 0.9 * 32767 / np.max(np.abs(audio_array))
                    audio_array = (audio_array * norm_factor).astype(np.int16)
            
            info_log(f"[STT] éŒ„éŸ³å®Œæˆï¼Œé•·åº¦: {len(audio_array) / self.sample_rate:.2f} ç§’")
            return audio_array
            
        except Exception as e:
            error_log(f"[STT] éŒ„éŸ³å¤±æ•—: {str(e)}")
            return None

    def _calculate_transformers_confidence(self, result: dict) -> float:
        """è¨ˆç®— Transformers Whisper çµæœçš„ä¿¡å¿ƒåº¦"""
        try:
            # åŸºæ–¼æ–‡æœ¬å…§å®¹å’Œæ™‚é–“æˆ³è³‡è¨Šä¼°ç®—ä¿¡å¿ƒåº¦
            text = result.get("text", "").strip()
            
            if not text:
                return 0.0
            
            base_confidence = 0.8  # Transformers æ¨¡å‹é€šå¸¸æœ‰æ›´é«˜çš„åŸºç¤ä¿¡å¿ƒåº¦
            
            # åŸºæ–¼æ–‡æœ¬é•·åº¦èª¿æ•´
            text_length = len(text.split())
            if 1 <= text_length <= 30:
                length_bonus = 0.15
            elif text_length > 30:
                length_bonus = 0.1
            else:
                length_bonus = 0.0
            
            # åŸºæ–¼æ˜¯å¦åŒ…å«å¸¸è¦‹è©å½™
            common_words = ["UEP", "help", "please", "can", "you", "the", "a", "an", "is", "are"]
            common_word_count = sum(1 for word in text.lower().split() if word in common_words)
            common_word_bonus = min(common_word_count * 0.03, 0.1)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ™‚é–“æˆ³è³‡è¨Šï¼ˆè¡¨ç¤ºæ¨¡å‹å°çµæœæœ‰ä¿¡å¿ƒï¼‰
            if "chunks" in result and result["chunks"]:
                timestamp_bonus = 0.05
            else:
                timestamp_bonus = 0.0
            
            confidence = min(base_confidence + length_bonus + common_word_bonus + timestamp_bonus, 1.0)
            return confidence
            
        except Exception as e:
            debug_log(3, f"[STT] è¨ˆç®—ä¿¡å¿ƒåº¦å¤±æ•—: {str(e)}")
            return 0.8  # é»˜èªä¿¡å¿ƒåº¦

    def shutdown(self):
        """é—œé–‰æ¨¡çµ„"""
        if self.pyaudio_instance:
            self.pyaudio_instance.terminate()
        
        # é—œé–‰æ–°çš„ç¨ç«‹æ¨¡çµ„
        if hasattr(self, 'vad_module'):
            self.vad_module.shutdown()
        if hasattr(self, 'speaker_module'):
            self.speaker_module.shutdown()

    def _continuous_recognition(self, input_data: STTInput) -> dict:
        """æŒçºŒèƒŒæ™¯ç›£è½ - æŒçºŒéŒ„éŸ³ä¸¦å¯¦æ™‚å‚³é€çµæœçµ¦NLPæ¨¡çµ„"""
        try:
            info_log("[STT] é–‹å§‹æŒçºŒèƒŒæ™¯ç›£è½æ¨¡å¼...")
            
            # è¨­å®šç›£è½æ™‚é•·ï¼Œå¦‚æœæœªæŒ‡å®šå‰‡ä½¿ç”¨é»˜èªå€¼
            duration = input_data.duration or 30.0
            start_time = time.time()
            
            info_log(f"[STT] æŒçºŒç›£è½æ™‚é•·: {duration} ç§’")
            
            # å‰µå»ºèªè€…ä¸Šä¸‹æ–‡ï¼Œç”¨æ–¼ç´¯ç©èªè€…è³‡è¨Š
            context_id = None
            if self.working_context_manager:
                from core.working_context import ContextType
                # å‰µå»ºæˆ–ç²å–SPEAKER_ACCUMULATIONä¸Šä¸‹æ–‡
                context_id = self.working_context_manager.create_context(
                    ContextType.SPEAKER_ACCUMULATION, 
                    threshold=15,  # æ¨£æœ¬é–¾å€¼
                    timeout=300.0  # 5åˆ†é˜éæœŸ
                )
                debug_log(2, f"[STT] å·²å»ºç«‹æŒçºŒç›£è½çš„èªéŸ³ç´¯ç©ä¸Šä¸‹æ–‡: {context_id}")
            
            # æŒçºŒç›£è½ç›´åˆ°é”åˆ°æŒ‡å®šæ™‚é–“
            while time.time() - start_time < duration:
                # çŸ­æš«éŒ„éŸ³æª¢æ¸¬
                chunk_duration = 2.0
                audio_data = self._record_audio(chunk_duration)
                
                if audio_data is None:
                    continue
                
                # ä½¿ç”¨VADæª¢æŸ¥æ˜¯å¦æœ‰èªéŸ³å…§å®¹
                if not self.vad_module.has_sufficient_speech(audio_data, min_duration=0.05):
                    debug_log(3, "[STT] éŸ³é »ä¸­èªéŸ³å…§å®¹ä¸è¶³ï¼Œç¹¼çºŒç›£è½")
                    continue
                
                # å°‡éŸ³é »æ•¸æ“šæ·»åŠ åˆ°èªè€…ä¸Šä¸‹æ–‡
                if context_id and self.working_context_manager:
                    self.working_context_manager.add_data_to_context(
                        context_id, 
                        audio_data,
                        metadata={"timestamp": time.time(), "type": "audio_sample"}
                    )
                
                # ä½¿ç”¨Whisperé€²è¡ŒèªéŸ³è­˜åˆ¥
                info_log("[STT] æª¢æ¸¬åˆ°èªéŸ³ï¼Œé€²è¡Œè­˜åˆ¥...")
                audio_float = audio_data.astype(np.float32) / 32768.0
                
                # è­˜åˆ¥åƒæ•¸
                recognition_kwargs = {
                    "max_new_tokens": 128,
                    "num_beams": 1,
                    "condition_on_prev_tokens": False,
                    "compression_ratio_threshold": 1.35,  # èˆ‡æ‰‹å‹•æ¨¡å¼ä¿æŒä¸€è‡´
                    "temperature": 0.0,  # åœ¨é€£çºŒæ¨¡å¼ä¸‹ä½¿ç”¨å›ºå®šæº«åº¦ä»¥æé«˜é€Ÿåº¦
                    "logprob_threshold": -1.0,  # æ·»åŠ æ­¤åƒæ•¸é¿å… logprobs éŒ¯èª¤
                    "no_speech_threshold": 0.4,  # è¼ƒä½çš„é–¾å€¼
                    "return_timestamps": True,
                    "language": "en",  # ä½¿ç”¨æ¨™æº–ä»£ç¢¼
                }
                
                result = self.pipe(audio_float, generate_kwargs=recognition_kwargs)
                text = result["text"].strip()
                text = correct_stt(text)  # æ‡‰ç”¨STTä¿®æ­£
                
                # è¨ˆç®—ä¿¡å¿ƒåº¦
                confidence = self._calculate_transformers_confidence(result)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰è­˜åˆ¥å‡ºæ–‡æœ¬
                if not text or text.isspace():
                    debug_log(2, "[STT] æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹ï¼Œç¹¼çºŒç›£è½")
                    continue
                
                info_log(f"[STT] è­˜åˆ¥åˆ°èªéŸ³å…§å®¹: '{text}' (ä¿¡å¿ƒåº¦: {confidence:.2f})")
                
                # é€²è¡Œèªªè©±äººè­˜åˆ¥
                speaker_info = None
                if input_data.enable_speaker_id:
                    speaker_info = self._identify_speaker_with_mode(audio_data)
                    debug_log(2, f"[STT] è­˜åˆ¥èªè€…: {speaker_info.speaker_id} (ä¿¡å¿ƒåº¦: {speaker_info.confidence:.2f})")
                
                # å‰µå»ºè¼¸å‡ºç‰©ä»¶
                output = STTOutput(
                    text=text,
                    confidence=confidence,
                    speaker_info=speaker_info,
                    activation_reason="continuous_listening",
                    error=None
                )
                
                # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
                unified_data = output.to_unified_format()
                
                # å¦‚æœæœ‰ä¸Šä¸‹æ–‡IDï¼Œæ·»åŠ åˆ°metadata
                if context_id:
                    unified_data.metadata["context_id"] = context_id
                
                # é€šéå›èª¿å°‡çµæœç™¼é€çµ¦NLPæ¨¡çµ„
                if self.result_callback:
                    try:
                        # å°‡çµæœç™¼é€çµ¦å›èª¿å‡½æ•¸
                        self.result_callback(unified_data)
                        info_log(f"[STT] å°‡è­˜åˆ¥çµæœå¯¦æ™‚ç™¼é€çµ¦NLPæ¨¡çµ„ï¼š'{text}' (èªè€…: {speaker_info.speaker_id if speaker_info else 'unknown'})")
                    except Exception as e:
                        error_log(f"[STT] ç™¼é€è­˜åˆ¥çµæœå¤±æ•—: {e}")
                else:
                    debug_log(2, "[STT] æœªè¨­å®šçµæœå›èª¿å‡½æ•¸ï¼Œè­˜åˆ¥çµæœå°‡ä¸æœƒç™¼é€çµ¦NLPæ¨¡çµ„")
                
                # çŸ­æš«ä¼‘æ¯
                time.sleep(0.1)
            
            # ç›£è½çµæŸ
            if context_id and self.working_context_manager:
                # ä¸è¦æ¨™è¨˜ç‚ºå®Œæˆï¼Œå› ç‚ºæ˜¯æŒçºŒç›£è½
                # self.working_context_manager.mark_context_completed(context_id)
                pass
                
            # è¿”å›æœ€å¾Œçš„ç›£è½ç‹€æ…‹
            return STTOutput(
                text="",
                confidence=0.0,
                speaker_info=None,
                activation_reason="continuous_listening_completed",
                error=None
            ).model_dump()
            
        except Exception as e:
            error_log(f"[STT] æŒçºŒç›£è½å¤±æ•—: {str(e)}")
            return STTOutput(
                text="",
                confidence=0.0,
                error=f"æŒçºŒç›£è½å¤±æ•—: {str(e)}",
                activation_reason="continuous_listening_failed"
            ).model_dump()
            
    def _identify_speaker_with_mode(self, audio_data: np.ndarray) -> SpeakerInfo:
        """æ ¹æ“šé…ç½®çš„æ¨¡å¼é€²è¡Œèªªè©±äººè­˜åˆ¥"""
        try:
            # æˆ‘å€‘ç¾åœ¨åªæœ‰ä¸€ç¨®èªè€…è­˜åˆ¥ç³»çµ±ï¼Œç„¡éœ€å†æ ¹æ“šæ¨¡å¼é¸æ“‡
            debug_log(2, f"[STT] ä½¿ç”¨çµ±ä¸€çš„èªè€…è­˜åˆ¥ç³»çµ±")
            return self.speaker_module.identify_speaker(audio_data)
                
        except Exception as e:
            error_log(f"[STT] èªªè©±äººè­˜åˆ¥å®Œå…¨å¤±æ•—: {e}")
            # è¿”å›é»˜èªçµæœ
            return SpeakerInfo(
                speaker_id="unknown",
                confidence=0.0,
                is_new_speaker=False,
                voice_features={"error": str(e)}
            )

    def shutdown(self):
        # æ¸…ç† GPU è¨˜æ†¶é«”
        if self.model is not None:
            del self.model
        if self.pipe is not None:
            del self.pipe
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        info_log("[STT] æ¨¡çµ„å·²é—œé–‰")
