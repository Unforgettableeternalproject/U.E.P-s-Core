#!/usr/bin/env python3
"""
åŸºæ–¼Transformerçš„åºåˆ—æ¨™è¨»æ¨¡å‹
å¯¦ç¾BIOæ¨™è¨˜çš„å¤šæ„åœ–åˆ†æ®µè­˜åˆ¥
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import TrainingArguments, Trainer
from transformers import DataCollatorForTokenClassification
from typing import List, Dict, Any, Tuple, Optional
import json
from pathlib import Path
import numpy as np
from utils.debug_helper import debug_log, info_log, error_log

class BIOTagger:
    """BIOæ¨™è¨˜çš„åºåˆ—æ¨™è¨»å™¨"""
    
    # BIOæ¨™ç±¤å®šç¾©ï¼ˆStage 4 æ›´æ–°ï¼‰
    BIO_LABELS = [
        "O",                    # Outside
        "B-CALL",              # Begin Call
        "I-CALL",              # Inside Call  
        "B-CHAT",              # Begin Chat
        "I-CHAT",              # Inside Chat
        "B-DIRECT_WORK",       # Begin Direct Work (ç·Šæ€¥å·¥ä½œ)
        "I-DIRECT_WORK",       # Inside Direct Work
        "B-BACKGROUND_WORK",   # Begin Background Work (èƒŒæ™¯ä»»å‹™)
        "I-BACKGROUND_WORK",   # Inside Background Work
        "B-UNKNOWN",           # Begin Unknown (ç„¡æ³•è­˜åˆ¥)
        "I-UNKNOWN"            # Inside Unknown
    ]
    
    def __init__(self, model_name: str = "distilbert-base-uncased"):
        """åˆå§‹åŒ–BIOæ¨™è¨»å™¨"""
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.label2id = {label: i for i, label in enumerate(self.BIO_LABELS)}
        self.id2label = {i: label for i, label in enumerate(self.BIO_LABELS)}
        
        info_log(f"[BIOTagger] åˆå§‹åŒ–åºåˆ—æ¨™è¨»å™¨: {model_name}")
    
    def load_model(self, model_path: Optional[str] = None):
        """è¼‰å…¥æ¨¡å‹"""
        try:
            if model_path and Path(model_path).exists():
                # è¼‰å…¥å¾®èª¿å¾Œçš„æ¨¡å‹
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                self.model = AutoModelForTokenClassification.from_pretrained(
                    model_path,
                    num_labels=len(self.BIO_LABELS),
                    id2label=self.id2label,
                    label2id=self.label2id
                )
                info_log(f"[BIOTagger] è¼‰å…¥å¾®èª¿æ¨¡å‹: {model_path}")
            else:
                # è¼‰å…¥é è¨“ç·´æ¨¡å‹
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self.model = AutoModelForTokenClassification.from_pretrained(
                    self.model_name,
                    num_labels=len(self.BIO_LABELS),
                    id2label=self.id2label,
                    label2id=self.label2id
                )
                info_log(f"[BIOTagger] è¼‰å…¥é è¨“ç·´æ¨¡å‹: {self.model_name}")
                
        except Exception as e:
            error_log(f"[BIOTagger] æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            # ä½¿ç”¨å‚™ç”¨æ¨¡å¼
            info_log("[BIOTagger] å•Ÿç”¨å‚™ç”¨æ¨¡å¼")
            self.tokenizer = None
            self.model = None
            return True
            
        return True
    
    def predict(self, text: str) -> List[Dict[str, Any]]:
        """é æ¸¬æ–‡æœ¬çš„BIOæ¨™ç±¤ä¸¦è¿”å›åˆ†æ®µçµæœ"""
        if not self.model or not self.tokenizer:
            # å‚™ç”¨å¯¦ç¾ï¼šç°¡å–®çš„è¦å‰‡åˆ†æ®µ
            return self._fallback_segmentation(text)
        
        try:
            # åˆ†è©
            inputs = self.tokenizer(
                text, 
                return_tensors="pt",
                truncation=True,
                is_split_into_words=False,
                return_offsets_mapping=True
            )
            
            # é æ¸¬
            with torch.no_grad():
                outputs = self.model(**{k: v for k, v in inputs.items() if k != 'offset_mapping'})
                predictions = torch.argmax(outputs.logits, dim=-1)
                # è¨ˆç®— softmax æ©Ÿç‡ä½œç‚º confidence
                probabilities = torch.softmax(outputs.logits, dim=-1)
                confidences = torch.max(probabilities, dim=-1).values
            
            # ç²å–tokenåˆ°å­—ç¬¦çš„æ˜ å°„
            offset_mapping = inputs['offset_mapping'][0]
            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
            predicted_labels = [self.id2label[pred.item()] for pred in predictions[0]] # type: ignore
            token_confidences = confidences[0].tolist()
            
            # å°‡BIOæ¨™ç±¤è½‰æ›ç‚ºåˆ†æ®µï¼ˆå‚³é confidence å€¼ï¼‰
            segments = self._bio_to_segments(text, tokens, predicted_labels, offset_mapping, token_confidences)
            
            debug_log(3, f"[BIOTagger] è­˜åˆ¥åˆ° {len(segments)} å€‹åˆ†æ®µ")
            return segments
            
        except Exception as e:
            error_log(f"[BIOTagger] é æ¸¬å¤±æ•—: {e}")
            return self._fallback_segmentation(text)
    
    def _fallback_segmentation(self, text: str) -> List[Dict[str, Any]]:
        """å‚™ç”¨åˆ†æ®µå¯¦ç¾ - åŸºæ–¼ç°¡å–®è¦å‰‡"""
        segments = []
        
        # ç°¡å–®çš„é—œéµè©æª¢æ¸¬
        text_lower = text.lower()
        
        # ğŸ”§ ç‰¹æ®Šè™•ç†ï¼šU.E.P åå­—è­˜åˆ¥ï¼ˆæåŠåå­—è¦–ç‚ºèŠå¤©æˆ–å‘¼å«ï¼‰
        # æ”¯æ´å¤šç¨®æ ¼å¼: u.e.p / uep / U.E.P / UEP
        uep_patterns = ['u.e.p', 'u e p', 'uep']
        contains_uep_name = any(pattern in text_lower for pattern in uep_patterns)
        
        # æª¢æ¸¬å‘¼å«æ„åœ–
        call_keywords = ['hey', 'hello', 'hi', 'uep', 'system', 'wake up']
        chat_keywords = ['how are you', 'what do you think', 'tell me', 'story', 'chat', 'name', 'nickname', 'real name']
        direct_work_keywords = ['save', 'open', 'create', 'delete', 'show', 'search', 'find']
        background_work_keywords = ['play', 'sync', 'backup', 'download', 'install', 'update']
        
        # ğŸ”§ å¦‚æœæåŠ U.E.P åå­—ä¸”æœ‰å•é¡Œï¼Œè¦–ç‚ºèŠå¤©
        if contains_uep_name and ('?' in text or 'what' in text_lower or 'who' in text_lower or 'name' in text_lower):
            intent = 'chat'
        elif any(keyword in text_lower for keyword in call_keywords):
            intent = 'call'
        elif any(keyword in text_lower for keyword in direct_work_keywords):
            intent = 'direct_work'
        elif any(keyword in text_lower for keyword in background_work_keywords):
            intent = 'background_work'
        elif any(keyword in text_lower for keyword in chat_keywords):
            intent = 'chat'
        else:
            intent = 'unknown'  # ç„¡æ³•è­˜åˆ¥æ™‚è¿”å› unknown
        
        # å‰µå»ºå–®ä¸€åˆ†æ®µï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        segment = {
            'text': text,
            'intent': intent,
            'start_pos': 0,
            'end_pos': len(text),
            'confidence': 0.7  # è¼ƒä½çš„ä¿¡å¿ƒåº¦è¡¨ç¤ºé€™æ˜¯å‚™ç”¨å¯¦ç¾
        }
        
        segments.append(segment)
        info_log(f"[BIOTagger] å‚™ç”¨åˆ†æ®µ: '{text}' -> {intent}")
        
        return segments
    
    def _bio_to_segments(self, text: str, tokens: List[str], labels: List[str], 
                        offset_mapping: torch.Tensor, token_confidences: Optional[List[float]] = None) -> List[Dict[str, Any]]:
        """å°‡BIOæ¨™ç±¤åºåˆ—è½‰æ›ç‚ºåˆ†æ®µçµæœ
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            tokens: tokenizer åˆ†è©çµæœ
            labels: BIO æ¨™ç±¤åºåˆ—
            offset_mapping: token åˆ°å­—ç¬¦çš„æ˜ å°„
            token_confidences: æ¯å€‹ token çš„é æ¸¬ä¿¡å¿ƒåº¦ï¼ˆå¯é¸ï¼‰
        """
        segments = []
        current_segment = None
        segment_confidences = []  # è¿½è¹¤ç•¶å‰åˆ†æ®µçš„æ‰€æœ‰ token confidence
        
        for i, (token, label, offset) in enumerate(zip(tokens, labels, offset_mapping)):
            confidence = token_confidences[i] if token_confidences else 0.9
            # è·³éç‰¹æ®Štoken
            if token in ['[CLS]', '[SEP]', '[PAD]']:
                continue
                
            start_pos, end_pos = offset.tolist()
            
            if label.startswith('B-'):
                # é–‹å§‹æ–°åˆ†æ®µ
                if current_segment:
                    # è¨ˆç®—ç•¶å‰åˆ†æ®µçš„å¹³å‡ confidence
                    avg_confidence = sum(segment_confidences) / len(segment_confidences) if segment_confidences else 0.9
                    current_segment['confidence'] = round(avg_confidence, 3)
                    segments.append(current_segment)
                
                intent_type = label[2:].lower()  # ç§»é™¤ 'B-' å‰ç¶´
                current_segment = {
                    'text': text[start_pos:end_pos],
                    'intent': intent_type,
                    'start_pos': start_pos,
                    'end_pos': end_pos
                }
                segment_confidences = [confidence]  # é‡ç½®ä¸¦æ·»åŠ ç•¶å‰ token confidence
                
            elif label.startswith('I-'):
                intent_type = label[2:].lower()
                
                if current_segment and current_segment['intent'] == intent_type:
                    # å»¶çºŒç•¶å‰åˆ†æ®µ
                    current_segment['end_pos'] = end_pos
                    current_segment['text'] = text[current_segment['start_pos']:end_pos]
                    segment_confidences.append(confidence)  # è¿½è¹¤ confidence
                else:
                    # æ²’æœ‰å°æ‡‰çš„ B- æ¨™ç±¤ï¼Œæˆ–æ¨™ç±¤ä¸ä¸€è‡´
                    # å°‡å­¤ç«‹çš„ I- æ¨™ç±¤è¦–ç‚º B- é–‹å§‹æ–°åˆ†æ®µ
                    if current_segment:
                        avg_confidence = sum(segment_confidences) / len(segment_confidences) if segment_confidences else 0.9
                        current_segment['confidence'] = round(avg_confidence, 3)
                        segments.append(current_segment)
                    
                    current_segment = {
                        'text': text[start_pos:end_pos],
                        'intent': intent_type,
                        'start_pos': start_pos,
                        'end_pos': end_pos
                    }
                    segment_confidences = [confidence]  # é‡ç½®
            
            elif label == 'O':
                # çµæŸç•¶å‰åˆ†æ®µ
                if current_segment:
                    avg_confidence = sum(segment_confidences) / len(segment_confidences) if segment_confidences else 0.9
                    current_segment['confidence'] = round(avg_confidence, 3)
                    segments.append(current_segment)
                    current_segment = None
                    segment_confidences = []  # é‡ç½®
        
        # æ·»åŠ æœ€å¾Œä¸€å€‹åˆ†æ®µ
        if current_segment:
            avg_confidence = sum(segment_confidences) / len(segment_confidences) if segment_confidences else 0.9
            current_segment['confidence'] = round(avg_confidence, 3)
            segments.append(current_segment)
        
        return segments
    
    def prepare_training_data(self, annotated_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æº–å‚™è¨“ç·´æ•¸æ“š"""
        texts = []
        labels = []
        
        for example in annotated_data:
            text = example['text']
            segments = example['segments']  # [{'start': 0, 'end': 10, 'label': 'CALL'}, ...]
            
            # è½‰æ›ç‚ºBIOæ¨™ç±¤
            bio_labels = self._segments_to_bio(text, segments)
            
            texts.append(text)
            labels.append(bio_labels)
        
        # ä½¿ç”¨tokenizerè™•ç†
        tokenized_inputs = self.tokenizer(
            texts,
            truncation=True,
            is_split_into_words=False,
            return_offsets_mapping=True,
            padding=True
        ) # type: ignore
        
        # å°é½Šæ¨™ç±¤
        aligned_labels = []
        for i, label_seq in enumerate(labels):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            aligned_label = self._align_labels_with_tokens(label_seq, word_ids)
            aligned_labels.append(aligned_label)
        
        return {
            'input_ids': tokenized_inputs['input_ids'],
            'attention_mask': tokenized_inputs['attention_mask'],
            'labels': aligned_labels
        }
    
    def _segments_to_bio(self, text: str, segments: List[Dict[str, Any]]) -> List[str]:
        """å°‡åˆ†æ®µæ¨™è¨»è½‰æ›ç‚ºBIOæ¨™ç±¤åºåˆ—"""
        # é€™æ˜¯ä¸€å€‹ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›å¯¦ç¾éœ€è¦æ›´ç²¾ç¢ºçš„tokenå°é½Š
        bio_labels = ['O'] * len(text.split())
        
        for segment in segments:
            start = segment['start']
            end = segment['end']
            label = segment['label'].upper()
            
            # ç°¡åŒ–ï¼šå‡è¨­æ¯å€‹è©ä¸€å€‹token
            words = text.split()
            char_pos = 0
            
            for i, word in enumerate(words):
                word_start = char_pos
                word_end = char_pos + len(word)
                
                if word_start >= start and word_end <= end:
                    if bio_labels[i] == 'O':  # ç¬¬ä¸€å€‹è©
                        bio_labels[i] = f'B-{label}'
                    else:  # å¾ŒçºŒè©
                        bio_labels[i] = f'I-{label}'
                
                char_pos = word_end + 1  # +1 for space
        
        return bio_labels
    
    def _align_labels_with_tokens(self, labels: List[str], word_ids: List[Optional[int]]) -> List[int]:
        """å°é½Šæ¨™ç±¤èˆ‡tokens"""
        aligned_labels = []
        previous_word_idx = None
        
        for word_idx in word_ids:
            if word_idx is None:
                # ç‰¹æ®Štoken
                aligned_labels.append(-100)
            elif word_idx != previous_word_idx:
                # æ–°è©çš„ç¬¬ä¸€å€‹subtoken
                aligned_labels.append(self.label2id[labels[word_idx]])
            else:
                # åŒä¸€è©çš„å¾ŒçºŒsubtoken
                aligned_labels.append(-100)
            
            previous_word_idx = word_idx
        
        return aligned_labels


class EnhancedIntentAnalyzer:
    """å¢å¼·çš„æ„åœ–åˆ†æå™¨ï¼Œæ•´åˆBIOæ¨™è¨»å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.bio_tagger = BIOTagger(config.get('model_name', 'distilbert-base-uncased'))
        self.fallback_analyzer = None  # åŸæœ‰çš„æ„åœ–åˆ†æå™¨ä½œç‚ºå¾Œå‚™
        
    def initialize(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        try:
            model_path = self.config.get('bio_model_path')
            if not self.bio_tagger.load_model(model_path):
                error_log("[EnhancedIntentAnalyzer] BIOæ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨å¾Œå‚™æ–¹æ¡ˆ")
                return False
            
            info_log("[EnhancedIntentAnalyzer] å¢å¼·æ„åœ–åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            error_log(f"[EnhancedIntentAnalyzer] åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def analyze_intent(self, text: str, enable_segmentation: bool = True, 
                      context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """åˆ†ææ„åœ–ï¼ˆå¢å¼·ç‰ˆï¼‰"""
        
        result = {
            "primary_intent": "unknown",
            "intent_segments": [],
            "overall_confidence": 0.0,
            "segmentation_method": "bio_tagging"
        }
        
        try:
            # ä½¿ç”¨BIOæ¨™è¨»å™¨é€²è¡Œåˆ†æ®µ
            segments = self.bio_tagger.predict(text)
            
            if segments:
                # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
                intent_segments = []
                confidences = []
                
                for segment in segments:
                    from modules.nlp_module.schemas import IntentSegment, IntentType
                    
                    # æ˜ å°„intenté¡å‹ï¼ˆçµ±ä¸€ç‚º WORKï¼Œä½¿ç”¨ work_mode å€åˆ†ï¼‰
                    intent_mapping = {
                        'call': IntentType.CALL,
                        'chat': IntentType.CHAT,
                        'direct_work': IntentType.WORK,
                        'background_work': IntentType.WORK,
                        'unknown': IntentType.UNKNOWN
                    }
                    
                    # work_mode æ˜ å°„ï¼ˆåƒ…ç”¨æ–¼ WORK é¡å‹ï¼‰
                    work_mode_mapping = {
                        'direct_work': 'direct',
                        'background_work': 'background'
                    }
                    
                    intent_type = intent_mapping.get(segment['intent'], IntentType.UNKNOWN)
                    work_mode = work_mode_mapping.get(segment['intent'])
                    
                    # æ§‹å»º metadata
                    metadata = {}
                    if work_mode:
                        metadata['work_mode'] = work_mode
                    
                    intent_seg = IntentSegment(
                        text=segment['text'],
                        intent=intent_type,
                        confidence=segment['confidence'],
                        start_pos=segment['start_pos'],
                        end_pos=segment['end_pos'],
                        entities=[],  # å¯ä»¥é€²ä¸€æ­¥æ•´åˆå¯¦é«”è­˜åˆ¥
                        metadata=metadata
                    )
                    
                    intent_segments.append(intent_seg)
                    confidences.append(segment['confidence'])
                
                # è¨ˆç®—ä¸»è¦æ„åœ–
                primary_intent = self._determine_primary_intent(intent_segments)
                overall_confidence = np.mean(confidences) if confidences else 0.0
                
                result.update({
                    "primary_intent": primary_intent,
                    "intent_segments": intent_segments,
                    "overall_confidence": overall_confidence
                })
                
                info_log(f"[EnhancedIntentAnalyzer] BIOåˆ†æ®µæˆåŠŸ: {len(intent_segments)} å€‹ç‰‡æ®µ")
                
            else:
                # BIOæ¨™è¨»å¤±æ•—ï¼Œä½¿ç”¨å¾Œå‚™æ–¹æ¡ˆ
                debug_log(2, "[EnhancedIntentAnalyzer] BIOæ¨™è¨»ç„¡çµæœï¼Œä½¿ç”¨å¾Œå‚™åˆ†æ")
                if self.fallback_analyzer:
                    return self.fallback_analyzer.analyze_intent(text, enable_segmentation, context)
                    
        except Exception as e:
            error_log(f"[EnhancedIntentAnalyzer] åˆ†æå¤±æ•—: {e}")
            
        return result
    
    def _determine_primary_intent(self, segments) -> str:
        """æ±ºå®šä¸»è¦æ„åœ–"""
        if not segments:
            return "unknown"
            
        # å„ªå…ˆç´šè¦å‰‡ï¼ˆStage 4 æ›´æ–°ï¼‰
        priority = {
            "direct_work": 4,      # æœ€é«˜å„ªå…ˆç´š
            "call": 3,
            "background_work": 2,
            "chat": 1,
            "unknown": 0
        }
        
        # é¸æ“‡æœ€é«˜å„ªå…ˆç´šçš„æ„åœ–
        best_intent = "unknown"
        best_priority = -1
        
        for segment in segments:
            intent_str = segment.intent.value if hasattr(segment.intent, 'value') else str(segment.intent)
            intent_priority = priority.get(intent_str, 0)
            
            if intent_priority > best_priority:
                best_priority = intent_priority
                best_intent = intent_str
                
        return best_intent
