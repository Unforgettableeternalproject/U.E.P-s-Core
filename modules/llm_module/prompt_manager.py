# modules/llm_module/prompt_manager.py
"""
Prompt Manager - Integrated prompt building functionality

Responsible for dynamically building prompts based on different states and modes:
- CHAT mode: Personalized conversation prompts
- WORK mode: Workflow guidance prompts  
- System values integration
- Identity information integration
- Memory context integration
- Module interface integration (MEM/SYS)
"""

import os
import time
from typing import Dict, Any, Optional, List, Callable
from pathlib import Path
from utils.debug_helper import debug_log, info_log, error_log
from core.status_manager import status_manager
from .module_interfaces import state_aware_interface


class PromptManager:
    """æç¤ºè©ç®¡ç†å™¨ - ä½¿ç”¨éœæ…‹é…ç½®å’Œå‹•æ…‹æ¨¡çµ„è³‡æ–™æ§‹å»ºæç¤ºè©"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # Load static system instructions from config
        self.system_instructions = config.get("system_instructions", {})
        
        # Cache for built prompts
        self._template_cache = {}
        
        debug_log(2, "[PromptManager] æç¤ºè©ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ˆæ”¯æ´éœæ…‹é…ç½®èˆ‡å‹•æ…‹æ¨¡çµ„è³‡æ–™ï¼‰")
    
    def build_chat_prompt(self, user_input: str, identity_context: Optional[Dict] = None,
                         memory_context: Optional[str] = None, 
                         conversation_history: Optional[List] = None,
                         is_internal: bool = False,
                         relevant_memories: Optional[List[Dict]] = None) -> str:
        """æ§‹å»ºå°è©±æ¨¡å¼æç¤ºè© - æ•´åˆéœæ…‹é…ç½®èˆ‡å‹•æ…‹æ¨¡çµ„è³‡æ–™"""
        
        prompt_parts = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ {system_values} ä½”ä½ç¬¦ï¼ˆåœ¨æ›¿æ›å‰æª¢æŸ¥ï¼‰
        base_has_placeholder = "{system_values}" in self.system_instructions.get("base_personality", "")
        chat_has_placeholder = "{system_values}" in self.system_instructions.get("chat_mode", "")
        
        # åŸºç¤äººæ ¼ï¼ˆç¸½æ˜¯åŒ…å«ï¼Œé™¤éæ˜¯å…§éƒ¨èª¿ç”¨ï¼‰
        if not is_internal and "base_personality" in self.system_instructions:
            base_instruction = self.system_instructions["base_personality"]
            base_instruction = self._replace_system_values_placeholder(base_instruction)
            prompt_parts.append(base_instruction)
        
        # å°è©±æ¨¡å¼ç‰¹å®šæŒ‡ä»¤ï¼ˆéå…§éƒ¨èª¿ç”¨æ‰æ·»åŠ ï¼‰
        if not is_internal and "chat_mode" in self.system_instructions:
            chat_instruction = self.system_instructions["chat_mode"]
            chat_instruction = self._replace_system_values_placeholder(chat_instruction)
            prompt_parts.append(chat_instruction)
        
        # ç³»çµ±ç‹€æ…‹è³‡è¨Šï¼ˆå¦‚æœæŒ‡ä»¤ä¸­æ²’æœ‰ {system_values} ä½”ä½ç¬¦æ‰æ·»åŠ ï¼‰
        if not is_internal and not (base_has_placeholder or chat_has_placeholder):
            status_info = self._build_status_context_with_guide()
            if status_info:
                prompt_parts.append(status_info)
        
        # èº«ä»½è³‡è¨Š
        identity_info = self._build_identity_context(identity_context)
        if identity_info:
            prompt_parts.append(identity_info)
        
        # è¨˜æ†¶ä¸Šä¸‹æ–‡ - å¾ MEM æ¨¡çµ„ç²å–æˆ–ä½¿ç”¨å‚³å…¥çš„å…§å®¹ï¼Œä¸¦æ•´åˆæª¢ç´¢åˆ°çš„è¨˜æ†¶
        memory_section = self._build_memory_context(memory_context, relevant_memories)
        if memory_section:
            prompt_parts.append(memory_section)
        
        # å°è©±æ­·å²
        history_section = self._build_conversation_history(conversation_history)
        if history_section:
            prompt_parts.append(history_section)
        
        # ç”¨æˆ¶è¼¸å…¥
        user_section = f"User: {user_input}"
        prompt_parts.append(user_section)
        
        # å›æ‡‰å¼•å°
        if not is_internal:
            prompt_parts.append("Please respond as U.E.P, considering current mood and status.")
        
        return "\n\n".join(prompt_parts)
    
    def build_work_prompt(self, user_input: str, available_functions: Optional[str] = None,
                         workflow_context: Optional[Dict] = None,
                         identity_context: Optional[Dict] = None,
                         workflow_hint: Optional[str] = None,
                         use_mcp_tools: bool = False,
                         suppress_start_workflow_instruction: bool = False) -> str:
        """æ§‹å»ºå·¥ä½œæ¨¡å¼æç¤ºè© - æ•´åˆç³»çµ±åŠŸèƒ½èˆ‡å·¥ä½œæµä¸Šä¸‹æ–‡
        
        Args:
            suppress_start_workflow_instruction: ç•¶å·²æœ‰å·¥ä½œæµé‹è¡Œæ™‚ï¼ŒæŠ‘åˆ¶ã€Œç«‹å³å•Ÿå‹•å·¥ä½œæµã€çš„å¼·åˆ¶æŒ‡ç¤º
        """
        
        prompt_parts = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ {system_values} ä½”ä½ç¬¦ï¼ˆåœ¨æ›¿æ›å‰æª¢æŸ¥ï¼‰
        base_has_placeholder = "{system_values}" in self.system_instructions.get("base_personality", "")
        work_has_placeholder = "{system_values}" in self.system_instructions.get("work_mode", "")
        
        # åŸºç¤äººæ ¼ï¼ˆç¸½æ˜¯åŒ…å«ï¼‰
        if "base_personality" in self.system_instructions:
            base_instruction = self.system_instructions["base_personality"]
            base_instruction = self._replace_system_values_placeholder(base_instruction)
            prompt_parts.append(base_instruction)
        
        # å·¥ä½œæ¨¡å¼ç‰¹å®šæŒ‡ä»¤
        if "work_mode" in self.system_instructions:
            work_instruction = self.system_instructions["work_mode"]
            work_instruction = self._replace_system_values_placeholder(work_instruction)
            prompt_parts.append(work_instruction)
        
        # ç³»çµ±ç‹€æ…‹ï¼ˆå¦‚æœæŒ‡ä»¤ä¸­æ²’æœ‰ {system_values} ä½”ä½ç¬¦æ‰æ·»åŠ ï¼‰
        if not (base_has_placeholder or work_has_placeholder):
            status_info = self._build_status_context_with_guide(focus_on_work=True)
            if status_info:
                prompt_parts.append(status_info)
        
        # èº«ä»½è³‡è¨Šï¼ˆç°¡åŒ–ç‰ˆï¼‰
        identity_info = self._build_identity_context(identity_context, simplified=True)
        if identity_info:
            prompt_parts.append(identity_info)
        
        # å¯ç”¨ç³»çµ±åŠŸèƒ½ - åªåœ¨ä¸ä½¿ç”¨ MCP tools æ™‚æ‰æ·»åŠ æ–‡å­—æè¿°
        if not use_mcp_tools:
            functions_section = self._build_functions_context(available_functions)
            if functions_section:
                prompt_parts.append(functions_section)
        
        # å·¥ä½œæµä¸Šä¸‹æ–‡
        if workflow_context:
            workflow_section = self._build_workflow_context(workflow_context)
            if workflow_section:
                prompt_parts.append(workflow_section)
        
        # ç”¨æˆ¶è«‹æ±‚
        request_section = f"User Request: {user_input}"
        prompt_parts.append(request_section)
        
        # âœ… å·¥ä½œæ¨¡å¼æŒ‡å¼•ï¼ˆæ ¹æ“šæ˜¯å¦ä½¿ç”¨ MCP tools è€Œä¸åŒï¼‰
        if use_mcp_tools:
            work_guidance = (
                "Available Workflows:\n"
                "- drop_and_read: Read file content (for requests like 'read file', 'show content', 'open file')\n"
                "- intelligent_archive: Smart file organization (for 'organize', 'archive', 'sort files')\n"
                "- summarize_tag: Summarize and tag files (for 'summarize', 'tag', 'analyze files')\n"
                "- file_processing: General file operations (for other file tasks)\n\n"
            )
            
            # âœ… NLP å·¥ä½œæµæç¤ºï¼ˆå¦‚æœæœ‰ï¼‰- æ”¾åœ¨å¯ç”¨å·¥ä½œæµä¹‹å¾Œ
            if workflow_hint:
                if isinstance(workflow_hint, dict):
                    workflow_name = workflow_hint.get('workflow_name', 'unknown')
                    confidence = workflow_hint.get('confidence', 0)
                    work_guidance += (
                        f"**NLP Analysis Result:**\n"
                        f"The system has analyzed the user's request and identified a matching workflow:\n"
                        f"- Recommended workflow: '{workflow_name}'\n"
                        f"- Match confidence: {confidence:.2f}\n"
                        f"- YOU MUST use this workflow name as the 'workflow_type' parameter\n\n"
                    )
                elif isinstance(workflow_hint, str):
                    work_guidance += (
                        f"**NLP Analysis Result:**\n"
                        f"- Recommended workflow: '{workflow_hint}'\n"
                        f"- YOU MUST use this workflow name as the 'workflow_type' parameter\n\n"
                    )
                else:
                    work_guidance += f"**NLP suggests:** {workflow_hint}\n\n"
            
            # âœ… åªåœ¨æ²’æœ‰æ´»èºå·¥ä½œæµæ™‚æ‰æ·»åŠ ã€Œç«‹å³å•Ÿå‹•ã€æŒ‡ç¤º
            if not suppress_start_workflow_instruction:
                work_guidance += (
                    "**CRITICAL INSTRUCTIONS - DO NOT IGNORE:**\n"
                    "1. YOU MUST IMMEDIATELY call the 'start_workflow' function. DO NOT ask for clarification.\n"
                    "2. Required parameters:\n"
                    "   - workflow_type: Use the NLP recommended workflow above (if provided) or select from the available workflows list\n"
                    "   - command: Copy the user's original request exactly as provided\n"
                    "   - initial_data: Use empty dict {} (the workflow will collect missing information)\n"
                    "3. DO NOT respond with plain text asking for more information\n"
                    "4. DO NOT say you need the file name - the workflow will handle file selection\n"
                    "5. The workflow system is designed to collect missing information interactively AFTER it starts\n\n"
                    "**REPEAT: You MUST call start_workflow immediately. Do not ask questions first.**\n\n"
                    "Example:\n"
                    "User: 'read the file'\n"
                    "YOU MUST: call start_workflow(workflow_type='drop_and_read', command='read the file', initial_data={})"
                )
            else:
                # å·²æœ‰å·¥ä½œæµé‹è¡Œï¼Œæª¢æŸ¥æ˜¯å¦ç‚ºæ­¥é©Ÿå›æ‡‰ä¸Šä¸‹æ–‡
                # âœ… ä¿®å¾©ï¼šç¢ºä¿ workflow_context ä¸ç‚º None
                is_step_response = (workflow_context is not None and 
                                   workflow_context.get('type') == 'workflow_step_response')
                
                if is_step_response:
                    # âœ… æ­¥é©Ÿå·²å®Œæˆï¼ŒLLM æ‡‰è©²ç”Ÿæˆå›æ‡‰ï¼Œä¸è¦å‘¼å«å·¥å…·
                    work_guidance += (
                        "\n**Instructions:**\n"
                        "A workflow step has completed. The step data is provided in the context above.\n"
                        "Your task:\n"
                        "1. Read the workflow data from the context\n"
                        "2. Generate a natural, friendly response in ENGLISH explaining the result to the user\n"
                        "3. DO NOT call any MCP tools (review_step, approve_step, etc.) - just provide a text response\n"
                        "4. The workflow context already contains all the data you need\n"
                    )
                else:
                    # å·¥ä½œæµæ­£åœ¨é€²è¡Œä¸­ï¼Œç­‰å¾…ç”¨æˆ¶è¼¸å…¥æˆ–ç³»çµ±æ“ä½œ
                    work_guidance += (
                        "\n**Instructions:**\n"
                        "The workflow is currently running. Based on the situation:\n"
                        "- If you need to check workflow status: use get_workflow_status\n"
                        "- If the workflow is waiting for user input: provide guidance on what's needed\n"
                        "- DO NOT call start_workflow again - a workflow is already active\n"
                    )
        else:
            work_guidance = (
                "Instructions:\n"
                "1. Analyze the user's request against the Available System Functions above\n"
                "2. If the request matches a system function:\n"
                "   - Set sys_action.action to the appropriate action type (start_workflow, execute_function, or provide_options)\n"
                "   - Set sys_action.target to the exact function name from the list\n"
                "   - Provide sys_action.reason explaining why this function matches\n"
                "   - Include any required parameters in sys_action.parameters\n"
                "3. If the request is unclear or needs more information:\n"
                "   - Ask for specific clarification in your text response\n"
                "   - Set sys_action.action to 'provide_options' and sys_action.target to 'clarification'\n"
                "4. Always provide a helpful text response to the user"
            )
        prompt_parts.append(work_guidance)
        
        return "\n\n".join(prompt_parts)
    
    def build_direct_prompt(self, user_input: str) -> str:
        """æ§‹å»ºç›´æ¥æ¨¡å¼æç¤ºè©ï¼ˆæœ€å°åŒ–ï¼‰"""
        return user_input
    
    def _build_status_context_with_guide(self, focus_on_work: bool = False) -> Optional[str]:
        """æ§‹å»ºç³»çµ±ç‹€æ…‹ä¸Šä¸‹æ–‡ - åŒ…å«ç³»çµ±å€¼èªªæ˜ï¼ˆè‹±æ–‡ç‰ˆï¼‰"""
        try:
            modifiers = status_manager.get_personality_modifiers()
            
            # Convert status to English
            status_text = self._format_system_values_english(modifiers)
            
            parts = []
            
            if focus_on_work:
                # Work mode focuses on efficiency-related states
                parts.append(f"Current Status: {status_text}")
            else:
                # Chat mode includes full status information
                parts.append(f"Current Status: {status_text}")
            
            # Add system values explanations from config
            system_values_guide = self.config.get("system_values_guide", {})
            if system_values_guide:
                explanations = []
                for value_name, explanation_text in system_values_guide.items():
                    if explanation_text and isinstance(explanation_text, str):
                        # Clean up the explanation text
                        clean_explanation = explanation_text.strip()
                        if clean_explanation:
                            explanations.append(f"{value_name.upper()}:\n{clean_explanation}")
                
                if explanations:
                    parts.append("System Values Guide:\n" + "\n\n".join(explanations))
            
            return "\n\n".join(parts)
                
        except Exception as e:
            error_log(f"[PromptManager] æ§‹å»ºç‹€æ…‹ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return None
    
    def _format_system_values_english(self, modifiers: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç³»çµ±å€¼ç‚ºè‹±æ–‡æ ¼å¼"""
        status_mapping = {
            "éå¸¸ç©æ¥µ": "very positive", "ç©æ¥µ": "positive", "ä¸­æ€§": "neutral",
            "æ¶ˆæ¥µ": "negative", "éå¸¸æ¶ˆæ¥µ": "very negative",
            "éå¸¸è‡ªä¿¡": "very confident", "è‡ªä¿¡": "confident", "æ™®é€š": "normal", 
            "æ²’æœ‰è‡ªä¿¡": "not confident", "éå¸¸æ²’æœ‰è‡ªä¿¡": "very unconfident",
            "éå¸¸é¡˜æ„å¹«åŠ©": "very helpful", "é¡˜æ„å¹«åŠ©": "helpful", "ä¸­ç­‰": "moderate",
            "ä¸å¤ªé¡˜æ„å¹«åŠ©": "less helpful", "ä¸é¡˜æ„å¹«åŠ©": "unhelpful",
            "éå¸¸ç„¡èŠ": "very bored", "ç„¡èŠ": "bored", "æœ‰é»ç„¡èŠ": "slightly bored",
            "ä¸ç„¡èŠ": "not bored", "æ„Ÿèˆˆè¶£": "interested"
        }
        
        mood_en = status_mapping.get(modifiers['mood_level'], modifiers['mood_level'])
        pride_en = status_mapping.get(modifiers['pride_level'], modifiers['pride_level'])
        helpfulness_en = status_mapping.get(modifiers['helpfulness_level'], modifiers['helpfulness_level'])
        boredom_en = status_mapping.get(modifiers['boredom_level'], modifiers['boredom_level'])
        
        return f"mood={mood_en}, pride={pride_en}, helpfulness={helpfulness_en}, boredom={boredom_en}"
    
    def _build_memory_context(self, memory_context: Optional[str] = None, 
                             relevant_memories: Optional[List[Dict]] = None) -> Optional[str]:
        """æ§‹å»ºè¨˜æ†¶ä¸Šä¸‹æ–‡å€æ®µ - æ•´åˆæª¢ç´¢åˆ°çš„è¨˜æ†¶"""
        context_parts = []
        
        # åŸæœ‰çš„è¨˜æ†¶ä¸Šä¸‹æ–‡
        if memory_context:
            context_parts.append(f"Memory Context:\n{memory_context}")
        
        # æ–°çš„æª¢ç´¢è¨˜æ†¶
        if relevant_memories:
            memory_text_parts = ["Retrieved Relevant Memories:"]
            for i, memory in enumerate(relevant_memories, 1):
                memory_type = memory.get("type", "general")
                content = memory.get("content", "")
                
                if memory_type == "conversation":
                    user_input = memory.get("user_input", "")
                    assistant_response = memory.get("assistant_response", "")
                    memory_text_parts.append(f"{i}. [Conversation] User: {user_input} | Assistant: {assistant_response}")
                elif memory_type == "user_info":
                    memory_text_parts.append(f"{i}. [User Info] {content}")
                else:
                    memory_text_parts.append(f"{i}. [{memory_type}] {content}")
                    
            context_parts.append("\n".join(memory_text_parts))
        
        # å¦‚æœéƒ½æ²’æœ‰ï¼Œå˜—è©¦å¾ MEM æ¨¡çµ„ç²å– (å‘å¾Œå…¼å®¹)
        if not context_parts:
            try:
                mem_data = state_aware_interface.get_chat_mem_data("context")
                if mem_data and isinstance(mem_data, dict):
                    mem_memories = mem_data.get("relevant_memories", "")
                    if mem_memories:
                        context_parts.append(f"Relevant Memory:\n{mem_memories}")
            except Exception as e:
                debug_log(1, f"[PromptManager] ç„¡æ³•å¾ MEM æ¨¡çµ„ç²å–è¨˜æ†¶è³‡æ–™: {e}")
        
        return "\n\n".join(context_parts) if context_parts else None
    
    def _build_functions_context(self, available_functions: Optional[str] = None) -> Optional[str]:
        """æ§‹å»ºç³»çµ±åŠŸèƒ½ä¸Šä¸‹æ–‡ - å„ªå…ˆä½¿ç”¨å‚³å…¥å…§å®¹ï¼Œå¦å‰‡å¾ SYS æ¨¡çµ„ç²å–"""
        if available_functions:
            return f"Available System Functions:\n{available_functions}"
        
        # Try to get functions data from SYS module
        try:
            sys_data = state_aware_interface.get_work_sys_data("function_registry")
            if sys_data:
                if isinstance(sys_data, (list, tuple, set)):
                    return "Available System Functions:\n" + "\n".join(map(str, sys_data))
                if isinstance(sys_data, dict) and sys_data.get("available_functions"):
                    return "Available System Functions:\n" + "\n".join(sys_data["available_functions"])
            return None
        except Exception as e:
            debug_log(1, f"[PromptManager] ç„¡æ³•å¾ SYS æ¨¡çµ„ç²å–åŠŸèƒ½è³‡æ–™: {e}")
        
        return None
    
    def _build_identity_context(self, identity_context: Optional[Dict], 
                               simplified: bool = False) -> Optional[str]:
        """æ§‹å»ºèº«ä»½ä¸Šä¸‹æ–‡"""
        if not identity_context:
            return None
        
        try:
            identity = identity_context.get("identity", {})
            preferences = identity_context.get("preferences", {})
            
            if simplified:
                # Simplified version with basic info only
                name = identity.get("name", "User")
                return f"User: {name}"
            else:
                # Full version with preferences
                parts = []
                
                name = identity.get("name", "User")
                parts.append(f"User: {name}")
                
                # Conversation preferences
                conversation_prefs = preferences.get("conversation", {})
                if conversation_prefs:
                    formality = conversation_prefs.get("formality", "neutral")
                    verbosity = conversation_prefs.get("verbosity", "moderate")
                    parts.append(f"Preferences: {formality} formality, {verbosity} verbosity")
                
                return "; ".join(parts)
                
        except Exception as e:
            error_log(f"[PromptManager] æ§‹å»ºèº«ä»½ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return None
    
    def _build_conversation_history(self, history: Optional[List]) -> Optional[str]:
        """æ§‹å»ºå°è©±æ­·å²ä¸Šä¸‹æ–‡"""
        if not history or len(history) == 0:
            return None
        
        try:
            # Only take recent conversations
            recent_history = history[-3:] if len(history) > 3 else history
            
            history_parts = []
            for entry in recent_history:
                # è™•ç† ConversationEntry å°è±¡æˆ–å­—å…¸
                if hasattr(entry, 'role') and hasattr(entry, 'content'):
                    # é€™æ˜¯ ConversationEntry å°è±¡
                    role = entry.role
                    content = entry.content
                elif isinstance(entry, dict):
                    # é€™æ˜¯å­—å…¸æ ¼å¼
                    role = entry.get("role", "unknown")
                    content = entry.get("content", "")
                else:
                    # æœªçŸ¥æ ¼å¼ï¼Œè·³é
                    continue
                
                if role == "user":
                    history_parts.append(f"User: {content}")
                elif role == "assistant":
                    history_parts.append(f"U.E.P: {content}")
            
            if history_parts:
                return "Recent Conversation:\n" + "\n".join(history_parts)
                
        except Exception as e:
            error_log(f"[PromptManager] æ§‹å»ºå°è©±æ­·å²å¤±æ•—: {e}")
            
        return None
    
    def _build_workflow_context(self, workflow_context: Dict) -> Optional[str]:
        """æ§‹å»ºå·¥ä½œæµä¸Šä¸‹æ–‡"""
        try:
            # ğŸ†• æª¢æŸ¥æ˜¯å¦ç‚ºå·¥ä½œæµæ­¥é©Ÿå›æ‡‰é¡å‹
            context_type = workflow_context.get("type")
            
            if context_type == "workflow_step_response":
                # é€™æ˜¯å·¥ä½œæµæ­¥é©Ÿå®Œæˆï¼Œéœ€è¦ LLM ç”Ÿæˆç”¨æˆ¶å›æ‡‰
                return self._build_workflow_step_response_context(workflow_context)
            
            # åŸæœ‰é‚è¼¯ï¼šå·¥ä½œæµé€²åº¦è¿½è¹¤
            parts = []
            
            # Current step
            current_step = workflow_context.get("current_step")
            if current_step:
                parts.append(f"Current Step: {current_step}")
            
            # Previous result
            previous_result = workflow_context.get("previous_result")
            if previous_result:
                parts.append(f"Previous Result: {previous_result}")
            
            # Remaining steps
            remaining_steps = workflow_context.get("remaining_steps", [])
            if remaining_steps:
                steps_text = ", ".join(remaining_steps[:3])  # Show only first 3
                parts.append(f"Remaining Steps: {steps_text}")
            
            if parts:
                return "Workflow Progress: " + "; ".join(parts)
                
        except Exception as e:
            error_log(f"[PromptManager] æ§‹å»ºå·¥ä½œæµä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            
        return None
    
    def _build_workflow_step_response_context(self, workflow_context: Dict) -> str:
        """
        æ§‹å»ºå·¥ä½œæµæ­¥é©Ÿå›æ‡‰çš„ä¸Šä¸‹æ–‡ï¼ˆé€šç”¨æ¡†æ¶æ¨¡å¼ï¼‰
        
        é€™å€‹æ–¹æ³•ç”Ÿæˆé€šç”¨çš„æŒ‡å¼•ï¼Œè®“ LLM èƒ½å¤ è™•ç†ä»»ä½•é¡å‹çš„å·¥ä½œæµæ­¥é©Ÿæ•¸æ“š
        è€Œä¸æ˜¯é‡å°ç‰¹å®šå·¥ä½œæµç¡¬ç·¨ç¢¼é‚è¼¯
        
        Args:
            workflow_context: å·¥ä½œæµä¸Šä¸‹æ–‡æ•¸æ“š
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        workflow_type = workflow_context.get('workflow_type', 'unknown')
        is_complete = workflow_context.get('is_complete', False)
        should_end_session = workflow_context.get('should_end_session', False)
        review_data = workflow_context.get('review_data', {})
        step_result = workflow_context.get('step_result', {})
        
        context_parts = []
        
        # åŸºç¤è³‡è¨Š
        context_parts.append("=" * 60)
        context_parts.append("WORKFLOW STEP COMPLETED - GENERATE USER RESPONSE")
        context_parts.append("=" * 60)
        
        context_parts.append(f"\nWorkflow Type: {workflow_type}")
        context_parts.append(f"Step Status: {'COMPLETED (Final)' if is_complete else 'IN PROGRESS'}")
        
        # æ­¥é©Ÿçµæœè³‡è¨Š
        if step_result:
            context_parts.append(f"\nStep Result:")
            if 'success' in step_result:
                context_parts.append(f"  - Success: {step_result['success']}")
            if 'message' in step_result:
                context_parts.append(f"  - Message: {step_result['message']}")
        
        # å·¥ä½œæµæ•¸æ“šï¼ˆé€šç”¨è™•ç†ï¼‰
        if review_data:
            context_parts.append(f"\nWorkflow Data:")
            for key, value in review_data.items():
                # æ ¼å¼åŒ–ä¸åŒé¡å‹çš„æ•¸æ“š
                if isinstance(value, str):
                    # é•·æ–‡æœ¬æˆªæ–·é è¦½
                    if len(value) > 200:
                        preview = value[:200] + f"... ({len(value)} chars total)"
                        context_parts.append(f"  - {key}: {preview}")
                    else:
                        context_parts.append(f"  - {key}: {value}")
                elif isinstance(value, (int, float, bool)):
                    context_parts.append(f"  - {key}: {value}")
                elif isinstance(value, (list, dict)):
                    context_parts.append(f"  - {key}: {type(value).__name__} with {len(value)} items")
                else:
                    context_parts.append(f"  - {key}: {str(value)[:100]}")
        
        # ğŸ”§ é€šç”¨æŒ‡å¼•ï¼ˆæ¡†æ¶æ¨¡å¼ï¼Œä¸é‡å°ç‰¹å®šå·¥ä½œæµï¼‰
        context_parts.append("\n" + "=" * 60)
        context_parts.append("YOUR TASK:")
        context_parts.append("=" * 60)
        
        if is_complete:
            context_parts.append("\nâœ… The workflow has completed successfully.")
            context_parts.append("\nGenerate a natural, friendly response in ENGLISH that:")
            context_parts.append("1. Acknowledges the workflow completion")
            context_parts.append("2. Summarizes the key results/data provided above")
            context_parts.append("3. If the data contains content in another language (e.g., Chinese):")
            context_parts.append("   - Translate or explain it naturally in English")
            context_parts.append("   - Focus on the main points and key information")
            context_parts.append("4. Keep your response conversational and concise (2-4 sentences)")
            context_parts.append("5. Maintain your U.E.P. personality")
            
            if should_end_session:
                context_parts.append("\nâš ï¸ IMPORTANT: This is the final response.")
                context_parts.append("   Set session_control={'action': 'end_session'} in metadata")
        else:
            context_parts.append("\nâ³ The workflow is in progress.")
            context_parts.append("\nGenerate a brief progress update in ENGLISH that:")
            context_parts.append("1. Acknowledges the current step completion")
            context_parts.append("2. Briefly mentions what's happening or what will happen next")
            context_parts.append("3. Keep it short and reassuring (1-2 sentences)")
        
        context_parts.append("\n" + "=" * 60)
        context_parts.append("LANGUAGE REQUIREMENT:")
        context_parts.append("=" * 60)
        context_parts.append("âš ï¸ CRITICAL: Always respond in ENGLISH, regardless of the")
        context_parts.append("   original language in the workflow data.")
        context_parts.append("   Your role is to translate/interpret the data for the user.")
        context_parts.append("=" * 60)
        
        return "\n".join(context_parts)
    
    def _replace_system_values_placeholder(self, instruction: str) -> str:
        """æ›¿æ›ç³»çµ±æŒ‡ä»¤ä¸­çš„ {system_values} ä½”ä½ç¬¦"""
        if "{system_values}" not in instruction:
            return instruction
        
        try:
            # Use the same status context building logic
            status_context = self._build_status_context_with_guide()
            system_values_text = status_context if status_context else ""
            
            return instruction.replace("{system_values}", system_values_text)
            
        except Exception as e:
            error_log(f"[PromptManager] æ›¿æ›ç³»çµ±å€¼ä½”ä½ç¬¦å¤±æ•—: {e}")
            return instruction.replace("{system_values}", "")
    
    def get_system_instruction(self, mode: str = "main") -> str:
        """ç²å–ç³»çµ±æŒ‡ä»¤"""
        return self.system_instructions.get(mode, self.system_instructions.get("main", ""))
    
    def update_system_instruction(self, mode: str, instruction: str):
        """æ›´æ–°ç³»çµ±æŒ‡ä»¤"""
        self.system_instructions[mode] = instruction
        debug_log(2, f"[PromptManager] æ›´æ–°ç³»çµ±æŒ‡ä»¤: {mode}")
    
    def register_mem_provider(self, data_type: str, provider_func: Callable):
        """è¨»å†Š MEM æ¨¡çµ„è³‡æ–™æä¾›è€…"""
        state_aware_interface.register_chat_mem_provider(data_type, provider_func)
        debug_log(2, f"[PromptManager] MEM æ¨¡çµ„è³‡æ–™æä¾›è€…å·²è¨»å†Š: {data_type}")
    
    def register_sys_provider(self, data_type: str, provider_func: Callable):
        """è¨»å†Š SYS æ¨¡çµ„è³‡æ–™æä¾›è€…"""
        state_aware_interface.register_work_sys_provider(data_type, provider_func)
        debug_log(2, f"[PromptManager] SYS æ¨¡çµ„è³‡æ–™æä¾›è€…å·²è¨»å†Š: {data_type}")
    
    def get_template_stats(self) -> Dict[str, Any]:
        """ç²å–æ¨¡æ¿ä½¿ç”¨çµ±è¨ˆ"""
        data_info = state_aware_interface.get_available_data_types()
        return {
            "cached_templates": len(self._template_cache),
            "available_instructions": list(self.system_instructions.keys()),
            "mem_providers_count": len(data_info.get("chat_mem_providers", [])),
            "sys_providers_count": len(data_info.get("work_sys_providers", [])),
            "total_providers": data_info.get("total_providers", 0)
        }