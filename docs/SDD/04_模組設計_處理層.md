# 第四章:模組設計 - 處理層 (Module Design - Processing Layer)

[← 上一章:模組設計 - 輸入層](./03_模組設計_輸入層.md) | [返回主文件](../System%20Design%20Documentation.md) | [下一章:模組設計 - 輸出層 →](./05_模組設計_輸出層.md)

---

## 4.1 MEM 模組 (Memory Management Module)

**檔案位置**: `modules/mem_module/`  
**職責**: 身份隔離記憶管理、語義檢索、對話快照系統

### 4.1.1 模組架構

```
mem_module/
├── mem_module.py                    # 主模組類
├── __init__.py                      # 註冊函數
├── schemas.py                       # Input/Output Schemas
├── config.yaml                      # 模組配置
│
├── identity_manager.py              # 身份管理 (MEM 層)
├── semantic_retriever.py            # 語義檢索引擎
├── memory_analyzer.py               # 記憶分析器
├── snapshot_manager.py              # 對話快照管理
├── learning_data_manager.py         # 學習資料管理
│
└── faiss_index/                     # FAISS 索引檔案
```

### 4.1.2 核心技術棧

| 技術組件 | 版本/模型 | 用途 |
|:---|:---|:---|
| **Sentence Transformers** | `all-MiniLM-L6-v2` | 文本嵌入 (384-dim) |
| **FAISS** | `faiss-cpu==1.9.0.post1` | 向量相似度檢索 |
| **Index Type** | `IndexFlatL2` | L2 距離檢索 |
| **Identity Manager** | Custom Implementation | 身份令牌管理 |
| **Snapshot Manager** | Custom Implementation | 對話快照系統 |

### 4.1.3 核心功能

1. **身份隔離記憶系統**: 基於 Memory Token 實現多用戶記憶隔離
2. **分層記憶管理**: 短期記憶 (Snapshot) / 長期記憶 (Profile, Preference) 分層
3. **語義檢索**: FAISS + Sentence Transformers 實現相似度搜索
4. **對話快照系統**: 自動記錄與管理對話上下文
5. **CHAT_MEM 協作通道**: 提供記憶上下文給 LLM Module

### 4.1.4 設計理念

#### 為何需要身份隔離？
- 多用戶系統需要確保記憶不會跨身份洩漏
- 每個身份有獨立的 `memory_token` 作為存取控制
- 匿名用戶支援 (Anonymous Token)

#### 為何分層管理？
- **短期記憶 (Snapshot)**: 對話上下文,生命週期與 Chatting Session 綁定
- **長期記憶 (Profile/Preference)**: 用戶特質,持久化儲存
- 不同記憶類型有不同的重要性與生命週期

#### 為何使用 FAISS？
- 支持大規模記憶庫 (10萬+ entries)
- L2 距離檢索比 Cosine 更適合 384-dim 向量
- 高效向量檢索 (毫秒級)

### 4.1.5 Input/Output Schemas

```python
@dataclass
class MEMInput(BaseModel):
    """MEM 模組輸入"""
    query_text: str                     # 查詢文本
    memory_token: str                   # 身份令牌
    memory_types: Optional[List[MemoryType]] = None  # 記憶類型過濾
    max_results: int = 10               # 最大檢索數量
    similarity_threshold: float = 0.7   # 相似度閾值

@dataclass
class MEMOutput(BaseModel):
    """MEM 模組輸出"""
    memories: List[MemoryEntry]         # 檢索到的記憶
    summary: str                        # 記憶總結 (供 LLM 使用)
    total_retrieved: int                # 檢索數量
    
class MemoryType(Enum):
    """記憶類型"""
    SNAPSHOT = "snapshot"               # 對話快照
    LONG_TERM = "long_term"             # 長期記憶
    PROFILE = "profile"                 # 用戶檔案
    PREFERENCE = "preference"           # 用戶偏好
    LEARNING_DATA = "learning_data"     # 學習資料
```

### 4.1.6 身份隔離系統

#### Identity Manager (MEM 層)

**與 NLP 層 IdentityManager 的區別**:
- **NLP 層**: 說話者 ID → 使用者身份映射
- **MEM 層**: 身份 → Memory Token 管理,記憶存取控制

#### Memory Token 結構

| 身份類型 | Token 格式 | 範例 |
|:---|:---|:---|
| 已確認用戶 | `mem_{identity_id}_{hash}` | `mem_alice_a8f3c2` |
| 匿名用戶 | `mem_anonymous_{session_id}` | `mem_anonymous_gs123` |
| 系統 | `mem_system_core` | `mem_system_core` |

#### 存取控制流程

```
查詢記憶請求 (含 memory_token)
    ↓
Identity Manager 驗證 Token
    ├─ 有效 → 允許存取
    └─ 無效 → 拒絕 (返回空結果)
    ↓
FAISS 檢索 (僅該 Token 的記憶)
```

### 4.1.7 語義檢索系統

#### Semantic Retriever 架構

**處理流程**:
```
查詢文本 → Sentence Transformer 嵌入 (384-dim)
    ↓
FAISS IndexFlatL2 檢索
    ↓
相似度過濾 (threshold: 0.7)
    ↓
身份過濾 (memory_token)
    ↓
記憶類型過濾 (memory_types)
    ↓
Top-K 結果排序
```

#### 檢索策略

| 策略 | 說明 | 適用場景 |
|:---|:---|:---|
| **Semantic Search** | 語義相似度檢索 | 日常對話 |
| **Recent First** | 最近記憶優先 | 短期上下文 |
| **Importance Weighted** | 重要性加權 | 關鍵資訊檢索 |
| **Hybrid** | 組合多種策略 | 複雜查詢 |

#### 關鍵參數

| 參數 | 預設值 | 說明 |
|:---|:---|:---|
| `similarity_threshold` | `0.7` | 相似度閾值 (0-1) |
| `max_distance` | `0.85` | L2 距離上限 |
| `max_results` | `10` | 最大檢索數量 |
| `embedding_dim` | `384` | 向量維度 |

### 4.1.8 對話快照系統

#### Snapshot Manager 設計理念

**為何需要快照系統？**
- 對話上下文需要保留完整性,避免片段化
- 自動決策何時創建新快照 vs 延續現有快照
- 與 Chatting Session 生命週期綁定

#### 快照生命週期

```
Chatting Session 創建
    ↓
Snapshot Manager 決策:
    ├─ 新主題? → 創建新快照
    └─ 延續? → 更新現有快照
    ↓
對話輪次累積 (turns)
    ↓
CS 結束 → 快照封存
```

#### 快照決策邏輯

| 條件 | 決策 | 理由 |
|:---|:---|:---|
| 主題轉換 (語義距離 > 0.6) | 創建新快照 | 避免主題混淆 |
| 時間間隔 > 5 分鐘 | 創建新快照 | 新對話上下文 |
| 延續現有對話 | 更新現有快照 | 保持連貫性 |

### 4.1.9 CHAT_MEM 協作通道

#### 資料提供流程

```
MEM Module 完成檢索
    ↓
記憶總結 (Memory Summarizer)
    - 提取關鍵資訊
    - 去重與排序
    - 生成簡潔摘要
    ↓
Working Context 註冊資料:
channel_id = "CHAT_MEM"
data = {
    "memories": [...],
    "summary": "用戶 Alice 喜歡...",
    "snapshot_context": "上次對話提到..."
}
    ↓
LLM Module 消費
```

### 4.1.10 記憶儲存結構

#### 資料持久化

```
memory/
├── faiss_index                      # FAISS 索引
├── mem_metadata.json                # 記憶元資料
├── identities/                      # 身份資料
│   ├── alice.json
│   └── bob.json
├── session_records.json             # 會話記錄
└── learning_data/                   # 學習資料
    └── preferences.json
```

#### MemoryEntry 結構

```python
{
    "memory_id": "mem_20251107_001",
    "memory_token": "mem_alice_a8f3c2",
    "memory_type": "snapshot",
    "importance": "medium",
    "content": "用戶提到喜歡藍色",
    "embedding": [0.123, -0.456, ...],  # 384-dim
    "timestamp": 1699344000.0,
    "metadata": {
        "snapshot_id": "snap_001",
        "session_id": "cs_abc123"
    }
}
```

### 4.1.11 配置參數

| 參數 | 預設值 | 說明 |
|:---|:---|:---|
| `embedding_model` | `all-MiniLM-L6-v2` | Sentence Transformer 模型 |
| `faiss_index_type` | `IndexFlatL2` | FAISS 索引類型 |
| `similarity_threshold` | `0.7` | 檢索相似度閾值 |
| `max_memory_entries` | `100000` | 最大記憶條目數 |
| `snapshot_theme_threshold` | `0.6` | 主題轉換閾值 |
| `snapshot_time_gap` | `300` | 快照時間間隔 (秒) |

### 4.1.12 已知問題與改進方向

#### 已知問題
1. **FAISS IndexFlatL2 效能**: 大規模記憶庫 (10萬+) 檢索速度下降
   - **改進方向**: 升級至 IndexIVFFlat (分區索引)

2. **記憶總結品質**: 簡單拼接,缺乏語義壓縮
   - **改進方向**: 整合 LLM 進行智能總結

#### 改進方向
- 支援向量量化 (Product Quantization) 降低記憶體使用
- 實作記憶遺忘機制 (過期記憶自動清理)
- 多模態記憶支援 (圖片、語音記憶)

---

## 4.2 LLM 模組 (Large Language Model Module)

**檔案位置**: `modules/llm_module/`  
**職責**: 大語言模型整合、工具調用、學習引擎

### 4.2.1 模組架構

```
llm_module/
├── llm_module.py                    # 主模組類
├── __init__.py                      # 註冊函數
├── schemas.py                       # Input/Output Schemas
├── config.yaml                      # 模組配置
│
├── mcp_client.py                    # MCP Client (工具調用)
├── learning_engine.py               # 累積評分學習引擎
├── prompt_builder.py                # Prompt 構建器
├── response_parser.py               # 響應解析器
└── context_cache_manager.py         # Context Cache 管理
```

### 4.2.2 核心技術棧

| 技術組件 | 版本/模型 | 用途 |
|:---|:---|:---|
| **Google Gemini** | `gemini-2.0-flash-exp` | 大語言模型 |
| **Vertex AI** | Google Cloud | LLM 服務 |
| **Context Caching** | Gemini 1.5+ | 系統提示詞快取 |
| **MCP Protocol** | v1.0 | 工具調用標準 |
| **Structured Output** | JSON Schema | 結構化響應 |
| **Learning Engine** | Custom | 累積評分學習 |

### 4.2.3 核心功能

1. **Gemini 整合**: Google Vertex AI 雲端模型調用
2. **MCP Client 工具調用**: 透過 Model Context Protocol 與 SYS 模組協作
3. **Context Caching**: 系統提示詞與功能列表快取 (降低成本 90%)
4. **學習引擎**: 累積評分制學習用戶對話風格與偏好
5. **CHAT/WORK 模式分離**: 日常對話與工作任務的不同處理路徑

### 4.2.4 設計理念

#### 為何選擇 Gemini？
- **超長上下文**: 1M tokens 上下文視窗
- **原生 Function Calling**: 無需額外訓練
- **Context Caching**: 系統提示詞快取節省成本
- **Structured Output**: 原生支援 JSON Schema 驗證

#### 為何使用 Context Caching？
- **成本優勢**: 快取 Token 成本降低 90% ($0.01/1M → $0.001/1M)
- **高重複率**: 系統提示詞、功能列表在每次調用中重複
- **效能提升**: 快取命中減少 Latency

#### 為何需要累積評分學習？
- **穩定性**: 比單次反饋更穩定,避免過擬合
- **漸進式**: 使用指數移動平均 (EMA) 平滑學習曲線
- **低成本**: 無需額外模型訓練

### 4.2.5 Input/Output Schemas

```python
@dataclass
class LLMInput(BaseModel):
    """LLM 模組輸入"""
    mode: LLMMode                       # CHAT | WORK
    text: str                           # 用戶輸入
    identity_context: Dict[str, Any]    # 身份資訊
    memory_context: Optional[str]       # MEM 提供的記憶
    workflow_context: Optional[Dict]    # 工作流狀態 (WORK 模式)
    
class LLMMode(Enum):
    """LLM 模式"""
    CHAT = "chat"                       # 日常對話
    WORK = "work"                       # 工作流任務

@dataclass
class LLMOutput(BaseModel):
    """LLM 模組輸出"""
    text: str                           # 生成文本
    sys_action: Optional[SystemAction]  # 系統功能調用
    status_updates: Optional[StatusUpdate]  # 情緒狀態更新
    learning_data: Optional[LearningData]   # 學習信號
    memory_observation: Optional[str]   # 記憶觀察
    session_control: Dict               # 會話控制建議
```

### 4.2.6 Gemini 整合

#### API 調用流程

```
LLM Module 接收輸入
    ↓
Prompt Builder 構建 Prompt:
    ├─ 系統角色 (Cached)
    ├─ 功能列表 (Cached)
    ├─ 身份資訊
    ├─ 記憶上下文 (MEM)
    └─ 用戶輸入
    ↓
Vertex AI API 調用:
    - Context Cache 命中檢查
    - Function Calling (MCP tools)
    - Structured Output 驗證
    ↓
Response Parser 解析響應:
    - 提取生成文本
    - 解析 Function Call
    - 提取 Status Updates
    - 提取 Learning Signals
    ↓
返回 LLMOutput
```

#### Context Cache 管理

| 快取內容 | TTL | 大小 | 更新頻率 |
|:---|:---|:---|:---|
| 系統角色提示詞 | 3600s | ~2K tokens | 系統更新時 |
| MCP 功能列表 | 3600s | ~5K tokens | 新功能添加時 |
| 身份檔案 (Profile) | 1800s | ~1K tokens | 用戶資料更新時 |

**快取策略**:
- 最小輸入要求: 32K tokens (Gemini 限制)
- 自動續期: TTL 剩餘 < 10% 時更新
- 失效處理: Cache miss 時回退至標準調用

### 4.2.7 MCP Client 工具調用

#### MCP Protocol 整合

**為何使用 MCP？**
- **標準化**: 工具調用介面標準化
- **解耦合**: LLM 與系統功能實現分離
- **可擴展**: 新功能僅需添加 MCP 工具定義

#### 工具調用流程

```
Gemini 返回 Function Call
    ↓
MCP Client 解析:
tool_name = "start_workflow"
arguments = {"type": "drop_and_read", "files": [...]}
    ↓
MCP Client → SYS Module MCP Server:
    - 序列化參數
    - 異步調用
    - 等待結果
    ↓
SYS Module 執行 → 返回結果
    ↓
MCP Client → Gemini (Function Result):
    - 結果嵌入 Prompt
    - 續寫生成
    ↓
最終響應
```

#### 可用 MCP 工具

| 工具名稱 | 用途 | 參數 |
|:---|:---|:---|
| `start_workflow` | 啟動工作流 | `type`, `params` |
| `approve_step` | 批准工作流步驟 | `workflow_id`, `step_id` |
| `modify_step` | 修改步驟參數 | `workflow_id`, `modifications` |
| `cancel_workflow` | 取消工作流 | `workflow_id` |
| `query_workflow_status` | 查詢工作流狀態 | `workflow_id` |

### 4.2.8 學習引擎

#### 累積評分學習機制

**設計理念**:
- 使用**指數移動平均 (EMA)** 平滑學習曲線
- 累積正負反饋,避免單次波動
- 無需額外模型訓練,成本低

#### EMA 公式

```
new_score = α × current_feedback + (1 - α) × old_score

其中:
- α = 0.1 (學習率)
- current_feedback ∈ [-1, 1] (負面到正面)
- old_score: 歷史累積分數
```

#### 學習信號來源

| 信號類型 | 權重 | 範例 |
|:---|:---|:---|
| 明確反饋 (用戶評分) | 1.0 | "這個回答很好" → +0.8 |
| 隱式反饋 (對話延續) | 0.3 | 用戶繼續對話 → +0.3 |
| 負面信號 (重複詢問) | -0.5 | 用戶重複同一問題 → -0.5 |
| 會話結束 (自然) | 0.2 | 正常結束對話 → +0.2 |

#### 學習資料儲存

```json
{
    "identity_id": "alice",
    "preferences": {
        "formality": 0.65,      // EMA 累積分數 (0=casual, 1=formal)
        "detail_level": 0.42,   // 0=concise, 1=detailed
        "humor": 0.78           // 0=serious, 1=humorous
    },
    "conversation_style": {
        "greeting_preference": "informal",
        "topic_interests": ["technology", "music"]
    }
}
```

### 4.2.9 CHAT/WORK 模式分離

#### CHAT 模式處理

**流程**:
```
CHAT 輸入 → Prompt 構建:
    - 系統角色: "你是 U.E.P,一個友善的 AI 助手..."
    - 記憶上下文: MEM 檢索結果
    - 身份偏好: Learning Engine 資料
    ↓
Gemini 生成 → 自然對話回應
    ↓
Status Updates: 情緒狀態更新
Learning Signals: 收集反饋
```

#### WORK 模式處理

**流程**:
```
WORK 輸入 → Prompt 構建:
    - 系統角色: "你是工作流協調者..."
    - 工作流狀態: SYS 提供的 WORK_SYS 資料
    - MCP 工具列表: 可用功能
    ↓
Gemini 生成 → Function Call (MCP):
    - 解析用戶任務
    - 決定調用哪些工具
    - 生成執行計劃
    ↓
MCP Client 執行 → SYS Module
    ↓
結果返回 → 續寫生成最終響應
```

### 4.2.10 配置參數

| 參數 | 預設值 | 說明 |
|:---|:---|:---|
| `model_name` | `gemini-2.0-flash-exp` | Gemini 模型版本 |
| `project_id` | (GCP 專案 ID) | Vertex AI 專案 |
| `location` | `asia-east1` | 模型部署區域 |
| `max_tokens` | `2048` | 最大生成 Token 數 |
| `temperature` | `0.7` | 生成隨機性 |
| `context_cache_ttl` | `3600` | 快取存活時間 (秒) |
| `learning_rate` | `0.1` | EMA 學習率 (α) |

### 4.2.11 已知問題與改進方向

#### 已知問題
1. **Context Cache 最小限制**: Gemini 要求最小 32K tokens,小專案無法充分利用
2. **Learning Engine 簡單**: 僅累積評分,缺乏細粒度風格學習
3. **Function Calling 延遲**: 多輪工具調用增加響應時間

#### 改進方向
- 整合更細粒度的風格學習模型
- 支援多模態輸入 (圖片、語音)
- 實作 Streaming Response (逐字輸出)

---

## 4.3 SYS 模組 (System Functionality Module)

**檔案位置**: `modules/sys_module/`  
**職責**: 工作流引擎、MCP Server、系統功能實現

### 4.3.1 模組架構

```
sys_module/
├── sys_module.py                    # 主模組類
├── __init__.py                      # 註冊函數
├── schemas.py                       # Input/Output Schemas
├── config.yaml                      # 模組配置
│
├── workflow_engine.py               # 工作流引擎
├── mcp_server.py                    # MCP Server
├── background_executor.py           # 背景執行器
├── file_workflows.py                # 檔案工作流
├── functions.yaml                   # 功能定義
│
└── workflows/                       # 工作流定義
    ├── drop_and_read.py
    ├── archive_files.py
    └── summarize_tags.py
```

### 4.3.2 核心技術棧

| 技術組件 | 用途 |
|:---|:---|
| **ThreadPoolExecutor** | 背景工作流執行 (max_workers=4) |
| **YAML 函數註冊** | 功能定義與實現分離 |
| **MCP Protocol** | 標準化工具調用接口 |
| **Event Bus** | 異步工作流通知 |
| **Workflow Session** | 工作流生命週期管理 |

### 4.3.3 核心功能

1. **Workflow Engine**: 多步驟任務編排與狀態管理
2. **MCP Server**: 提供工具調用接口給 LLM
3. **Background Executor**: 非阻塞工作流執行
4. **File Workflows**: 檔案讀取、智能歸檔、標籤摘要
5. **Session-Aware 執行**: 與 Workflow Session 整合的生命週期管理

### 4.3.4 設計理念

#### 為何需要 Workflow Engine？
- 複雜任務需要多步驟協調 (如檔案歸檔: 讀取 → 分析 → 移動 → 總結)
- 狀態機保證可靠性 (支援暫停、恢復、回滾)
- 支援 LLM 審核決策 (Interactive Steps)

#### 為何使用 MCP Server？
- LLM 需要結構化的工具調用接口,而非自然語言解析
- 標準化接口便於 LLM 理解與調用
- 解耦 LLM 與系統功能實現

#### 為何背景執行？
- 長時間任務 (如檔案處理) 不應阻塞主循環
- 支援並發執行多個工作流
- ThreadPoolExecutor 限制並發數,避免資源耗盡

#### 為何 YAML 註冊？
- 功能定義與實現分離,便於動態擴展
- LLM 可直接讀取功能列表 (Context Cache)
- 便於非開發人員新增功能

### 4.3.5 Input/Output Schemas

```python
@dataclass
class SYSInput(BaseModel):
    """SYS 模組輸入 (MCP 工具調用)"""
    tool_name: str                      # 工具名稱
    arguments: Dict[str, Any]           # 工具參數
    workflow_session_id: Optional[str]  # 工作流會話 ID
    
@dataclass
class SYSOutput(BaseModel):
    """SYS 模組輸出"""
    success: bool                       # 執行成功
    result: Any                         # 執行結果
    workflow_id: Optional[str]          # 工作流 ID (若為工作流)
    requires_review: bool               # 是否需要 LLM 審核
    review_data: Optional[Dict]         # 審核資料
```

### 4.3.6 Workflow Engine

#### 工作流狀態機

```
┌─────────────────────────────────────────┐
│       Workflow State Machine            │
├─────────────────────────────────────────┤
│                                         │
│  READY → EXECUTING → WAITING_REVIEW    │
│            ↓              ↓             │
│        COMPLETED    WAITING_INPUT       │
│            ↓              ↓             │
│        CANCELLED ← FAILED               │
│                                         │
└─────────────────────────────────────────┘
```

#### 工作流步驟類型

| 步驟類型 | 說明 | 執行方式 |
|:---|:---|:---|
| **System Step** | 系統自動執行 | 同步執行 |
| **Processing Step** | 耗時處理 (如檔案 I/O) | 背景執行 |
| **Interactive Step** | 需要 LLM 審核決策 | 等待輸入 |

#### 工作流定義範例

```python
# workflows/drop_and_read.py
class DropAndReadWorkflow(BaseWorkflow):
    """檔案讀取工作流"""
    
    workflow_type = WorkflowType.DROP_AND_READ
    workflow_mode = WorkflowMode.BACKGROUND
    
    steps = [
        SystemStep(
            name="validate_files",
            description="驗證檔案路徑"
        ),
        ProcessingStep(
            name="read_files",
            description="讀取檔案內容"
        ),
        InteractiveStep(
            name="llm_review",
            description="LLM 審核內容",
            requires_llm_review=True
        )
    ]
```

### 4.3.7 MCP Server

#### 提供的 MCP 工具

| 工具名稱 | 功能 | 參數 |
|:---|:---|:---|
| `start_workflow` | 啟動工作流 | `type`: 工作流類型<br>`params`: 工作流參數 |
| `approve_step` | 批准工作流步驟 | `workflow_id`: 工作流 ID<br>`step_id`: 步驟 ID |
| `modify_step` | 修改步驟參數後批准 | `workflow_id`, `modifications` |
| `cancel_workflow` | 取消工作流 | `workflow_id` |
| `query_workflow_status` | 查詢工作流狀態 | `workflow_id` |

#### MCP Server 啟動流程

```
SYS Module 初始化
    ↓
MCP Server 啟動:
    - 載入 functions.yaml
    - 註冊工具處理器
    - 監聽 LLM 調用
    ↓
接收 MCP 請求:
tool_call = {
    "name": "start_workflow",
    "arguments": {...}
}
    ↓
路由至對應處理器
    ↓
返回 MCP 響應
```

### 4.3.8 Background Executor

#### ThreadPoolExecutor 設計

**配置**:
- `max_workers=4`: 最多 4 個並發工作流
- 非阻塞提交: `executor.submit(workflow.execute)`
- Future 追蹤: 儲存工作流執行 Future

#### 執行流程

```
LLM 調用 start_workflow
    ↓
SYS Module 創建 Workflow 實例
    ↓
提交至 Background Executor:
future = executor.submit(workflow.execute)
    ↓
立即返回 workflow_id 給 LLM
    ↓
背景執行:
    - 步驟循環執行
    - 發布 WORKFLOW_STEP_COMPLETED 事件
    - 遇到 Interactive Step → 暫停,等待 LLM
    ↓
LLM 調用 approve_step → 繼續執行
    ↓
完成 → 發布 WORKFLOW_COMPLETED 事件
```

### 4.3.9 檔案工作流範例

#### DROP_AND_READ 工作流

**用途**: 讀取用戶拖放的檔案

**步驟**:
1. **validate_files**: 驗證檔案路徑與權限
2. **read_files**: 讀取檔案內容
3. **llm_review**: LLM 審核內容並決定下一步

**LLM 互動**:
```
用戶: "幫我讀取這些檔案"
    ↓
LLM 調用: start_workflow(type="drop_and_read", files=[...])
    ↓
SYS 執行 → 讀取完成
    ↓
SYS 返回: {
    "workflow_id": "wf_001",
    "requires_review": true,
    "review_data": {
        "file_contents": [...],
        "suggestions": ["summarize", "archive"]
    }
}
    ↓
LLM 審核 → 決策:
approve_step(workflow_id="wf_001", decision="summarize")
```

#### ARCHIVE_FILES 工作流

**用途**: 智能歸檔檔案

**步驟**:
1. **analyze_files**: 分析檔案屬性 (類型、日期、內容)
2. **generate_structure**: 生成歸檔結構建議
3. **llm_review**: LLM 審核歸檔計劃
4. **move_files**: 執行檔案移動
5. **create_summary**: 生成歸檔總結

### 4.3.10 WORK_SYS 協作通道

#### 資料提供流程

```
Workflow 執行中
    ↓
Working Context 更新:
channel_id = "WORK_SYS"
data = {
    "workflow_id": "wf_001",
    "current_step": "llm_review",
    "status": "WAITING_REVIEW",
    "review_data": {...}
}
    ↓
LLM Module 消費 (WORK 模式)
    ↓
LLM 根據 review_data 生成決策
    ↓
調用 approve_step / modify_step
```

### 4.3.11 配置參數

| 參數 | 預設值 | 說明 |
|:---|:---|:---|
| `executor_max_workers` | `4` | 背景執行器最大並發數 |
| `workflow_timeout` | `600` | 工作流超時時間 (秒) |
| `step_timeout` | `120` | 單步驟超時時間 (秒) |
| `enable_workflow_logging` | `true` | 啟用工作流日誌 |
| `mcp_server_port` | `8765` | MCP Server 埠號 |

### 4.3.12 已知問題與改進方向

#### 已知問題
1. **工作流序列化**: 工作流狀態無法持久化,系統重啟後遺失
2. **錯誤恢復**: 步驟失敗後無自動重試機制
3. **並發限制**: ThreadPoolExecutor 限制並發數,大量工作流會排隊

#### 改進方向
- 實作工作流狀態持久化 (Redis/SQLite)
- 添加步驟重試與錯誤恢復機制
- 支援分散式工作流引擎 (Celery/RQ)

---

## 總結 (Summary)

本章節說明了處理層的三個核心模組:

✅ **MEM 模組**:
- 身份隔離記憶系統 (Memory Token)
- FAISS + Sentence Transformers 語義檢索
- 對話快照系統 (Snapshot Manager)
- CHAT_MEM 協作通道

✅ **LLM 模組**:
- Gemini 2.0 整合 (Context Caching)
- MCP Client 工具調用
- 累積評分學習引擎 (EMA)
- CHAT/WORK 模式分離

✅ **SYS 模組**:
- Workflow Engine (多步驟任務編排)
- MCP Server (標準化工具接口)
- Background Executor (非阻塞執行)
- 檔案工作流 (DROP_AND_READ, ARCHIVE_FILES)

這三個模組透過協作通道 (CHAT_MEM, WORK_SYS) 與 MCP Protocol 緊密協作,構成系統的智能核心。

---

[← 上一章:模組設計 - 輸入層](./03_模組設計_輸入層.md) | [返回主文件](../System%20Design%20Documentation.md) | [下一章:模組設計 - 輸出層 →](./05_模組設計_輸出層.md)
