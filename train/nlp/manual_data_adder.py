#!/usr/bin/env python3
"""
æ‰‹å‹•æ•¸æ“šæ·»åŠ å·¥å…·
å¹«åŠ©ç”¨æˆ¶æ­£ç¢ºæ ¼å¼åŒ–å’Œæ·»åŠ æ–°çš„è¨“ç·´æ•¸æ“š
"""

import json
import uuid
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple
import sys

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from utils.debug_helper import debug_log, info_log, error_log

class ManualDataAdder:
    """æ‰‹å‹•æ•¸æ“šæ·»åŠ å·¥å…·"""
    
    def __init__(self):
        self.data_dir = Path("./data")
        self.annotated_dir = self.data_dir / "annotated"
        self.backup_dir = self.data_dir / "backup"
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self.backup_dir.mkdir(exist_ok=True)
    
    def tokenize_text(self, text: str) -> List[str]:
        """å°‡æ–‡æœ¬åˆ†è©"""
        # ç°¡å–®çš„åˆ†è©ï¼šæŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œä¿ç•™æ¨™é»
        tokens = []
        current_token = ""
        
        for char in text:
            if char.isspace():
                if current_token:
                    tokens.append(current_token)
                    current_token = ""
            elif char in ".,!?;:":
                if current_token:
                    tokens.append(current_token)
                    current_token = ""
                tokens.append(char)
            else:
                current_token += char
        
        if current_token:
            tokens.append(current_token)
        
        return tokens
    
    def calculate_segment_positions(self, text: str, segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¨ˆç®—åˆ†æ®µåœ¨åŸæ–‡ä¸­çš„æº–ç¢ºä½ç½®"""
        updated_segments = []
        
        for segment in segments:
            segment_text = segment['text']
            label = segment['label']
            
            # åœ¨åŸæ–‡ä¸­æŸ¥æ‰¾åˆ†æ®µä½ç½®
            start_pos = text.find(segment_text)
            if start_pos == -1:
                # å¦‚æœæ‰¾ä¸åˆ°å®Œå…¨åŒ¹é…ï¼Œå˜—è©¦æ¨¡ç³ŠåŒ¹é…
                error_log(f"âš ï¸  ç„¡æ³•åœ¨åŸæ–‡ä¸­æ‰¾åˆ°åˆ†æ®µ: '{segment_text}'")
                error_log(f"   åŸæ–‡: '{text}'")
                # ä½¿ç”¨ç”¨æˆ¶æä¾›çš„ä½ç½®æˆ–ä¼°ç®—
                start_pos = segment.get('start', 0)
                end_pos = segment.get('end', len(segment_text))
            else:
                end_pos = start_pos + len(segment_text)
            
            updated_segments.append({
                'text': segment_text,
                'label': label.upper(),
                'start': start_pos,
                'end': end_pos,
                'confidence': segment.get('confidence', 1.0),
                'annotator_notes': segment.get('annotator_notes', '')
            })
        
        return updated_segments
    
    def generate_bio_labels(self, tokens: List[str], segments: List[Dict[str, Any]], text: str) -> List[str]:
        """ç”ŸæˆBIOæ¨™ç±¤"""
        bio_labels = ['O'] * len(tokens)
        
        # è¨ˆç®—æ¯å€‹tokenåœ¨åŸæ–‡ä¸­çš„ä½ç½®
        token_positions = []
        char_pos = 0
        
        for token in tokens:
            # è·³éç©ºç™½å­—ç¬¦
            while char_pos < len(text) and text[char_pos].isspace():
                char_pos += 1
            
            token_start = char_pos
            token_end = char_pos + len(token)
            token_positions.append((token_start, token_end))
            char_pos = token_end
        
        # ç‚ºæ¯å€‹åˆ†æ®µåˆ†é…BIOæ¨™ç±¤
        for segment in segments:
            seg_start = segment['start']
            seg_end = segment['end']
            label = segment['label']
            
            first_token = True
            for i, (token_start, token_end) in enumerate(token_positions):
                # æª¢æŸ¥tokenæ˜¯å¦åœ¨åˆ†æ®µç¯„åœå…§
                if (token_start >= seg_start and token_end <= seg_end) or \
                   (token_start < seg_end and token_end > seg_start):
                    
                    if first_token:
                        bio_labels[i] = f'B-{label}'
                        first_token = False
                    else:
                        bio_labels[i] = f'I-{label}'
        
        return bio_labels
    
    def create_training_example(self, text: str, segments: List[Dict[str, Any]], 
                              metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """å‰µå»ºè¨“ç·´ç¯„ä¾‹"""
        # ç”Ÿæˆå”¯ä¸€ID
        example_id = f"manual_{uuid.uuid4().hex[:8]}"
        
        # åˆ†è©
        tokens = self.tokenize_text(text)
        
        # è¨ˆç®—åˆ†æ®µä½ç½®
        updated_segments = self.calculate_segment_positions(text, segments)
        
        # ç”ŸæˆBIOæ¨™ç±¤
        bio_labels = self.generate_bio_labels(tokens, updated_segments, text)
        
        # é è¨­metadata
        default_metadata = {
            "source": "manual_annotation",
            "scenario": "user_input",
            "created_date": datetime.now().isoformat(),
            "annotated": True,
            "quality_checked": True,
            "annotator": "human",
            "annotation_date": datetime.now().isoformat()
        }
        
        if metadata:
            default_metadata.update(metadata)
        
        return {
            "id": example_id,
            "text": text,
            "tokens": tokens,
            "bio_labels": bio_labels,
            "segments": updated_segments,
            "metadata": default_metadata
        }
    
    def validate_example(self, example: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """é©—è­‰è¨“ç·´ç¯„ä¾‹çš„æ­£ç¢ºæ€§"""
        errors = []
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_fields = ['id', 'text', 'tokens', 'bio_labels', 'segments']
        for field in required_fields:
            if field not in example:
                errors.append(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {field}")
        
        if errors:
            return False, errors
        
        # æª¢æŸ¥é•·åº¦ä¸€è‡´æ€§
        if len(example['tokens']) != len(example['bio_labels']):
            errors.append(f"tokenså’Œbio_labelsé•·åº¦ä¸ä¸€è‡´: {len(example['tokens'])} vs {len(example['bio_labels'])}")
        
        # æª¢æŸ¥åˆ†æ®µä½ç½®
        text = example['text']
        for i, segment in enumerate(example['segments']):
            start, end = segment['start'], segment['end']
            if start >= end:
                errors.append(f"åˆ†æ®µ{i+1}ä½ç½®ç„¡æ•ˆ: start({start}) >= end({end})")
            elif end > len(text):
                errors.append(f"åˆ†æ®µ{i+1}çµæŸä½ç½®è¶…å‡ºæ–‡æœ¬ç¯„åœ: {end} > {len(text)}")
            else:
                actual_text = text[start:end]
                expected_text = segment['text']
                if actual_text != expected_text:
                    errors.append(f"åˆ†æ®µ{i+1}æ–‡æœ¬ä¸åŒ¹é…: æœŸæœ›'{expected_text}', å¯¦éš›'{actual_text}'")
        
        # æª¢æŸ¥BIOæ¨™ç±¤æ ¼å¼
        valid_labels = ['O', 'B-CALL', 'I-CALL', 'B-CHAT', 'I-CHAT', 'B-COMMAND', 'I-COMMAND']
        for i, label in enumerate(example['bio_labels']):
            if label not in valid_labels:
                errors.append(f"ç„¡æ•ˆçš„BIOæ¨™ç±¤: {label} (ä½ç½® {i})")
        
        return len(errors) == 0, errors
    
    def backup_existing_data(self):
        """å‚™ä»½ç¾æœ‰æ•¸æ“š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for split in ['train', 'dev', 'test']:
            jsonl_file = self.annotated_dir / f"{split}.jsonl"
            conllu_file = self.annotated_dir / f"{split}.conllu"
            
            if jsonl_file.exists():
                backup_jsonl = self.backup_dir / f"{split}_{timestamp}.jsonl"
                backup_jsonl.write_text(jsonl_file.read_text(encoding='utf-8'), encoding='utf-8')
                info_log(f"å‚™ä»½: {jsonl_file} -> {backup_jsonl}")
            
            if conllu_file.exists():
                backup_conllu = self.backup_dir / f"{split}_{timestamp}.conllu"
                backup_conllu.write_text(conllu_file.read_text(encoding='utf-8'), encoding='utf-8')
                info_log(f"å‚™ä»½: {conllu_file} -> {backup_conllu}")
    
    def add_examples_to_training_set(self, examples: List[Dict[str, Any]], 
                                   split: str = "train") -> bool:
        """å°‡ç¯„ä¾‹æ·»åŠ åˆ°è¨“ç·´é›†"""
        try:
            # å‚™ä»½ç¾æœ‰æ•¸æ“š
            self.backup_existing_data()
            
            # è¼‰å…¥ç¾æœ‰æ•¸æ“š
            jsonl_file = self.annotated_dir / f"{split}.jsonl"
            existing_examples = []
            
            if jsonl_file.exists():
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            existing_examples.append(json.loads(line))
            
            # æ·»åŠ æ–°ç¯„ä¾‹
            all_examples = existing_examples + examples
            
            # ä¿å­˜æ›´æ–°çš„æ•¸æ“š
            with open(jsonl_file, 'w', encoding='utf-8') as f:
                for example in all_examples:
                    f.write(json.dumps(example, ensure_ascii=False) + '\n')
            
            info_log(f"æˆåŠŸæ·»åŠ  {len(examples)} å€‹ç¯„ä¾‹åˆ° {split}.jsonl")
            info_log(f"ç¸½è¨ˆ {len(all_examples)} å€‹è¨“ç·´ç¯„ä¾‹")
            
            # é‡æ–°ç”ŸæˆCoNLL-Uæ ¼å¼
            self._generate_conllu_format(all_examples, split)
            
            return True
            
        except Exception as e:
            error_log(f"æ·»åŠ æ•¸æ“šå¤±æ•—: {e}")
            return False
    
    def _generate_conllu_format(self, examples: List[Dict[str, Any]], split: str):
        """ç”ŸæˆCoNLL-Uæ ¼å¼æ•¸æ“š"""
        conllu_file = self.annotated_dir / f"{split}.conllu"
        
        with open(conllu_file, 'w', encoding='utf-8') as f:
            for example in examples:
                f.write(f"# sent_id = {example['id']}\n")
                f.write(f"# text = {example['text']}\n")
                
                for i, (token, label) in enumerate(zip(example['tokens'], example['bio_labels'])):
                    f.write(f"{i+1}\t{token}\t_\t_\t_\t_\t_\t_\t_\t{label}\n")
                
                f.write("\n")
        
        info_log(f"ç”ŸæˆCoNLL-Uæ ¼å¼: {conllu_file}")
    
    def interactive_add_example(self):
        """äº’å‹•å¼æ·»åŠ ç¯„ä¾‹"""
        info_log("ğŸ› ï¸  äº’å‹•å¼æ•¸æ“šæ·»åŠ å·¥å…·")
        info_log("è«‹æŒ‰ç…§æç¤ºè¼¸å…¥æ–°çš„è¨“ç·´ç¯„ä¾‹")
        
        # è¼¸å…¥æ–‡æœ¬
        print("\nè«‹è¼¸å…¥è¦æ¨™è¨»çš„æ–‡æœ¬:")
        text = input("æ–‡æœ¬: ").strip()
        
        if not text:
            error_log("æ–‡æœ¬ä¸èƒ½ç‚ºç©º")
            return False
        
        # è¼¸å…¥åˆ†æ®µ
        segments = []
        print(f"\næ–‡æœ¬: '{text}'")
        print("è«‹è¼¸å…¥åˆ†æ®µè³‡è¨Š (è¼¸å…¥ç©ºè¡ŒçµæŸ):")
        
        segment_num = 1
        while True:
            print(f"\n=== åˆ†æ®µ {segment_num} ===")
            segment_text = input("åˆ†æ®µæ–‡æœ¬: ").strip()
            
            if not segment_text:
                break
            
            print("æ„åœ–é¡åˆ¥: 1=CALL, 2=CHAT, 3=COMMAND")
            intent_choice = input("é¸æ“‡ (1-3): ").strip()
            
            intent_map = {'1': 'CALL', '2': 'CHAT', '3': 'COMMAND'}
            intent = intent_map.get(intent_choice, 'CHAT')
            
            segments.append({
                'text': segment_text,
                'label': intent,
                'confidence': 1.0,
                'annotator_notes': ''
            })
            
            segment_num += 1
        
        if not segments:
            error_log("è‡³å°‘éœ€è¦ä¸€å€‹åˆ†æ®µ")
            return False
        
        # å‰µå»ºç¯„ä¾‹
        try:
            example = self.create_training_example(text, segments)
            
            # é©—è­‰ç¯„ä¾‹
            is_valid, errors = self.validate_example(example)
            
            if not is_valid:
                error_log("ç¯„ä¾‹é©—è­‰å¤±æ•—:")
                for error in errors:
                    error_log(f"  - {error}")
                return False
            
            # é¡¯ç¤ºé è¦½
            print("\nğŸ“‹ ç¯„ä¾‹é è¦½:")
            print(f"ID: {example['id']}")
            print(f"æ–‡æœ¬: {example['text']}")
            print(f"Tokens: {example['tokens']}")
            print(f"BIOæ¨™ç±¤: {example['bio_labels']}")
            print(f"åˆ†æ®µ:")
            for i, seg in enumerate(example['segments'], 1):
                print(f"  {i}. [{seg['start']}:{seg['end']}] {seg['label']}: '{seg['text']}'")
            
            # ç¢ºèªæ·»åŠ 
            confirm = input("\nç¢ºèªæ·»åŠ æ­¤ç¯„ä¾‹? (y/N): ").strip().lower()
            
            if confirm == 'y':
                success = self.add_examples_to_training_set([example])
                if success:
                    info_log("âœ… ç¯„ä¾‹æ·»åŠ æˆåŠŸ!")
                    return True
                else:
                    error_log("âŒ ç¯„ä¾‹æ·»åŠ å¤±æ•—!")
                    return False
            else:
                info_log("å·²å–æ¶ˆæ·»åŠ ")
                return False
                
        except Exception as e:
            error_log(f"å‰µå»ºç¯„ä¾‹å¤±æ•—: {e}")
            return False
    
    def add_batch_examples(self, examples_data: List[Tuple[str, List[Dict[str, Any]]]]) -> bool:
        """æ‰¹é‡æ·»åŠ ç¯„ä¾‹"""
        info_log(f"ğŸ”„ æ‰¹é‡æ·»åŠ  {len(examples_data)} å€‹ç¯„ä¾‹...")
        
        all_examples = []
        
        for i, (text, segments) in enumerate(examples_data, 1):
            try:
                example = self.create_training_example(text, segments)
                
                # é©—è­‰ç¯„ä¾‹
                is_valid, errors = self.validate_example(example)
                
                if not is_valid:
                    error_log(f"ç¯„ä¾‹ {i} é©—è­‰å¤±æ•—:")
                    for error in errors:
                        error_log(f"  - {error}")
                    continue
                
                all_examples.append(example)
                info_log(f"âœ… ç¯„ä¾‹ {i}: '{text[:50]}{'...' if len(text) > 50 else ''}'")
                
            except Exception as e:
                error_log(f"âŒ ç¯„ä¾‹ {i} è™•ç†å¤±æ•—: {e}")
                continue
        
        if all_examples:
            success = self.add_examples_to_training_set(all_examples)
            if success:
                info_log(f"ğŸ‰ æˆåŠŸæ·»åŠ  {len(all_examples)} å€‹ç¯„ä¾‹!")
                return True
        
        return False

def main():
    """ä¸»å‡½æ•¸"""
    adder = ManualDataAdder()
    
    print("ğŸ“ æ‰‹å‹•æ•¸æ“šæ·»åŠ å·¥å…·")
    print("="*50)
    print("1. äº’å‹•å¼æ·»åŠ å–®å€‹ç¯„ä¾‹")
    print("2. æ‰¹é‡æ·»åŠ ç¯„ä¾‹ (ç¨‹å¼ç¢¼ä¸­å®šç¾©)")
    print("3. æŸ¥çœ‹ç¾æœ‰æ•¸æ“šçµ±è¨ˆ")
    
    choice = input("\nè«‹é¸æ“‡æ“ä½œ (1-3): ").strip()
    
    if choice == '1':
        adder.interactive_add_example()
    
    elif choice == '2':
        # ç¤ºä¾‹æ‰¹é‡æ•¸æ“š
        batch_examples = [
            ("Hey UEP, how's the weather today?", [
                {'text': 'Hey UEP', 'label': 'CALL'},
                {'text': "how's the weather today?", 'label': 'COMMAND'}
            ]),
            ("I'm really excited about this project!", [
                {'text': "I'm really excited about this project!", 'label': 'CHAT'}
            ]),
            ("System wake up, please save my work", [
                {'text': 'System wake up', 'label': 'CALL'},
                {'text': 'please save my work', 'label': 'COMMAND'}
            ])
        ]
        
        adder.add_batch_examples(batch_examples)
    
    elif choice == '3':
        # é¡¯ç¤ºçµ±è¨ˆ
        for split in ['train', 'dev', 'test']:
            jsonl_file = adder.annotated_dir / f"{split}.jsonl"
            if jsonl_file.exists():
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    count = sum(1 for line in f if line.strip())
                info_log(f"{split}.jsonl: {count} å€‹ç¯„ä¾‹")
            else:
                info_log(f"{split}.jsonl: ä¸å­˜åœ¨")

if __name__ == "__main__":
    main()
