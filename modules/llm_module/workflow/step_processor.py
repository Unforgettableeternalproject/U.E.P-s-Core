# modules/llm_module/workflow/step_processor.py
"""
å·¥ä½œæµæ­¥é©Ÿè™•ç†å™¨

è™•ç† LLM_PROCESSING è«‹æ±‚å’Œå·¥ä½œæµå®Œæˆé‚è¼¯
"""

import asyncio
from typing import Dict, Any

from utils.debug_helper import debug_log, info_log, error_log


class WorkflowStepProcessor:
    """è™•ç†å·¥ä½œæµæ­¥é©Ÿçš„åŸ·è¡Œå’Œå®Œæˆ"""
    
    def __init__(self, llm_module):
        """
        åˆå§‹åŒ–æ­¥é©Ÿè™•ç†å™¨
        
        Args:
            llm_module: LLM æ¨¡çµ„å¯¦ä¾‹
        """
        self.llm_module = llm_module
    
    def handle_llm_processing_request(self, session_id: str, workflow_type: str, llm_request_data: dict):
        """
        è™•ç†å·¥ä½œæµä¸­çš„ LLM_PROCESSING è«‹æ±‚
        
        ç•¶å·¥ä½œæµæ­¥é©Ÿé¡å‹ç‚º STEP_TYPE_LLM_PROCESSING æ™‚ï¼Œæœƒèª¿ç”¨æ­¤æ–¹æ³•ä¾†ï¼š
        1. æå– LLM è«‹æ±‚æ•¸æ“šï¼ˆprompt, output_keyï¼‰
        2. ç”Ÿæˆ LLM å›æ‡‰
        3. å°‡çµæœå¯«å…¥å·¥ä½œæµæœƒè©±æ•¸æ“š
        4. è§¸ç™¼å·¥ä½œæµç¹¼çºŒåŸ·è¡Œ
        
        Args:
            session_id: å·¥ä½œæµæœƒè©±ID
            workflow_type: å·¥ä½œæµé¡å‹
            llm_request_data: åŒ…å« prompt, output_data_key ç­‰çš„è«‹æ±‚æ•¸æ“š
        """
        try:
            debug_log(2, f"[LLM.StepProcessor] é–‹å§‹è™•ç† LLM_PROCESSING è«‹æ±‚: {workflow_type}")
            
            # æå–è«‹æ±‚æ•¸æ“š
            prompt = llm_request_data.get('prompt')
            output_key = llm_request_data.get('output_data_key')
            task_description = llm_request_data.get('task_description', '')
            
            if not prompt:
                error_log(f"[LLM.StepProcessor] LLM_PROCESSING è«‹æ±‚ç¼ºå°‘ prompt")
                return
            
            if not output_key:
                error_log(f"[LLM.StepProcessor] LLM_PROCESSING è«‹æ±‚ç¼ºå°‘ output_data_key")
                return
            
            debug_log(3, f"[LLM.StepProcessor] ä»»å‹™æè¿°: {task_description}")
            debug_log(3, f"[LLM.StepProcessor] è¼¸å‡ºéµ: {output_key}")
            debug_log(3, f"[LLM.StepProcessor] Prompt é•·åº¦: {len(prompt)} å­—ç¬¦")
            
            # ä½¿ç”¨ internal æ¨¡å¼ç”Ÿæˆ LLM å›æ‡‰ï¼ˆç¯€çœ tokenï¼‰
            debug_log(2, f"[LLM.StepProcessor] æ­£åœ¨èª¿ç”¨ Gemini APIï¼ˆinternal æ¨¡å¼ï¼‰...")
            
            # æ§‹å»ºç°¡æ½”çš„ç³»çµ±æç¤ºè©ï¼ˆåƒ…é‡å°å·¥ä½œæµä»»å‹™ï¼‰
            workflow_system_prompt = (
                "You are a helpful assistant processing workflow tasks. "
                "Provide clear, concise responses based on the given instructions. "
                "Follow the format requirements strictly. And ALWAYS respond in English"
            )
            
            response_data = self.llm_module.model.query(
                prompt, 
                mode="internal",
                cached_content=None,
                tools=None,
                system_instruction=workflow_system_prompt
            )
            
            if not response_data or 'text' not in response_data:
                error_log(f"[LLM.StepProcessor] Gemini API å›æ‡‰ç„¡æ•ˆ: {response_data}")
                return
            
            llm_result = response_data['text']
            debug_log(2, f"[LLM.StepProcessor] LLM å›æ‡‰å·²ç”Ÿæˆ (é•·åº¦: {len(llm_result)})")
            debug_log(3, f"[LLM.StepProcessor] å›æ‡‰å…§å®¹é è¦½: {llm_result[:200]}...")
            
            # å¯«å…¥å·¥ä½œæµæœƒè©±æ•¸æ“š
            from core.sessions.session_manager import session_manager
            workflow_session = session_manager.get_workflow_session(session_id)
            
            if not workflow_session:
                error_log(f"[LLM.StepProcessor] æ‰¾ä¸åˆ°å·¥ä½œæµæœƒè©±: {session_id}")
                return
            
            workflow_session.add_data(output_key, llm_result)
            debug_log(2, f"[LLM.StepProcessor] å·²å°‡ LLM çµæœå¯«å…¥æœƒè©±æ•¸æ“šéµ: {output_key}")
            
            # è§¸ç™¼å·¥ä½œæµç¹¼çºŒåŸ·è¡Œ
            debug_log(2, f"[LLM.StepProcessor] èª¿ç”¨ provide_workflow_input æ¨é€²å·¥ä½œæµ...")
            
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # èª¿ç”¨ MCP å·¥å…·æ¨é€²å·¥ä½œæµ
            continue_result = loop.run_until_complete(
                self.llm_module.mcp_client.call_tool(
                    "provide_workflow_input",
                    {
                        "session_id": session_id,
                        "user_input": "",
                        "use_fallback": True
                    }
                )
            )
            
            debug_log(2, f"[LLM.StepProcessor] å·¥ä½œæµæ¨é€²çµæœ: {continue_result.get('status')}")
            debug_log(2, f"[LLM.StepProcessor] LLM_PROCESSING è«‹æ±‚è™•ç†å®Œæˆ")
            
        except Exception as e:
            import traceback
            error_log(f"[LLM.StepProcessor] è™•ç† LLM_PROCESSING è«‹æ±‚å¤±æ•—: {e}")
            error_log(f"[LLM.StepProcessor] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
    
    def process_workflow_completion(self, session_id: str, workflow_type: str, 
                                     step_result: dict, review_data: dict):
        """
        è™•ç†å·¥ä½œæµå®Œæˆï¼Œç”Ÿæˆæœ€çµ‚ç¸½çµå›æ‡‰ä¸¦è§¸ç™¼ TTS
        
        Args:
            session_id: å·¥ä½œæµæœƒè©± ID
            workflow_type: å·¥ä½œæµé¡å‹
            step_result: æœ€å¾Œæ­¥é©Ÿçš„çµæœ
            review_data: å¯©æ ¸æ•¸æ“šï¼ˆåŒ…å«å®Œæ•´çš„å·¥ä½œæµçµæœï¼‰
        """
        try:
            debug_log(2, f"[LLM.StepProcessor] é–‹å§‹è™•ç†å·¥ä½œæµå®Œæˆ: {workflow_type} ({session_id})")
            debug_log(2, f"[LLM.StepProcessor] review_data keys: {list(review_data.keys()) if review_data else 'None'}")
            
            # æ§‹å»ºç¸½çµ prompt
            result_message = step_result.get('message', 'Task completed successfully')
            
            prompt = (
                f"The '{workflow_type}' workflow has completed successfully.\n\n"
                f"Result: {result_message}\n"
            )
            
            # è™•ç† full_contentï¼ˆæ–‡ä»¶è®€å–çµæœï¼‰
            if review_data and 'full_content' in review_data:
                debug_log(2, f"[LLM.StepProcessor] ç™¼ç¾ full_contentï¼Œæ·»åŠ åˆ° prompt")
                file_name = review_data.get('file_name', 'unknown')
                content = review_data.get('full_content', '')
                content_length = review_data.get('content_length', len(content))
                
                # åˆ¤æ–·å…§å®¹æ˜¯å¦æ‡‰è©²å®Œæ•´å”¸å‡º
                should_read_full = content_length <= 500 and content.strip()
                
                if should_read_full:
                    prompt += (
                        f"\nFile Read Results:\n"
                        f"- File: {file_name}\n"
                        f"- Content ({content_length} characters):\n{content}\n\n"
                        f"Generate a natural response that:\n"
                        f"1. Briefly confirms you've read the file '{file_name}'\n"
                        f"2. READ OUT THE ACTUAL FILE CONTENT directly\n"
                        f"3. Keep your introduction brief, then read the content naturally\n"
                        f"IMPORTANT: Actually read the file content aloud. Respond in English only."
                    )
                else:
                    prompt += (
                        f"\nFile Read Results:\n"
                        f"- File: {file_name}\n"
                        f"- Content Length: {content_length} characters\n"
                        f"- Content Preview:\n{content[:200]}...\n\n"
                        f"Generate a natural response that:\n"
                        f"1. Confirms the file has been read successfully\n"
                        f"2. State that the file is too long to read out completely\n"
                        f"3. Offer to help in other ways\n"
                        f"IMPORTANT: Respond in English only."
                    )
            elif review_data:
                # é€šç”¨æ•¸æ“šè™•ç†
                result_data = step_result.get('data', {}) or review_data.get('result_data', review_data)
                
                if result_data:
                    self._add_result_data_to_prompt(prompt, result_data, workflow_type)
                else:
                    prompt += (
                        f"Generate a natural, friendly response that:\n"
                        f"1. Confirms the task is complete\n"
                        f"2. Summarizes the key results\n"
                        f"IMPORTANT: Respond in English only."
                    )
            
            # ç”Ÿæˆå›æ‡‰
            info_log(f"[LLM.StepProcessor] ç”Ÿæˆå·¥ä½œæµå®Œæˆç¸½çµå›æ‡‰...")
            response = self.llm_module.model.query(prompt, mode="work", tools=None)
            response_text = response.get("text", "The task has been completed successfully.")
            
            info_log(f"[LLM.StepProcessor] å·¥ä½œæµå®Œæˆå›æ‡‰: {response_text[:100]}...")
            
            # è§¸ç™¼ TTS è¼¸å‡º
            from core.framework import core_framework
            tts_module = core_framework.get_module('tts')
            if tts_module:
                debug_log(2, f"[LLM.StepProcessor] è§¸ç™¼ TTS è¼¸å‡ºæœ€çµ‚ç¸½çµ")
                tts_module.handle({
                    "text": response_text,
                    "session_id": session_id,
                    "emotion": "neutral"
                })
            
            # æ¨™è¨˜å·¥ä½œæµæœƒè©±å¾…çµæŸ
            from core.sessions.session_manager import unified_session_manager
            unified_session_manager.mark_workflow_session_for_end(
                session_id, 
                reason=f"workflow_completed:{workflow_type}"
            )
            debug_log(1, f"[LLM.StepProcessor] ğŸ”š å·²æ¨™è¨˜ WS å¾…çµæŸ: {session_id}")
            
            # æ¸…é™¤ workflow_processing æ¨™èªŒ
            from core.working_context import working_context_manager
            working_context_manager.set_skip_input_layer(False, reason="workflow_completion_processed")
            debug_log(2, "[LLM.StepProcessor] å·²æ¸…é™¤ workflow_processing æ¨™èªŒ")
            
            debug_log(1, f"[LLM.StepProcessor] âœ… å·¥ä½œæµå®Œæˆè™•ç†å®Œç•¢: {session_id}")
            
        except Exception as e:
            import traceback
            error_log(f"[LLM.StepProcessor] è™•ç†å·¥ä½œæµå®Œæˆå¤±æ•—: {e}")
            error_log(f"[LLM.StepProcessor] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
    
    def _add_result_data_to_prompt(self, prompt: str, result_data: dict, workflow_type: str) -> str:
        """
        æ ¹æ“šçµæœæ•¸æ“šé¡å‹æ·»åŠ åˆ° prompt
        
        Args:
            prompt: åŸå§‹ prompt
            result_data: çµæœæ•¸æ“š
            workflow_type: å·¥ä½œæµé¡å‹
            
        Returns:
            æ›´æ–°å¾Œçš„ prompt
        """
        # æ–°èæ‘˜è¦
        if 'news_list' in result_data:
            news_list = result_data.get('news_list', [])
            source = result_data.get('source', 'unknown')
            count = result_data.get('count', len(news_list))
            prompt += f"\nNews Summary Results:\n- Source: {source}\n- Count: {count}\n"
            for i, title in enumerate(news_list[:10], 1):
                prompt += f"  {i}. {title}\n"
            prompt += (
                f"\nGenerate a natural response mentioning the news count and highlighting 1-2 interesting headlines.\n"
                f"IMPORTANT: Respond in English only."
            )
        
        # å¾…è¾¦äº‹é …æŸ¥è©¢
        elif 'tasks' in result_data:
            tasks = result_data.get('tasks', [])
            task_count = len(tasks)
            
            if task_count > 3:
                prompt += f"\nTodo Tasks ({task_count} total - showing first 3):\n"
                for i, task in enumerate(tasks[:3], 1):
                    prompt += f"{i}. {task.get('task_name', 'Unnamed')} (Priority: {task.get('priority', 'medium')})\n"
                prompt += (
                    f"\nSummarize all tasks and provide statistics.\n"
                    f"IMPORTANT: Don't read all tasks - summarize! Respond in English only."
                )
            else:
                prompt += f"\nTodo Tasks ({task_count} tasks):\n"
                for i, task in enumerate(tasks, 1):
                    prompt += f"{i}. {task.get('task_name', 'Unnamed')} (Priority: {task.get('priority', 'medium')})\n"
                prompt += f"\nList all tasks clearly. Respond in English only."
        
        # è¡Œäº‹æ›†æŸ¥è©¢
        elif 'events' in result_data:
            events = result_data.get('events', [])
            event_count = len(events)
            
            if event_count > 3:
                prompt += f"\nCalendar Events ({event_count} total - showing first 3):\n"
                for i, event in enumerate(events[:3], 1):
                    prompt += f"{i}. {event.get('summary', 'Untitled')} - {event.get('start_time', '')}\n"
                prompt += f"\nSummarize events. Respond in English only."
            else:
                prompt += f"\nCalendar Events ({event_count} events):\n"
                for i, event in enumerate(events, 1):
                    prompt += f"{i}. {event.get('summary', 'Untitled')} - {event.get('start_time', '')}\n"
                prompt += f"\nList all events. Respond in English only."
        
        else:
            prompt += f"Data: {str(result_data)[:500]}\n\nSummarize the results naturally. Respond in English only."
        
        return prompt
