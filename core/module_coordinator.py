"""
æ¨¡çµ„èª¿ç”¨å”èª¿å™¨ - è² è²¬ç³»çµ±å±¤çš„æ¨¡çµ„é–“èª¿ç”¨é‚è¼¯
å¯¦ç¾å®Œæ•´çš„ä¸‰å±¤æ¶æ§‹ï¼šè¼¸å…¥å±¤ â†’ è™•ç†å±¤ â†’ è¼¸å‡ºå±¤
æ ¹æ“š docs/å®Œæ•´ç³»çµ±æµç¨‹æ–‡æª”.md ä¸­å®šç¾©çš„åˆ†å±¤æ¶æ§‹

=== Flow-Based å»é‡æ©Ÿåˆ¶ ===
ç‚ºé˜²æ­¢äº‹ä»¶é‡è¤‡è™•ç†,æ¡ç”¨ flow-based å»é‡ç­–ç•¥:

1. Flow ID = session_id + cycle_index
   - session_id: GS (General Session) ID,ä¸æ˜¯æ¨¡çµ„å±¤ç´šçš„ CS/WS
   - cycle_index: ç³»çµ±å¾ªç’°è¨ˆæ•¸å™¨ (æ¯æ¬¡å°è©±å¾€è¿” +1)

2. Dedupe Key = flow_id + layer
   æ ¼å¼: "{session_id}:{cycle_index}:{layer}"
   ç¤ºä¾‹: "gs_20231013_001:7:PROCESSING"

3. ç”Ÿå‘½é€±æœŸç®¡ç†:
   - CYCLE_COMPLETED: æ¸…ç†ç•¶å‰ cycle çš„æ‰€æœ‰ layer éµ
   - SESSION_ENDED: æ¸…ç†æ•´å€‹ session çš„æ‰€æœ‰éµ
   - ä¿è­·æ€§æ¸…ç†: è¶…é 2000 å€‹éµæ™‚è‡ªå‹•æ¸…ç†ä¸€åŠ

4. æœƒè©±å±¤ç´šèªªæ˜:
   - GS (General Session): ç³»çµ±ç´šæœƒè©±,è·¨è¶Šæ•´å€‹å°è©±ç”Ÿå‘½é€±æœŸ
   - CS (Chatting Session): å°è©±æœƒè©±,MEM æ¨¡çµ„ä½¿ç”¨
   - WS (Workflow Session): å·¥ä½œæµæœƒè©±,LLM å’Œ SYS æ¨¡çµ„ä½¿ç”¨
   - æœ¬å”èª¿å™¨ä½¿ç”¨ GS ä½œç‚ºå»é‡çš„ session_id
   
è¨»: LLM æ˜¯å…©å€‹é‚è¼¯ç³»çµ± (å°è©±/å·¥ä½œæµ) çš„ä¸­æ¨
"""

import time
import threading
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

from utils.debug_helper import debug_log, info_log, error_log


class ProcessingLayer(Enum):
    """è™•ç†å±¤ç´šå®šç¾©"""
    INPUT = "input"          # è¼¸å…¥å±¤ï¼šSTT, NLP
    PROCESSING = "processing"  # è™•ç†å±¤ï¼šMEM, LLM, SYS
    OUTPUT = "output"        # è¼¸å‡ºå±¤ï¼šTTS


class InvocationResult(Enum):
    """èª¿ç”¨çµæœç‹€æ…‹"""
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"
    NO_TARGET = "no_target"


@dataclass
class LayerTransition:
    """å±¤ç´šè½‰æ›è«‹æ±‚"""
    from_layer: ProcessingLayer
    to_layer: ProcessingLayer
    data: Dict[str, Any]
    source_module: str
    reasoning: str


@dataclass
class ModuleInvocationRequest:
    """æ¨¡çµ„èª¿ç”¨è«‹æ±‚"""
    target_module: str
    input_data: Dict[str, Any]
    source_module: str
    reasoning: str
    layer: ProcessingLayer
    priority: int = 1
    timeout: float = 30.0


@dataclass
class ModuleInvocationResponse:
    """æ¨¡çµ„èª¿ç”¨å›æ‡‰"""
    target_module: str
    result: InvocationResult
    layer: ProcessingLayer
    output_data: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    execution_time: float = 0.0


class ModuleInvocationCoordinator:
    """æ¨¡çµ„èª¿ç”¨å”èª¿å™¨ - å¯¦ç¾å®Œæ•´ä¸‰å±¤æ¶æ§‹çš„æ¨¡çµ„é–“èª¿ç”¨ç®¡ç†"""
    
    # å®šç¾©æ¨¡çµ„æ‰€å±¬å±¤ç´š
    MODULE_LAYERS = {
        'stt': ProcessingLayer.INPUT,
        'nlp': ProcessingLayer.INPUT,
        'mem': ProcessingLayer.PROCESSING,
        'llm': ProcessingLayer.PROCESSING,
        'sys': ProcessingLayer.PROCESSING,
        'tts': ProcessingLayer.OUTPUT
    }
    
    def __init__(self):
        """åˆå§‹åŒ–å”èª¿å™¨"""
        self._invocation_lock = threading.Lock()
        self._active_invocations = {}
        self._invocation_history = []
        self._layer_transitions = []
        
        # æ–°çš„å»é‡æ©Ÿåˆ¶: flow_id + layer
        # dedupe_key = f"{session_id}:{cycle_index}:{layer}"
        self._layer_dedupe_keys = set()  # å±¤ç´šå»é‡é›†åˆ
        self._dedupe_lock = threading.Lock()
        self._max_dedupe_keys = 2000  # æœ€å¤šä¿ç•™ 2000 å€‹å»é‡éµ
        
        # çµ±è¨ˆä¿¡æ¯
        self._dedupe_hit_count = 0
        self._cleanup_count = 0
        
        # æœƒè©±çµæŸç®¡ç† - é›™æ¢ä»¶çµ‚æ­¢æ©Ÿåˆ¶
        self._pending_session_end = None  # æ¨™è¨˜æœƒè©±çµæŸè«‹æ±‚ï¼Œç­‰å¾… CYCLE_COMPLETED
        
        info_log("[ModuleCoordinator] ä¸‰å±¤æ¶æ§‹æ¨¡çµ„èª¿ç”¨å”èª¿å™¨åˆå§‹åŒ–")
        info_log("[ModuleCoordinator] ä½¿ç”¨ flow-based å»é‡ç­–ç•¥ (session_id:cycle_index:layer)")
        
        # âœ… è¨‚é–±äº‹ä»¶ç¸½ç·š
        self._setup_event_subscriptions()
    
    def _setup_event_subscriptions(self):
        """è¨­ç½®äº‹ä»¶è¨‚é–± - äº‹ä»¶é©…å‹•æ¶æ§‹çš„æ ¸å¿ƒ"""
        try:
            from core.event_bus import event_bus, SystemEvent
            
            info_log("[ModuleCoordinator] é–‹å§‹è¨‚é–±äº‹ä»¶ç¸½ç·š...")
            
            # è¨‚é–±è¼¸å…¥å±¤å®Œæˆäº‹ä»¶
            event_bus.subscribe(
                SystemEvent.INPUT_LAYER_COMPLETE,
                self._on_input_layer_complete,
                handler_name="ModuleCoordinator.input_complete"
            )
            info_log(f"[ModuleCoordinator] âœ“ å·²è¨‚é–± INPUT_LAYER_COMPLETE")
            
            # è¨‚é–±è™•ç†å±¤å®Œæˆäº‹ä»¶
            event_bus.subscribe(
                SystemEvent.PROCESSING_LAYER_COMPLETE,
                self._on_processing_layer_complete,
                handler_name="ModuleCoordinator.processing_complete"
            )
            info_log(f"[ModuleCoordinator] âœ“ å·²è¨‚é–± PROCESSING_LAYER_COMPLETE")
            
            # è¨‚é–±å¾ªç’°å®Œæˆäº‹ä»¶ (ç”¨æ–¼æ¸…ç†å»é‡éµ)
            event_bus.subscribe(
                SystemEvent.CYCLE_COMPLETED,
                self._on_cycle_completed,
                handler_name="ModuleCoordinator.cycle_complete"
            )
            info_log(f"[ModuleCoordinator] âœ“ å·²è¨‚é–± CYCLE_COMPLETED")
            
            # è¨‚é–±æœƒè©±çµæŸäº‹ä»¶ (ç”¨æ–¼æ¸…ç†å»é‡éµ)
            event_bus.subscribe(
                SystemEvent.SESSION_ENDED,
                self._on_session_ended,
                handler_name="ModuleCoordinator.session_end"
            )
            info_log(f"[ModuleCoordinator] âœ“ å·²è¨‚é–± SESSION_ENDED")
            
            info_log("[ModuleCoordinator] âœ… äº‹ä»¶è¨‚é–±å®Œæˆ (4 å€‹äº‹ä»¶)")
            
        except Exception as e:
            error_log(f"[ModuleCoordinator] âŒ äº‹ä»¶è¨‚é–±å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    def _on_input_layer_complete(self, event):
        """
        è¼¸å…¥å±¤å®Œæˆäº‹ä»¶è™•ç†å™¨
        ç•¶ NLP ç™¼å¸ƒ INPUT_LAYER_COMPLETE äº‹ä»¶æ™‚è§¸ç™¼
        ä½¿ç”¨ flow-based å»é‡: session_id + cycle_index + layer
        
        æ³¨æ„: é€™è£¡çš„ session_id æ˜¯ GS (General Session),
              ä¸æ˜¯æ¨¡çµ„å±¤ç´šçš„ CS (MEM) æˆ– WS (LLM/SYS)
        """
        try:
            # æå– flow è­˜åˆ¥è³‡è¨Š (ä¾†è‡ª General Session)
            session_id = event.data.get('session_id', 'unknown')  # GS ID
            cycle_index = event.data.get('cycle_index', -1)
            
            # æ§‹å»ºå»é‡éµ: flow_id + layer
            dedupe_key = f"{session_id}:{cycle_index}:INPUT"
            
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†éæ­¤ flow çš„æ­¤ layer
            with self._dedupe_lock:
                if dedupe_key in self._layer_dedupe_keys:
                    self._dedupe_hit_count += 1
                    debug_log(2, f"[ModuleCoordinator] âš ï¸ è·³éé‡è¤‡è™•ç† (dedupe_key={dedupe_key}, å‘½ä¸­æ¬¡æ•¸={self._dedupe_hit_count})")
                    return
                
                # æ¨™è¨˜ç‚ºå·²è™•ç†
                self._layer_dedupe_keys.add(dedupe_key)
                debug_log(3, f"[ModuleCoordinator] âœ“ å·²è¨˜éŒ„å»é‡éµ: {dedupe_key} (å…± {len(self._layer_dedupe_keys)} å€‹)")
                
                # ä¿è­·æ€§æ¸…ç†: é¿å…é›†åˆç„¡é™å¢é•·
                if len(self._layer_dedupe_keys) > self._max_dedupe_keys:
                    removed_count = len(self._layer_dedupe_keys) - (self._max_dedupe_keys // 2)
                    keys_to_remove = list(self._layer_dedupe_keys)[:(self._max_dedupe_keys // 2)]
                    for old_key in keys_to_remove:
                        self._layer_dedupe_keys.discard(old_key)
                    self._cleanup_count += removed_count
                    debug_log(3, f"[ModuleCoordinator] ä¿è­·æ€§æ¸…ç†: ç§»é™¤ {removed_count} å€‹èˆŠéµ")
            
            info_log(f"[ModuleCoordinator] ğŸ¯ æ”¶åˆ°è¼¸å…¥å±¤å®Œæˆäº‹ä»¶ (flow={session_id}:{cycle_index}, event_id={event.event_id})")
            debug_log(2, f"[ModuleCoordinator] äº‹ä»¶æ•¸æ“š: {list(event.data.keys())}")
            self.handle_layer_completion(ProcessingLayer.INPUT, event.data)
                    
        except Exception as e:
            error_log(f"[ModuleCoordinator] âŒ è™•ç†è¼¸å…¥å±¤å®Œæˆäº‹ä»¶å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    def _on_processing_layer_complete(self, event):
        """
        è™•ç†å±¤å®Œæˆäº‹ä»¶è™•ç†å™¨
        ç•¶ LLM ç™¼å¸ƒ PROCESSING_LAYER_COMPLETE äº‹ä»¶æ™‚è§¸ç™¼
        ä½¿ç”¨ flow-based å»é‡: session_id + cycle_index + layer
        
        æ³¨æ„: é€™è£¡çš„ session_id æ˜¯ GS (General Session),
              ä¸æ˜¯æ¨¡çµ„å±¤ç´šçš„ CS (MEM) æˆ– WS (LLM/SYS)
        """
        try:
            # æå– flow è­˜åˆ¥è³‡è¨Š (ä¾†è‡ª General Session)
            session_id = event.data.get('session_id', 'unknown')  # GS ID
            cycle_index = event.data.get('cycle_index', -1)
            
            # æ§‹å»ºå»é‡éµ: flow_id + layer
            dedupe_key = f"{session_id}:{cycle_index}:PROCESSING"
            
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†éæ­¤ flow çš„æ­¤ layer
            with self._dedupe_lock:
                if dedupe_key in self._layer_dedupe_keys:
                    self._dedupe_hit_count += 1
                    debug_log(2, f"[ModuleCoordinator] âš ï¸ è·³éé‡è¤‡è™•ç† (dedupe_key={dedupe_key}, å‘½ä¸­æ¬¡æ•¸={self._dedupe_hit_count})")
                    return
                
                # æ¨™è¨˜ç‚ºå·²è™•ç†
                self._layer_dedupe_keys.add(dedupe_key)
                debug_log(3, f"[ModuleCoordinator] âœ“ å·²è¨˜éŒ„å»é‡éµ: {dedupe_key} (å…± {len(self._layer_dedupe_keys)} å€‹)")
                
                # ä¿è­·æ€§æ¸…ç†
                if len(self._layer_dedupe_keys) > self._max_dedupe_keys:
                    removed_count = len(self._layer_dedupe_keys) - (self._max_dedupe_keys // 2)
                    keys_to_remove = list(self._layer_dedupe_keys)[:(self._max_dedupe_keys // 2)]
                    for old_key in keys_to_remove:
                        self._layer_dedupe_keys.discard(old_key)
                    self._cleanup_count += removed_count
                    debug_log(3, f"[ModuleCoordinator] ä¿è­·æ€§æ¸…ç†: ç§»é™¤ {removed_count} å€‹èˆŠéµ")
            
            info_log(f"[ModuleCoordinator] ğŸ¯ æ”¶åˆ°è™•ç†å±¤å®Œæˆäº‹ä»¶ (flow={session_id}:{cycle_index}, event_id={event.event_id})")
            debug_log(2, f"[ModuleCoordinator] äº‹ä»¶æ•¸æ“š: {list(event.data.keys())}")
            self.handle_layer_completion(ProcessingLayer.PROCESSING, event.data)
                    
        except Exception as e:
            error_log(f"[ModuleCoordinator] âŒ è™•ç†è™•ç†å±¤å®Œæˆäº‹ä»¶å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    def _on_cycle_completed(self, event):
        """
        å¾ªç’°å®Œæˆäº‹ä»¶è™•ç†å™¨
        
        è™•ç†å…©å€‹ä»»å‹™ï¼š
        1. æ¸…ç†å»é‡éµ - ç§»é™¤å·²å®Œæˆ cycle çš„æ‰€æœ‰ layer éµ
        2. æª¢æŸ¥æœƒè©±çµæŸ - é›™æ¢ä»¶çµ‚æ­¢æ©Ÿåˆ¶çš„ç¬¬äºŒå€‹æ¢ä»¶
        
        æ³¨æ„: é€™è£¡çš„ session_id æ˜¯ GS (General Session)
        """
        try:
            session_id = event.data.get('session_id', 'unknown')  # GS ID
            cycle_index = event.data.get('cycle_index', -1)
            flow_prefix = f"{session_id}:{cycle_index}:"
            
            # ä»»å‹™ 1: æ¸…ç†å»é‡éµ
            with self._dedupe_lock:
                # æ‰¾å‡ºä¸¦ç§»é™¤æ­¤ flow çš„æ‰€æœ‰ layer éµ
                keys_to_remove = [k for k in self._layer_dedupe_keys if k.startswith(flow_prefix)]
                for key in keys_to_remove:
                    self._layer_dedupe_keys.discard(key)
                
                self._cleanup_count += len(keys_to_remove)
                info_log(f"[ModuleCoordinator] ğŸ§¹ CYCLE_COMPLETED æ¸…ç†: ç§»é™¤ {len(keys_to_remove)} å€‹å»é‡éµ (flow={session_id}:{cycle_index})")
                debug_log(3, f"[ModuleCoordinator] å‰©é¤˜å»é‡éµæ•¸é‡: {len(self._layer_dedupe_keys)}")
            
            # ä»»å‹™ 2: æª¢æŸ¥æœƒè©±çµæŸè«‹æ±‚ï¼ˆé›™æ¢ä»¶çµ‚æ­¢æ©Ÿåˆ¶ï¼‰
            if self._pending_session_end:
                pending = self._pending_session_end
                pending_gs_id = pending.get('gs_id')
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯åŒä¸€å€‹ GS
                if pending_gs_id == session_id:
                    reason = pending.get('reason', 'LLM requested')
                    info_log(f"[ModuleCoordinator] âœ… é›™æ¢ä»¶çµ‚æ­¢æ©Ÿåˆ¶æ»¿è¶³ï¼š")
                    info_log(f"  â””â”€ æ¢ä»¶ 1: å¤–éƒ¨ä¸­æ–·é» (LLM session_control) âœ…")
                    info_log(f"  â””â”€ æ¢ä»¶ 2: å¾ªç’°çµæŸ (CYCLE_COMPLETED) âœ…")
                    info_log(f"[ModuleCoordinator] ç¾åœ¨åŸ·è¡Œæœƒè©±çµæŸ (gs_id={session_id}, reason={reason})")
                    
                    # åŸ·è¡Œæœƒè©±çµæŸ
                    self._handle_session_end(pending.get('session_control'))
                    
                    # æ¸…é™¤æ¨™è¨˜
                    self._pending_session_end = None
                else:
                    debug_log(2, f"[ModuleCoordinator] æœƒè©±çµæŸæ¨™è¨˜çš„ gs_id ({pending_gs_id}) èˆ‡ç•¶å‰å¾ªç’°çš„ gs_id ({session_id}) ä¸åŒ¹é…ï¼Œç¹¼çºŒç­‰å¾…")
                
        except Exception as e:
            error_log(f"[ModuleCoordinator] âŒ è™•ç†å¾ªç’°å®Œæˆäº‹ä»¶å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    def _on_session_ended(self, event):
        """
        æœƒè©±çµæŸäº‹ä»¶è™•ç†å™¨
        æ¸…ç†æ•´å€‹ session çš„å»é‡éµ
        
        æ³¨æ„: é€™è£¡çš„ session_id æ˜¯ GS (General Session)
              ç•¶ GS çµæŸæ™‚,å…¶ä¸‹çš„æ‰€æœ‰ CS (MEM) å’Œ WS (LLM/SYS) ä¹Ÿéƒ½çµæŸäº†
        """
        try:
            session_id = event.data.get('session_id', 'unknown')  # GS ID
            session_prefix = f"{session_id}:"
            
            with self._dedupe_lock:
                # æ‰¾å‡ºä¸¦ç§»é™¤æ­¤ session çš„æ‰€æœ‰éµ
                keys_to_remove = [k for k in self._layer_dedupe_keys if k.startswith(session_prefix)]
                for key in keys_to_remove:
                    self._layer_dedupe_keys.discard(key)
                
                self._cleanup_count += len(keys_to_remove)
                info_log(f"[ModuleCoordinator] ğŸ§¹ SESSION_ENDED æ¸…ç†: ç§»é™¤ {len(keys_to_remove)} å€‹å»é‡éµ (session={session_id})")
                debug_log(3, f"[ModuleCoordinator] å‰©é¤˜å»é‡éµæ•¸é‡: {len(self._layer_dedupe_keys)}")
                debug_log(3, f"[ModuleCoordinator] çµ±è¨ˆ - å‘½ä¸­æ¬¡æ•¸: {self._dedupe_hit_count}, æ¸…ç†æ¬¡æ•¸: {self._cleanup_count}")
                
        except Exception as e:
            error_log(f"[ModuleCoordinator] âŒ è™•ç†æœƒè©±çµæŸäº‹ä»¶å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    def handle_layer_completion(self, layer: ProcessingLayer, completion_data: Dict[str, Any]) -> bool:
        """
        è™•ç†å±¤ç´šå®Œæˆé€šçŸ¥ï¼Œå”èª¿ä¸‹ä¸€å±¤è™•ç†
        
        Args:
            layer: å®Œæˆçš„å±¤ç´š
            completion_data: å®Œæˆæ•¸æ“š
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè§¸ç™¼ä¸‹ä¸€å±¤è™•ç†
        """
        try:
            info_log(f"[ModuleCoordinator] {layer.value}å±¤å®Œæˆï¼Œå”èª¿ä¸‹ä¸€å±¤è™•ç†")
            debug_log(2, f"[ModuleCoordinator] å®Œæˆæ•¸æ“š: {list(completion_data.keys())}")
            
            # æ ¹æ“šç•¶å‰å±¤æ±ºå®šä¸‹ä¸€å±¤è™•ç†
            if layer == ProcessingLayer.INPUT:
                return self._transition_to_processing_layer(completion_data)
            elif layer == ProcessingLayer.PROCESSING:
                return self._transition_to_output_layer(completion_data)
            else:
                debug_log(2, f"[ModuleCoordinator] {layer.value}å±¤æ˜¯æœ€çµ‚å±¤ï¼Œç„¡éœ€é€²ä¸€æ­¥è™•ç†")
                return True
                
        except Exception as e:
            error_log(f"[ModuleCoordinator] è™•ç†{layer.value}å±¤å®Œæˆå¤±æ•—: {e}")
            return False
    
    def _transition_to_processing_layer(self, input_data: Dict[str, Any]) -> bool:
        """è¼¸å…¥å±¤ â†’ è™•ç†å±¤è½‰æ›
        
        é‡è¦æ¶æ§‹è¨­è¨ˆï¼š
        - ä¸ä½¿ç”¨ Routerï¼ˆRouter ä¾è³´ç³»çµ±ç‹€æ…‹ï¼Œè€Œç‹€æ…‹å°šæœªè½‰æ›ï¼‰
        - ç›´æ¥å¾ NLP çµæœçš„ primary_intent æ±ºå®šè™•ç†å±¤è·¯å¾‘
        - WORK â†’ LLM + SYS, CHAT â†’ MEM + LLM
        """
        try:
            info_log("[ModuleCoordinator] è¼¸å…¥å±¤ â†’ è™•ç†å±¤è½‰æ›")
            
            # å¾ NLP çµæœç²å–ä¸»è¦æ„åœ–
            nlp_result = input_data.get('nlp_result', {})
            primary_intent = nlp_result.get('primary_intent')
            
            if not primary_intent:
                debug_log(2, "[ModuleCoordinator] NLP çµæœç„¡ä¸»è¦æ„åœ–ï¼Œè·³éè™•ç†å±¤")
                return False
            
            # å°å…¥ IntentType ä»¥é€²è¡Œåˆ¤æ–·
            from modules.nlp_module.intent_types import IntentType
            
            # è™•ç†æšèˆ‰æˆ–å­—ç¬¦ä¸²å€¼
            intent_value = primary_intent.value if hasattr(primary_intent, 'value') else primary_intent
            intent_name = primary_intent.name if hasattr(primary_intent, 'name') else str(primary_intent)
            
            info_log(f"[ModuleCoordinator] ä¸»è¦æ„åœ–: {intent_name} (value={intent_value})")
            
            # âœ¨ æª¢æŸ¥æ˜¯å¦ç‚º WORK è·¯å¾‘çš„ Cycle 0ï¼ˆéœ€è¦ç‰¹æ®Šè™•ç†ï¼‰
            cycle_index = input_data.get('cycle_index', 0)
            
            if (primary_intent == IntentType.WORK or intent_value == "work") and cycle_index == 0:
                # WORK Cycle 0: ä¸‰éšæ®µè™•ç†ï¼ˆLLM æ±ºç­– â†’ SYS å•Ÿå‹• â†’ LLM å›æ‡‰ï¼‰
                info_log("[ModuleCoordinator] ğŸ¯ WORK Cycle 0 - é–‹å§‹ä¸‰éšæ®µè™•ç†")
                return self._handle_work_cycle_0(input_data)
            else:
                # CHAT è·¯å¾‘æˆ– WORK Cycle 1+: æ¨™æº–è™•ç†
                info_log(f"[ModuleCoordinator] æ¨™æº–è·¯å¾‘è™•ç† (intent={intent_name}, cycle={cycle_index})")
                
                # æº–å‚™è™•ç†å±¤èª¿ç”¨è«‹æ±‚ï¼ˆæ ¹æ“š primary_intent æ±ºå®š WORK/CHAT è·¯å¾‘ï¼‰
                requests = self._prepare_processing_requests(intent_name, input_data)
                
                # åŸ·è¡Œè™•ç†å±¤èª¿ç”¨
                responses = self.invoke_multiple_modules(requests)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„èª¿ç”¨
                success_count = sum(1 for r in responses if r.result == InvocationResult.SUCCESS)
                info_log(f"[ModuleCoordinator] è™•ç†å±¤å®Œæˆ: {success_count}/{len(responses)} æˆåŠŸ")
                
                return success_count > 0
            
        except Exception as e:
            error_log(f"[ModuleCoordinator] è¼¸å…¥å±¤ â†’ è™•ç†å±¤è½‰æ›å¤±æ•—: {e}")
            return False
    
    def _transition_to_output_layer(self, processing_data: Dict[str, Any]) -> bool:
        """è™•ç†å±¤ â†’ è¼¸å‡ºå±¤è½‰æ›"""
        try:
            info_log("[ModuleCoordinator] è™•ç†å±¤ â†’ è¼¸å‡ºå±¤è½‰æ›")
            
            # å¾è™•ç†å±¤çµæœä¸­æå–æ–‡å­—å…§å®¹
            response_text = self._extract_response_text(processing_data)
            
            if not response_text:
                debug_log(2, "[ModuleCoordinator] è™•ç†å±¤ç„¡æ–‡å­—è¼¸å‡ºï¼Œè·³éè¼¸å‡ºå±¤")
                return False
            
            info_log(f"[ModuleCoordinator] è™•ç†å±¤æ–‡å­—è¼¸å‡º: {response_text[:50]}...")
            
            # é€šé Router ç²å–è¼¸å‡ºå±¤è·¯ç”±æ±ºç­–
            from core.router import router
            routing_decision = router.route_system_output(
                text=response_text, 
                source_module="processing_layer"
            )
            
            if routing_decision.target_module != "tts":
                debug_log(1, f"[ModuleCoordinator] Router æœªæŒ‡å‘ TTS: {routing_decision.target_module}")
                return False
            
            info_log(f"[ModuleCoordinator] Router æ±ºç­–: {routing_decision.reasoning}")
            
            # æº–å‚™è¼¸å‡ºå±¤èª¿ç”¨ï¼ˆé€šå¸¸æ˜¯TTSï¼‰
            output_request = ModuleInvocationRequest(
                target_module="tts",
                input_data=self._prepare_output_input(processing_data),
                source_module="processing_layer",
                reasoning="è™•ç†å±¤å®Œæˆï¼Œè½‰é€è¼¸å‡ºå±¤",
                layer=ProcessingLayer.OUTPUT,
                priority=2
            )
            
            # åŸ·è¡Œè¼¸å‡ºå±¤èª¿ç”¨
            response = self.invoke_module(output_request)
            
            success = response.result == InvocationResult.SUCCESS
            if success:
                info_log("[ModuleCoordinator] è¼¸å‡ºå±¤å®Œæˆï¼Œä¸‰å±¤æµç¨‹çµæŸ")
                # âœ… TTS æ¨¡çµ„å·²ç¶“é€šéäº‹ä»¶ç¸½ç·šç™¼å¸ƒ OUTPUT_LAYER_COMPLETE äº‹ä»¶
                # âœ… SystemLoop æœƒè‡ªå‹•æ¥æ”¶ä¸¦è™•ç†ï¼Œä¸éœ€è¦é‡è¤‡é€šçŸ¥
                debug_log(2, "[ModuleCoordinator] ç­‰å¾… TTS ç™¼å¸ƒçš„ OUTPUT_LAYER_COMPLETE äº‹ä»¶å®Œæˆå¾ªç’°")
                
                # ğŸ†• Task 5: æª¢æŸ¥æœƒè©±çµæŸè«‹æ±‚ï¼ˆé›™æ¢ä»¶çµ‚æ­¢æ©Ÿåˆ¶ï¼‰
                llm_output = processing_data.get('llm_output', {})
                session_control = llm_output.get('metadata', {}).get('session_control')
                
                # Support both formats: {'action': 'end_session'} and {'session_ended': True}
                should_end = (session_control and 
                             (session_control.get('action') == 'end_session' or 
                              session_control.get('session_ended') is True))
                
                if should_end:
                    reason = session_control.get('reason', 'LLM requested')
                    info_log(f"[ModuleCoordinator] ğŸ”š LLM è«‹æ±‚çµæŸæœƒè©± (åŸå› : {reason})")
                    # âš ï¸ é›™æ¢ä»¶çµ‚æ­¢æ©Ÿåˆ¶ï¼š
                    # æ¢ä»¶ 1: å¤–éƒ¨ä¸­æ–·é»è¢«èª¿ç”¨ âœ… (æ­¤è™•æ¨™è¨˜)
                    # æ¢ä»¶ 2: æ‰€å±¬å¾ªç’°çµæŸ âŒ› (ç­‰å¾… CYCLE_COMPLETED)
                    # â†’ ä¸ç«‹å³çµæŸï¼Œç­‰å¾…å¾ªç’°å®Œæˆå¾Œå†æª¢æŸ¥ä¸¦åŸ·è¡Œ
                    
                    # å¾ processing_data é ‚å±¤ç²å– session_id (GS ID)
                    gs_id = processing_data.get('session_id', 'unknown')
                    self._pending_session_end = {
                        'reason': reason,
                        'session_control': session_control,
                        'gs_id': gs_id
                    }
                    info_log(f"[ModuleCoordinator] âœ… å·²æ¨™è¨˜æœƒè©±çµæŸè«‹æ±‚ï¼Œç­‰å¾…å¾ªç’°å®Œæˆ (gs_id={gs_id})")
            else:
                error_log(f"[ModuleCoordinator] è¼¸å‡ºå±¤èª¿ç”¨å¤±æ•—: {response.error_message}")
            
            return success
            
        except Exception as e:
            error_log(f"[ModuleCoordinator] è™•ç†å±¤ â†’ è¼¸å‡ºå±¤è½‰æ›å¤±æ•—: {e}")
            return False
    
    def _handle_work_cycle_0(self, input_data: Dict[str, Any]) -> bool:
        """è™•ç† WORK è·¯å¾‘çš„ Cycle 0ï¼ˆå•Ÿå‹•å·¥ä½œæµï¼‰
        
        âœ… MCP æ¶æ§‹ï¼šLLM é€šé MCP function calling å•Ÿå‹•å·¥ä½œæµ
        
        Cycle 0 æµç¨‹ï¼š
        1. LLM æ¥æ”¶ç”¨æˆ¶è«‹æ±‚å’Œå¯ç”¨çš„ MCP tools
        2. LLM æ±ºç­–ä¸¦èª¿ç”¨ start_workflow
        3. MCP Client â†’ SYS æ¨¡çµ„å•Ÿå‹•å·¥ä½œæµ
        4. LLM ç”Ÿæˆå›æ‡‰ï¼šã€Œå·¥ä½œæµå·²å•Ÿå‹•ï¼Œç¬¬ä¸€æ­¥æ˜¯...ã€
        
        Cycle 1+ æµç¨‹ï¼ˆå·¥ä½œæµæ­¥é©Ÿäº’å‹•ï¼‰ï¼š
        1. SYS é€šé review_step è¿”å›ç•¶å‰æ­¥é©Ÿä¿¡æ¯
        2. LLM å°‡æ­¥é©Ÿè½‰æ›ç‚ºç”¨æˆ¶å‹å¥½çš„æè¿°
        3. ç”¨æˆ¶å›æ‡‰ â†’ LLM æ±ºå®š approve_step/modify_step/cancel_workflow
        4. é€šé MCP èª¿ç”¨å°æ‡‰å·¥å…·
        5. é‡è¤‡ç›´åˆ°å·¥ä½œæµå®Œæˆ
        
        Args:
            input_data: è¼¸å…¥æ•¸æ“š
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            info_log("[ModuleCoordinator] ğŸ¯ WORK Cycle 0 - LLM é€šé MCP è™•ç†å·¥ä½œæµ")
            
            # èª¿ç”¨ LLMï¼ˆLLM æœƒé€šé MCP function calling å•Ÿå‹•å·¥ä½œæµï¼‰
            llm_data = self._prepare_llm_input(input_data)
            llm_data['phase'] = 'response'  # ç›´æ¥ç”Ÿæˆå›æ‡‰ï¼ˆåŒ…å« MCP èª¿ç”¨ï¼‰
            
            llm_request = ModuleInvocationRequest(
                target_module="llm",
                input_data=llm_data,
                source_module="input_layer",
                reasoning="WORK Cycle 0 - LLM è™•ç†å·¥ä½œæµï¼ˆå« MCP èª¿ç”¨ï¼‰",
                layer=ProcessingLayer.PROCESSING,
                priority=5
            )
            
            llm_response = self.invoke_module(llm_request)
            
            if llm_response.result != InvocationResult.SUCCESS:
                error_log("[ModuleCoordinator] LLM è™•ç†éšæ®µå¤±æ•—")
                return False
            
            llm_output = llm_response.output_data
            debug_log(2, f"[ModuleCoordinator] llm å®Œæ•´è¿”å›çµæœ: {llm_output}")
            
            # âœ… æª¢æŸ¥æ˜¯å¦æˆåŠŸèª¿ç”¨äº† MCP function
            function_call_made = llm_output.get('metadata', {}).get('function_call_made', False)
            function_call_result = llm_output.get('metadata', {}).get('function_call_result', {})
            
            if function_call_made:
                info_log("[ModuleCoordinator] âœ… LLM å·²é€šé MCP å•Ÿå‹•å·¥ä½œæµ")
                
                # âœ… æª¢æŸ¥å·¥ä½œæµæ˜¯å¦æˆåŠŸå•Ÿå‹•
                if function_call_result.get('status') == 'success':
                    debug_log(2, "[ModuleCoordinator] å·¥ä½œæµå•Ÿå‹•æˆåŠŸï¼ŒLLM å·²ç”Ÿæˆåˆå§‹å›æ‡‰")
                    # TODO: åœ¨æœªä¾†çš„ Cycle 1 ä¸­ï¼Œéœ€è¦ï¼š
                    # 1. èª¿ç”¨ review_step ç²å–ç¬¬ä¸€å€‹æ­¥é©Ÿ
                    # 2. LLM å°‡æ­¥é©Ÿè½‰æ›ç‚ºç”¨æˆ¶æè¿°
                    # 3. ç­‰å¾…ç”¨æˆ¶å›æ‡‰å¾Œå†ç¹¼çºŒ
                else:
                    debug_log(2, f"[ModuleCoordinator] å·¥ä½œæµå•Ÿå‹•å¤±æ•—: {function_call_result.get('error')}")
                    # LLM å·²ç¶“åœ¨ follow-up response ä¸­è§£é‡‹äº†éŒ¯èª¤
            else:
                debug_log(2, "[ModuleCoordinator] LLM æœªèª¿ç”¨ MCP functionï¼ˆå¯èƒ½åœ¨è©¢å•æ›´å¤šä¿¡æ¯ï¼‰")
            
            # èˆŠæ¶æ§‹çš„å…¼å®¹è™•ç†ï¼ˆå¦‚æœ LLM è¿”å›äº†èˆŠæ ¼å¼çš„ workflow_decisionï¼‰
            workflow_decision = llm_output.get('workflow_decision')
            if workflow_decision:
                debug_log(1, "[ModuleCoordinator] âš ï¸ LLM è¿”å›äº†èˆŠæ ¼å¼çš„ workflow_decisionï¼Œæ‡‰è©²ä½¿ç”¨ MCP function calling")
            
            # âœ… Cycle 0 å®Œæˆï¼ŒLLM å·²ç¶“è¿”å›åˆå§‹å›æ‡‰
            # âš ï¸ æ³¨æ„ï¼šå®Œæ•´çš„ Cycle 0 æ‡‰è©²åŒ…æ‹¬ï¼š
            #    1. å•Ÿå‹•å·¥ä½œæµ âœ“
            #    2. ç²å–ç¬¬ä¸€æ­¥ä¿¡æ¯ (TODO)
            #    3. LLM æè¿°ç¬¬ä¸€æ­¥çµ¦ç”¨æˆ¶ (TODO)
            #    4. ç­‰å¾…ç”¨æˆ¶å›æ‡‰æ‰é€²å…¥ Cycle 1
            
            info_log("[ModuleCoordinator] âœ“ WORK Cycle 0 å®Œæˆï¼ˆMCP æ¶æ§‹ï¼‰")
            return True
            
        except Exception as e:
            error_log(f"[ModuleCoordinator] WORK Cycle 0 è™•ç†å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _prepare_processing_requests(self, primary_target: str, input_data: Dict[str, Any]) -> List[ModuleInvocationRequest]:
        """æº–å‚™è™•ç†å±¤èª¿ç”¨è«‹æ±‚"""
        requests = []
        nlp_result = input_data.get('nlp_result', {})
        primary_intent = nlp_result.get('primary_intent')
        
        # å°å…¥ IntentType ä»¥é€²è¡Œæ­£ç¢ºçš„æšèˆ‰æ¯”è¼ƒ
        from modules.nlp_module.intent_types import IntentType
        
        # è™•ç†æšèˆ‰æˆ–å­—ç¬¦ä¸²å€¼
        intent_value = primary_intent.value if hasattr(primary_intent, 'value') else primary_intent
        intent_name = primary_intent.name if hasattr(primary_intent, 'name') else str(primary_intent)
        
        debug_log(2, f"[ModuleCoordinator] æº–å‚™è™•ç†å±¤è«‹æ±‚ - Intent: {intent_name} (value={intent_value})")
        
        # æ ¹æ“šæ„åœ–æ±ºå®šè™•ç†å±¤æ¨¡çµ„çµ„åˆ
        if primary_intent == IntentType.CHAT or intent_value == "chat":
            # CHATè·¯å¾‘ï¼šMEM + LLM
            info_log("[ModuleCoordinator] CHAT è·¯å¾‘: MEM + LLM")
            requests.extend([
                ModuleInvocationRequest(
                    target_module="mem",
                    input_data=self._prepare_mem_input(input_data),
                    source_module="input_layer",
                    reasoning="èŠå¤©æ¨¡å¼è¨˜æ†¶æŸ¥è©¢",
                    layer=ProcessingLayer.PROCESSING,
                    priority=4
                ),
                ModuleInvocationRequest(
                    target_module="llm",
                    input_data=self._prepare_llm_input(input_data),
                    source_module="input_layer", 
                    reasoning="èŠå¤©å°è©±ç”Ÿæˆ",
                    layer=ProcessingLayer.PROCESSING,
                    priority=3
                )
            ])
        elif primary_intent == IntentType.WORK or intent_value == "work":
            # WORKè·¯å¾‘ï¼šLLM + SYS (ä¸éœ€è¦ MEM)
            info_log("[ModuleCoordinator] WORK è·¯å¾‘: LLM + SYS")
            requests.extend([
                ModuleInvocationRequest(
                    target_module="llm",
                    input_data=self._prepare_llm_input(input_data),
                    source_module="input_layer",
                    reasoning="å·¥ä½œæ¨¡å¼ä»»å‹™åˆ†æ",
                    layer=ProcessingLayer.PROCESSING,
                    priority=4
                ),
                ModuleInvocationRequest(
                    target_module="sys",
                    input_data=self._prepare_sys_input(input_data),
                    source_module="input_layer",
                    reasoning="ç³»çµ±å·¥ä½œæµåŸ·è¡Œ",
                    layer=ProcessingLayer.PROCESSING,
                    priority=3
                )
            ])
        else:
            # é»˜èªï¼šä½¿ç”¨ä¸»è¦ç›®æ¨™
            info_log(f"[ModuleCoordinator] é»˜èªè·¯å¾‘: {primary_target}")
            requests.append(ModuleInvocationRequest(
                target_module=primary_target,
                input_data=self._prepare_module_input(primary_target, input_data),
                source_module="input_layer",
                reasoning=f"é»˜èªè™•ç†ï¼š{intent_name}",
                layer=ProcessingLayer.PROCESSING,
                priority=3
            ))
        
        return requests
    
    def _prepare_mem_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æº–å‚™MEMæ¨¡çµ„è¼¸å…¥"""
        nlp_result = input_data.get('nlp_result', {})
        return {
            "text": input_data.get('input_data', {}).get('text', ''),
            "source": "three_layer_coordinator",
            "operation": "store_and_retrieve",
            "memory_context": {
                "identity_id": nlp_result.get('identity', {}).get('identity_id'),
                "conversation_type": nlp_result.get('primary_intent'),
                "entities": nlp_result.get('entities', [])
            },
            "timestamp": input_data.get('timestamp', time.time()),
            "nlp_result": nlp_result
        }
    
    def _prepare_llm_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æº–å‚™LLMæ¨¡çµ„è¼¸å…¥
        
        é‡è¦æ¶æ§‹è¨­è¨ˆï¼š
        - ç¬¬ä¸€æ¬¡é€²å…¥è™•ç†å±¤ (cycle_index = 0)ï¼šå¾ç‹€æ…‹ä¸Šä¸‹æ–‡ (WorkflowSession) ç²å–å‘½ä»¤æ–‡æœ¬
        - ç¬¬äºŒæ¬¡ä»¥å¾Œ (cycle_index > 0)ï¼šå¾ Router ç²å–ç”¨æˆ¶å›æ‡‰æ–‡æœ¬
        """
        nlp_result = input_data.get('nlp_result', {})
        cycle_index = input_data.get('cycle_index', 0)
        primary_intent = nlp_result.get('primary_intent')
        
        # å°å…¥ IntentType ä»¥é€²è¡Œåˆ¤æ–·
        from modules.nlp_module.intent_types import IntentType
        intent_value = primary_intent.value if hasattr(primary_intent, 'value') else primary_intent
        
        # åˆ¤æ–·æ–‡æœ¬ä¾†æº
        if (primary_intent == IntentType.WORK or intent_value == "work") and cycle_index == 0:
            # âœ… Cycle 0: ä½¿ç”¨å®Œæ•´çš„åŸå§‹è¼¸å…¥æ–‡æœ¬é€²è¡Œæ±ºç­–
            # NLP å·²ç¶“åˆ†æéæ„åœ–ï¼Œä½† LLM éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡ä¾†ç†è§£å’Œæ±ºç­–
            input_text = input_data.get('input_data', {}).get('text', '')
            debug_log(2, f"[ModuleCoordinator] WORK Cycle 0 - ä½¿ç”¨å®Œæ•´åŸå§‹è¼¸å…¥: {input_text[:50]}...")
        else:
            # å…¶ä»–æƒ…æ³ï¼šå¾ Router ç²å–æ–‡æœ¬ï¼ˆCHAT è·¯å¾‘æˆ– WORK ç¬¬äºŒæ¬¡ä»¥å¾Œï¼‰
            input_text = input_data.get('input_data', {}).get('text', '')
            if cycle_index > 0:
                debug_log(2, f"[ModuleCoordinator] WORK cycle {cycle_index} - å¾ Router ç²å–ç”¨æˆ¶å›æ‡‰")
        
        # âœ… æ ¹æ“š primary_intent æ±ºå®š LLM æ¨¡å¼
        if primary_intent == IntentType.WORK or intent_value == "work":
            llm_mode = "work"
        else:
            llm_mode = "chat"
        
        # âœ… WORK Cycle 0: æå– NLP æ‰¾åˆ°çš„å·¥ä½œæµåŒ¹é…ä¿¡æ¯
        suggested_workflow = None
        if (primary_intent == IntentType.WORK or intent_value == "work") and cycle_index == 0:
            # å¾ NLP çš„ processing_notes æˆ– intent_segments ä¸­æå–å·¥ä½œæµæç¤º
            for note in nlp_result.get('processing_notes', []):
                if 'matching function' in note.lower() or 'workflow' in note.lower():
                    suggested_workflow = note
                    break
        
        base_data = {
            "text": input_text,
            "source": "three_layer_coordinator",
            "mode": llm_mode,  # âœ… æ·»åŠ  mode åƒæ•¸ï¼Œè®“ LLM çŸ¥é“æ˜¯ WORK é‚„æ˜¯ CHAT
            "conversation_type": nlp_result.get('primary_intent'),
            "user_input": input_text,
            "context": {
                "intent_segments": nlp_result.get('intent_segments', []),
                "entities": nlp_result.get('entities', []),
                "processing_notes": nlp_result.get('processing_notes', [])
            },
            "timestamp": input_data.get('timestamp', time.time()),
            "nlp_result": nlp_result,
            "identity": nlp_result.get('identity'),
            "intent": nlp_result.get('primary_intent'),
            "confidence": nlp_result.get('overall_confidence', 0.0),
            "cycle_index": cycle_index  # âœ… å‚³é cycle_index ä¾› LLM æ¨¡çµ„ä½¿ç”¨
        }
        
        # âœ… æ·»åŠ å·¥ä½œæµæç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
        if suggested_workflow:
            base_data['workflow_hint'] = suggested_workflow
        
        return base_data
    
    def _prepare_sys_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æº–å‚™SYSæ¨¡çµ„è¼¸å…¥
        
        é‡è¦æ¶æ§‹è¨­è¨ˆï¼š
        - ç¬¬ä¸€æ¬¡é€²å…¥è™•ç†å±¤ (cycle_index = 0)ï¼šå¾ç‹€æ…‹ä¸Šä¸‹æ–‡ (WorkflowSession) ç²å–å‘½ä»¤æ–‡æœ¬
        - ç¬¬äºŒæ¬¡ä»¥å¾Œ (cycle_index > 0)ï¼šå¾ Router ç²å–ç”¨æˆ¶å›æ‡‰æ–‡æœ¬
        """
        nlp_result = input_data.get('nlp_result', {})
        cycle_index = input_data.get('cycle_index', 0)
        
        # åˆ¤æ–·æ–‡æœ¬ä¾†æºï¼ˆèˆ‡ LLM ä½¿ç”¨ç›¸åŒé‚è¼¯ï¼‰
        if cycle_index == 0:
            # âœ… Cycle 0: ä½¿ç”¨å®Œæ•´çš„åŸå§‹è¼¸å…¥æ–‡æœ¬
            input_text = input_data.get('input_data', {}).get('text', '')
            debug_log(2, f"[ModuleCoordinator] WORK Cycle 0 - SYS ä½¿ç”¨å®Œæ•´åŸå§‹è¼¸å…¥: {input_text[:50]}...")
        else:
            # ç¬¬äºŒæ¬¡ä»¥å¾Œï¼šå¾ Router ç²å–ç”¨æˆ¶å›æ‡‰
            input_text = input_data.get('input_data', {}).get('text', '')
            debug_log(2, f"[ModuleCoordinator] WORK cycle {cycle_index} - SYS å¾ Router ç²å–æ–‡æœ¬")
        
        return {
            "text": input_text,
            "source": "three_layer_coordinator",
            "mode": "workflow",  # âœ… æ·»åŠ  mode åƒæ•¸ï¼ŒSYS æ¨¡çµ„å¿…éœ€
            "operation": "workflow_execution",
            "system_context": {
                "intent": nlp_result.get('primary_intent'),
                "entities": nlp_result.get('entities', []),
                "command_type": "work_task"
            },
            "timestamp": input_data.get('timestamp', time.time()),
            "nlp_result": nlp_result,
            "cycle_index": cycle_index  # âœ… å‚³é cycle_index ä¾› SYS æ¨¡çµ„ä½¿ç”¨
        }
    
    def _prepare_output_input(self, processing_data: Dict[str, Any]) -> Dict[str, Any]:
        """æº–å‚™è¼¸å‡ºå±¤ï¼ˆTTSï¼‰è¼¸å…¥"""
        return {
            "text": processing_data.get('response', processing_data.get('text', '')),
            "source": "three_layer_coordinator",
            "output_mode": "voice",
            "timestamp": time.time(),
            "processing_result": processing_data
        }
    
    def _prepare_module_input(self, target_module: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç‚ºç‰¹å®šæ¨¡çµ„æº–å‚™è¼¸å…¥æ•¸æ“šï¼ˆé€šç”¨æ–¹æ³•ï¼‰"""
        if target_module == "mem":
            return self._prepare_mem_input(input_data)
        elif target_module == "llm":
            return self._prepare_llm_input(input_data)
        elif target_module == "sys":
            return self._prepare_sys_input(input_data)
        elif target_module == "tts":
            return self._prepare_output_input(input_data)
        else:
            # é€šç”¨è¼¸å…¥æ ¼å¼
            nlp_result = input_data.get('nlp_result', {})
            return {
                "text": input_data.get('input_data', {}).get('text', ''),
                "source": "three_layer_coordinator",
                "timestamp": input_data.get('timestamp', time.time()),
                "nlp_result": nlp_result,
                "intent": nlp_result.get('primary_intent'),
                "confidence": nlp_result.get('overall_confidence', 0.0)
            }
    
    def invoke_module(self, request: ModuleInvocationRequest) -> ModuleInvocationResponse:
        """
        èª¿ç”¨ç›®æ¨™æ¨¡çµ„
        
        Args:
            request: æ¨¡çµ„èª¿ç”¨è«‹æ±‚
            
        Returns:
            ModuleInvocationResponse: èª¿ç”¨å›æ‡‰
        """
        start_time = time.time()
        
        with self._invocation_lock:
            try:
                info_log(f"[ModuleCoordinator] èª¿ç”¨{request.layer.value}å±¤æ¨¡çµ„: {request.target_module}")
                debug_log(2, f"[ModuleCoordinator] èª¿ç”¨åŸå› : {request.reasoning}")
                debug_log(3, f"[ModuleCoordinator] è¼¸å…¥æ•¸æ“š: {list(request.input_data.keys())}")
                
                # ç²å–ç›®æ¨™æ¨¡çµ„
                from core.framework import core_framework
                target_module = core_framework.get_module(request.target_module)
                
                if not target_module:
                    error_msg = f"ç„¡æ³•æ‰¾åˆ°ç›®æ¨™æ¨¡çµ„: {request.target_module}"
                    error_log(f"[ModuleCoordinator] {error_msg}")
                    return ModuleInvocationResponse(
                        target_module=request.target_module,
                        result=InvocationResult.NO_TARGET,
                        layer=request.layer,
                        error_message=error_msg,
                        execution_time=time.time() - start_time
                    )
                
                # è¨˜éŒ„æ´»èºèª¿ç”¨
                invocation_id = f"{request.target_module}_{int(time.time() * 1000)}"
                self._active_invocations[invocation_id] = {
                    "target": request.target_module,
                    "layer": request.layer.value,
                    "start_time": start_time,
                    "source": request.source_module
                }
                
                # å¯¦éš›èª¿ç”¨æ¨¡çµ„
                result_data = target_module.handle(request.input_data)
                
                # ç§»é™¤æ´»èºèª¿ç”¨è¨˜éŒ„
                if invocation_id in self._active_invocations:
                    del self._active_invocations[invocation_id]
                
                execution_time = time.time() - start_time
                
                # âœ… æ­£ç¢ºåˆ¤æ–·æˆåŠŸ: æª¢æŸ¥ result_data['success'] å­—æ®µ
                is_success = result_data and isinstance(result_data, dict) and result_data.get('success', False)
                
                if result_data:
                    # è¨˜éŒ„æ¨¡çµ„è¿”å›çµæœ
                    self._log_module_result(request.target_module, result_data)
                    
                    # æ ¹æ“š success å­—æ®µæ±ºå®šçµæœç‹€æ…‹
                    if is_success:
                        info_log(f"[ModuleCoordinator] {request.layer.value}å±¤æ¨¡çµ„ {request.target_module} è™•ç†å®Œæˆ ({execution_time:.3f}s)")
                        invocation_result = InvocationResult.SUCCESS
                    else:
                        # success=False è¦–ç‚ºå¤±æ•—,ä¸æ˜¯æˆåŠŸ
                        error_msg = result_data.get('error', 'æœªçŸ¥éŒ¯èª¤')
                        debug_log(2, f"[ModuleCoordinator] {request.layer.value}å±¤æ¨¡çµ„ {request.target_module} è™•ç†å¤±æ•—: {error_msg}")
                        invocation_result = InvocationResult.FAILED
                    
                    response = ModuleInvocationResponse(
                        target_module=request.target_module,
                        result=invocation_result,
                        layer=request.layer,
                        output_data=result_data,
                        execution_time=execution_time
                    )
                    
                    # åªæœ‰çœŸæ­£æˆåŠŸæ™‚æ‰æª¢æŸ¥å±¤ç´šå®Œæˆ
                    if is_success:
                        self._check_layer_completion(request.layer, result_data)
                    
                else:
                    debug_log(2, f"[ModuleCoordinator] {request.layer.value}å±¤æ¨¡çµ„ {request.target_module} ç„¡è¿”å›çµæœ")
                    response = ModuleInvocationResponse(
                        target_module=request.target_module,
                        result=InvocationResult.FAILED,  # ç„¡è¿”å›çµæœè¦–ç‚ºå¤±æ•—
                        layer=request.layer,
                        output_data=None,
                        execution_time=execution_time
                    )
                
                # è¨˜éŒ„èª¿ç”¨æ­·å²
                self._invocation_history.append({
                    "timestamp": time.time(),
                    "target_module": request.target_module,
                    "layer": request.layer.value,
                    "source_module": request.source_module,
                    "result": response.result.value,
                    "execution_time": execution_time
                })
                
                # ä¿æŒæ­·å²è¨˜éŒ„åœ¨åˆç†ç¯„åœå…§
                if len(self._invocation_history) > 100:
                    self._invocation_history = self._invocation_history[-50:]
                
                return response
                
            except Exception as e:
                execution_time = time.time() - start_time
                error_msg = f"èª¿ç”¨æ¨¡çµ„å¤±æ•—: {e}"
                error_log(f"[ModuleCoordinator] {error_msg}")
                
                # æ¸…ç†æ´»èºèª¿ç”¨è¨˜éŒ„
                invocation_id = f"{request.target_module}_{int(start_time * 1000)}"
                if invocation_id in self._active_invocations:
                    del self._active_invocations[invocation_id]
                
                return ModuleInvocationResponse(
                    target_module=request.target_module,
                    result=InvocationResult.FAILED,
                    layer=request.layer,
                    error_message=error_msg,
                    execution_time=execution_time
                )
    
    def _check_layer_completion(self, current_layer: ProcessingLayer, result_data: Dict[str, Any]):
        """æª¢æŸ¥ç•¶å‰å±¤æ˜¯å¦å®Œæˆï¼Œæ±ºå®šæ˜¯å¦è§¸ç™¼ä¸‹ä¸€å±¤"""
        try:
            # ç°¡åŒ–ç‰ˆæœ¬ï¼šç›´æ¥æª¢æŸ¥çµæœæ˜¯å¦åŒ…å«éœ€è¦å‚³éçš„æ•¸æ“š
            if result_data and 'response' in result_data:
                debug_log(2, f"[ModuleCoordinator] {current_layer.value}å±¤è™•ç†å®Œæˆï¼Œæº–å‚™è§¸ç™¼ä¸‹ä¸€å±¤")
                # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒæ ¹æ“šå…·é«”é‚è¼¯æ±ºå®šæ˜¯å¦è§¸ç™¼ä¸‹ä¸€å±¤
                # ç›®å‰ç°¡åŒ–ç‚ºæ—¥èªŒè¨˜éŒ„
            
        except Exception as e:
            debug_log(3, f"[ModuleCoordinator] æª¢æŸ¥å±¤ç´šå®Œæˆç‹€æ…‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def invoke_multiple_modules(self, requests: List[ModuleInvocationRequest]) -> List[ModuleInvocationResponse]:
        """
        æ‰¹é‡èª¿ç”¨å¤šå€‹æ¨¡çµ„
        
        Args:
            requests: æ¨¡çµ„èª¿ç”¨è«‹æ±‚åˆ—è¡¨
            
        Returns:
            List[ModuleInvocationResponse]: èª¿ç”¨å›æ‡‰åˆ—è¡¨
        """
        info_log(f"[ModuleCoordinator] æ‰¹é‡èª¿ç”¨ {len(requests)} å€‹æ¨¡çµ„")
        
        responses = []
        for request in requests:
            response = self.invoke_module(request)
            responses.append(response)
            
            # å¦‚æœæœ‰ä»»ä½•é—œéµæ¨¡çµ„èª¿ç”¨å¤±æ•—ï¼Œè€ƒæ…®çµ‚æ­¢å¾ŒçºŒèª¿ç”¨
            if response.result == InvocationResult.FAILED and request.priority >= 5:
                error_log(f"[ModuleCoordinator] é—œéµæ¨¡çµ„ {request.target_module} èª¿ç”¨å¤±æ•—ï¼Œçµ‚æ­¢å¾ŒçºŒèª¿ç”¨")
                break
        
        return responses
    
    def _log_module_result(self, module_name: str, result_data: Any):
        """è¨˜éŒ„æ¨¡çµ„è¿”å›çµæœçš„è©³ç´°ä¿¡æ¯"""
        try:
            # ç°¡åŒ–æ—¥èªŒï¼šç›´æ¥è¼¸å‡ºæ•´å€‹çµæœå­—å…¸
            debug_log(3, f"[ModuleCoordinator] {module_name} å®Œæ•´è¿”å›çµæœ: {result_data}")
                
        except Exception as e:
            debug_log(3, f"[ModuleCoordinator] è¨˜éŒ„ {module_name} çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _extract_response_text(self, processing_data: Dict[str, Any]) -> str:
        """å¾è™•ç†å±¤æ•¸æ“šä¸­æå–æ–‡å­—å›æ‡‰"""
        try:
            # å„ªå…ˆé †åºï¼šresponse > text > content
            if "response" in processing_data:
                return processing_data["response"]
            elif "text" in processing_data:
                return processing_data["text"]
            elif "content" in processing_data:
                return processing_data["content"]
            else:
                # å¦‚æœæ˜¯åµŒå¥—çµæ§‹ï¼Œå˜—è©¦æ·±åº¦æå–
                if isinstance(processing_data, dict):
                    for key in ["llm_output", "result", "data"]:
                        if key in processing_data:
                            nested_data = processing_data[key]
                            if isinstance(nested_data, dict):
                                if "text" in nested_data:
                                    return nested_data["text"]
                                elif "response" in nested_data:
                                    return nested_data["response"]
                
                debug_log(2, f"[ModuleCoordinator] ç„¡æ³•å¾è™•ç†å±¤æ•¸æ“šä¸­æå–æ–‡å­—: {list(processing_data.keys())}")
                return ""
                
        except Exception as e:
            error_log(f"[ModuleCoordinator] æå–å›æ‡‰æ–‡å­—å¤±æ•—: {e}")
            return ""
    
    def get_active_invocations(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰æ´»èºçš„èª¿ç”¨ç‹€æ…‹"""
        return dict(self._active_invocations)
    
    def get_invocation_stats(self) -> Dict[str, Any]:
        """ç²å–èª¿ç”¨çµ±è¨ˆä¿¡æ¯"""
        if not self._invocation_history:
            return {
                "total_invocations": 0,
                "avg_execution_time": 0.0,
                "success_rate": 0.0,
                "module_stats": {}
            }
        
        total = len(self._invocation_history)
        successful = sum(1 for h in self._invocation_history if h["result"] == "success")
        avg_time = sum(h["execution_time"] for h in self._invocation_history) / total
        
        # æ¨¡çµ„çµ±è¨ˆ
        module_stats = {}
        for history in self._invocation_history:
            module = history["target_module"]
            if module not in module_stats:
                module_stats[module] = {"count": 0, "success": 0, "avg_time": 0.0}
            
            module_stats[module]["count"] += 1
            if history["result"] == "success":
                module_stats[module]["success"] += 1
            module_stats[module]["avg_time"] = (
                module_stats[module]["avg_time"] * (module_stats[module]["count"] - 1) + 
                history["execution_time"]
            ) / module_stats[module]["count"]
        
        return {
            "total_invocations": total,
            "avg_execution_time": avg_time,
            "success_rate": successful / total,
            "active_invocations": len(self._active_invocations),
            "module_stats": module_stats
        }
    
    def _handle_session_end(self, session_control: Dict[str, Any]):
        """
        ğŸ†• è™•ç†æœƒè©±çµæŸè«‹æ±‚ï¼ˆé›™æ¢ä»¶çµ‚æ­¢æ©Ÿåˆ¶ï¼‰
        
        ç•¶ LLM æ±ºå®šçµæŸæœƒè©±æ™‚ï¼ˆé€šé session_controlï¼‰ï¼Œä¸¦ä¸”å¾ªç’°å·²å®Œæˆï¼ˆCYCLE_COMPLETEDï¼‰ï¼š
        1. çµæŸæ‰€æœ‰æ´»èºçš„ WSï¼ˆworkflow sessionï¼‰
        2. çµæŸæ‰€æœ‰æ´»èºçš„ CSï¼ˆchatting sessionï¼‰
        
        æ³¨æ„ï¼š
        - âŒ ä¸çµæŸ GSï¼GS æ˜¯ç³»çµ±å±¤ç´šæœƒè©±ï¼Œç”± Controller ç®¡ç†
        - âœ… GS çµæŸæ¢ä»¶ï¼šç‹€æ…‹ä½‡åˆ—æ¸…ç©º + ç³»çµ±å›åˆ° IDLEï¼ˆç”± Controller.check_gs_end_conditions æª¢æŸ¥ï¼‰
        - âœ… CS/WS çµæŸæœƒç™¼å¸ƒ SESSION_ENDED äº‹ä»¶ï¼ŒStateManager ç›£è½ä¸¦é€šçŸ¥ StateQueue
        - âœ… StateQueue è‡ªå‹•è™•ç†ä¸‹ä¸€å€‹ç‹€æ…‹ï¼ˆæˆ–å›åˆ° IDLEï¼‰
        
        Args:
            session_control: æœƒè©±æ§åˆ¶æŒ‡ä»¤
        """
        try:
            info_log("[ModuleCoordinator] è™•ç† CS/WS çµæŸè«‹æ±‚")
            
            # ç²å–æ‰€æœ‰æ´»èºçš„å­æœƒè©±
            from core.sessions.session_manager import unified_session_manager
            
            # çµæŸæ‰€æœ‰å·¥ä½œæµæœƒè©± (WS)
            active_ws = unified_session_manager.get_active_workflow_session_ids()
            for ws_id in active_ws:
                debug_log(2, f"[ModuleCoordinator] çµæŸå·¥ä½œæµæœƒè©±: {ws_id}")
                unified_session_manager.end_workflow_session(ws_id)
            
            # çµæŸæ‰€æœ‰èŠå¤©æœƒè©± (CS)
            active_cs = unified_session_manager.get_active_chatting_session_ids()
            for cs_id in active_cs:
                debug_log(2, f"[ModuleCoordinator] çµæŸèŠå¤©æœƒè©±: {cs_id}")
                unified_session_manager.end_chatting_session(cs_id)
            
            # âš ï¸ é‡è¦ï¼šä¸çµæŸ GSï¼
            # GS ç”Ÿå‘½é€±æœŸï¼š
            #   - å‰µå»ºï¼šé€²å…¥é IDLE ç‹€æ…‹æ™‚ï¼ˆç”± Controller._monitor_gs_lifecycleï¼‰
            #   - çµæŸï¼šç‹€æ…‹ä½‡åˆ—æ¸…ç©º + IDLE ç‹€æ…‹ï¼ˆç”± Controller.check_gs_end_conditionsï¼‰
            # 
            # CS/WS çµæŸå¾Œï¼š
            #   â†’ ç™¼å¸ƒ SESSION_ENDED äº‹ä»¶
            #   â†’ StateManager ç›£è½ä¸¦é€šçŸ¥ StateQueue.complete_current_state()
            #   â†’ StateQueue è™•ç†ä¸‹ä¸€å€‹ç‹€æ…‹ï¼ˆè‹¥æœ‰ï¼‰æˆ–è½‰æ›åˆ° IDLEï¼ˆè‹¥ä½‡åˆ—ç‚ºç©ºï¼‰
            #   â†’ ç•¶ StateQueue ç‚ºç©ºä¸” IDLE æ™‚ï¼ŒController æ‰çµæŸ GS
            
            info_log("[ModuleCoordinator] âœ… CS/WS çµæŸå®Œæˆï¼Œç­‰å¾…ç‹€æ…‹è½‰æ›")
            
        except Exception as e:
            error_log(f"[ModuleCoordinator] è™•ç†æœƒè©±çµæŸå¤±æ•—: {e}")
    
    def get_deduplication_stats(self) -> Dict[str, Any]:
        """
        ç²å–å»é‡çµ±è¨ˆä¿¡æ¯ (G. ç›£æ§èˆ‡é™¤éŒ¯)
        
        Returns:
            åŒ…å«å»é‡å‘½ä¸­æ¬¡æ•¸ã€æ¸…ç†æ¬¡æ•¸ã€æ´»èºéµæ•¸é‡ç­‰è¨ºæ–·ä¿¡æ¯
        """
        with self._dedupe_lock:
            # åˆ†ææ´»èºçš„ dedupe keys
            active_flows = set()
            layers_count = {"INPUT": 0, "PROCESSING": 0, "OUTPUT": 0}
            
            for key in self._layer_dedupe_keys:
                try:
                    parts = key.split(":")
                    if len(parts) >= 3:
                        session_id = parts[0]
                        cycle_index = parts[1]
                        layer = parts[2]
                        
                        flow_id = f"{session_id}:{cycle_index}"
                        active_flows.add(flow_id)
                        
                        if layer in layers_count:
                            layers_count[layer] += 1
                except:
                    pass
            
            return {
                "dedupe_hit_count": self._dedupe_hit_count,
                "cleanup_count": self._cleanup_count,
                "active_dedupe_keys": len(self._layer_dedupe_keys),
                "max_dedupe_keys": self._max_dedupe_keys,
                "active_flows": len(active_flows),
                "layers_distribution": layers_count,
                "memory_pressure": len(self._layer_dedupe_keys) / self._max_dedupe_keys if self._max_dedupe_keys > 0 else 0.0
            }


# å…¨å±€å”èª¿å™¨å¯¦ä¾‹
module_coordinator = ModuleInvocationCoordinator()