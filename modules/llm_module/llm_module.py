# modules/llm_module/llm_module.py
"""
LLM æ¨¡çµ„é‡æ§‹ç‰ˆæœ¬

æ–°åŠŸèƒ½ï¼š
1. æ”¯æ´ CHAT å’Œ WORK ç‹€æ…‹åˆ†é›¢è™•ç†
2. æ•´åˆ StatusManager ç³»çµ±æ•¸å€¼ç®¡ç†
3. Context Caching ä¸Šä¸‹æ–‡å¿«å–
4. å­¸ç¿’åŠŸèƒ½ï¼šè¨˜éŒ„ä½¿ç”¨è€…åå¥½å’Œå°è©±é¢¨æ ¼
5. èˆ‡ Working Context å’Œèº«ä»½ç®¡ç†ç³»çµ±æ•´åˆ
6. å…§å»º Prompt ç®¡ç†ï¼Œä¸å†ä¾è³´å¤–éƒ¨ prompt_builder
"""

import re
import time
import json
from typing import Dict, Any, Optional, List
from pathlib import Path

from core.bases.module_base import BaseModule
from core.working_context import working_context_manager, ContextType
from core.status_manager import status_manager
from core.states.state_manager import state_manager, UEPState

from .schemas import (
    LLMInput, LLMOutput, SystemAction, LLMMode, SystemState,
    ConversationEntry, LearningData, StatusUpdate
)
from .gemini_client import GeminiWrapper
from .prompt_manager import PromptManager
from .learning_engine import LearningEngine
from .cache_manager import cache_manager, CacheType
from .module_interfaces import (
    state_aware_interface, CollaborationChannel, set_collaboration_state
)

from configs.config_loader import load_module_config
from utils.debug_helper import debug_log, info_log, error_log


class LLMModule(BaseModule):
    def __init__(self, config=None):
        super().__init__()
        self.config = config or load_module_config("llm_module")
        
        # æ ¸å¿ƒçµ„ä»¶
        self.model = GeminiWrapper(self.config)
        self.prompt_manager = PromptManager(self.config)
        self.learning_engine = LearningEngine(self.config.get("learning", {}))
        
        # çµ±ä¸€å¿«å–ç®¡ç†å™¨ (æ•´åˆGeminié¡¯æ€§å¿«å– + æœ¬åœ°å¿«å–)
        self.cache_manager = cache_manager
        
        # ç‹€æ…‹å’Œæœƒè©±ç®¡ç†
        self.state_manager = state_manager
        self.status_manager = status_manager
        self.session_info = {}
        
        # ç‹€æ…‹æ„ŸçŸ¥æ¨¡çµ„æ¥å£
        self.module_interface = state_aware_interface
        
        # ç›£è½ç³»çµ±ç‹€æ…‹è®ŠåŒ–ä»¥è‡ªå‹•åˆ‡æ›å”ä½œç®¡é“
        self._setup_state_listener()
        
        # çµ±è¨ˆæ•¸æ“š
        self.processing_stats = {
            "total_requests": 0,
            "chat_requests": 0, 
            "work_requests": 0,
            "total_processing_time": 0.0,
            "cache_hits": 0
        }

    def debug(self):
        # Debug level = 1
        debug_log(1, "[LLM] Debug æ¨¡å¼å•Ÿç”¨ - é‡æ§‹ç‰ˆæœ¬")
        # Debug level = 2  
        debug_log(2, f"[LLM] æ¨¡å‹åç¨±: {self.model.model_name}")
        debug_log(2, f"[LLM] æº«åº¦: {self.model.temperature}")
        debug_log(2, f"[LLM] Top P: {self.model.top_p}")
        debug_log(2, f"[LLM] æœ€å¤§è¼¸å‡ºå­—å…ƒæ•¸: {self.model.max_tokens}")
        debug_log(2, f"[LLM] çµ±ä¸€å¿«å–ç®¡ç†å™¨: å•Ÿç”¨ (Gemini + æœ¬åœ°å¿«å–)")
        debug_log(2, f"[LLM] Learning Engine: {'å•Ÿç”¨' if self.learning_engine.learning_enabled else 'åœç”¨'}")
        # Debug level = 4
        debug_log(4, f"[LLM] å®Œæ•´æ¨¡çµ„è¨­å®š: {self.config}")
    
    def _setup_state_listener(self):
        """è¨­å®šç³»çµ±ç‹€æ…‹ç›£è½å™¨ï¼Œè‡ªå‹•åˆ‡æ›å”ä½œç®¡é“"""
        try:
            # ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹ä¸¦è¨­å®šåˆå§‹å”ä½œç®¡é“
            current_state = self.state_manager.get_current_state()
            set_collaboration_state(current_state)
            
            debug_log(2, f"[LLM] ç‹€æ…‹æ„ŸçŸ¥æ¨¡çµ„æ¥å£è¨­å®šå®Œæˆï¼Œåˆå§‹ç‹€æ…‹: {current_state}")
            debug_log(3, f"[LLM] ç®¡é“ç‹€æ…‹: {self.module_interface.get_channel_status()}")
            
        except Exception as e:
            error_log(f"[LLM] ç‹€æ…‹ç›£è½å™¨è¨­å®šå¤±æ•—: {e}")
    
    def _update_collaboration_channels(self, new_state: UEPState):
        """æ ¹æ“šç³»çµ±ç‹€æ…‹æ›´æ–°å”ä½œç®¡é“"""
        try:
            old_status = self.module_interface.get_channel_status()
            set_collaboration_state(new_state)
            new_status = self.module_interface.get_channel_status()
            
            if old_status != new_status:
                debug_log(2, f"[LLM] å”ä½œç®¡é“æ›´æ–°: {old_status} â†’ {new_status}")
                
        except Exception as e:
            error_log(f"[LLM] å”ä½œç®¡é“æ›´æ–°å¤±æ•—: {e}")
        
    def initialize(self):
        """åˆå§‹åŒ– LLM æ¨¡çµ„"""
        debug_log(1, "[LLM] åˆå§‹åŒ–ä¸­...")
        self.debug()
        
        try:
            # Gemini å®¢æˆ¶ç«¯åœ¨ __init__ ä¸­å·²ç¶“åˆå§‹åŒ–ï¼Œæª¢æŸ¥æ˜¯å¦æ­£å¸¸
            if not hasattr(self.model, 'client') or self.model.client is None:
                error_log("[LLM] Gemini æ¨¡å‹åˆå§‹åŒ–å¤±æ•—")
                return False
            
            # è¨»å†Š StatusManager å›èª¿
            self.status_manager.register_update_callback("llm_module", self._on_status_update)
            
            # ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹
            current_state = self.state_manager.get_current_state()
            debug_log(2, f"[LLM] ç•¶å‰ç³»çµ±ç‹€æ…‹: {current_state}")
            
            self.is_initialized = True
            info_log("[LLM] LLM æ¨¡çµ„é‡æ§‹ç‰ˆåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            error_log(f"[LLM] åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
        
    def handle(self, data: dict) -> dict:
        """ä¸»è¦è™•ç†æ–¹æ³• - é‡æ§‹ç‰ˆæœ¬ï¼Œæ”¯æ´æ–°çš„ CHAT/WORK æ¨¡å¼å’Œæ–° Router æ•´åˆ"""
        start_time = time.time()
        
        try:
            # è§£æè¼¸å…¥ç‚ºæ–°æ¶æ§‹
            llm_input = LLMInput(**data)
            info_log(f"[LLM] é–‹å§‹è™•ç†è«‹æ±‚ - æ¨¡å¼: {llm_input.mode}, ç”¨æˆ¶è¼¸å…¥: {llm_input.text[:50]}...")
            debug_log(1, f"[LLM] è™•ç†è¼¸å…¥ - æ¨¡å¼: {llm_input.mode}, ç”¨æˆ¶è¼¸å…¥: {llm_input.text[:100]}...")
            
            # æª¢æŸ¥æ˜¯å¦ä¾†è‡ªæ–° Router
            if llm_input.source_layer:
                info_log(f"[LLM] ä¾†è‡ªæ–°Router - ä¾†æºå±¤ç´š: {llm_input.source_layer}")
                debug_log(2, f"[LLM] ä¾†è‡ªæ–°Router - ä¾†æºå±¤ç´š: {llm_input.source_layer}")
                if llm_input.processing_context:
                    debug_log(3, f"[LLM] è™•ç†å±¤ä¸Šä¸‹æ–‡: {llm_input.processing_context}")
            
            # 1. ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹å’Œæœƒè©±ä¿¡æ¯
            current_state = self.state_manager.get_current_state()
            info_log(f"[LLM] ç•¶å‰ç³»çµ±ç‹€æ…‹: {current_state}")
            
            # 1.1 æ›´æ–°å”ä½œç®¡é“ï¼ˆç¢ºä¿èˆ‡ç³»çµ±ç‹€æ…‹åŒæ­¥ï¼‰
            self._update_collaboration_channels(current_state)
            
            status = self._get_current_system_status()
            self.session_info = self._get_current_session_info()
            
            # 1.2 æœƒè©±æ¶æ§‹æª¢æŸ¥ - LLM ä¸æ‡‰è©²åœ¨æ²’æœ‰é©ç•¶æœƒè©±çš„æƒ…æ³ä¸‹é‹ä½œ
            if not self._validate_session_architecture(current_state):
                error_log("[LLM] æœƒè©±æ¶æ§‹é•è¦ - æ‹’çµ•è™•ç†è«‹æ±‚")
                return {
                    "status": "error",
                    "message": "æœƒè©±æ¶æ§‹é•è¦ï¼šLLM éœ€è¦é©ç•¶çš„æœƒè©±ä¸Šä¸‹æ–‡",
                    "error_type": "session_architecture_violation",
                    "timestamp": time.time()
                }
            
            # 2. è™•ç†èº«ä»½ä¸Šä¸‹æ–‡ (å„ªå…ˆä½¿ç”¨ä¾†è‡ªRouterçš„)
            if llm_input.identity_context:
                identity_context = llm_input.identity_context
                debug_log(2, f"[LLM] ä½¿ç”¨Routeræä¾›çš„Identityä¸Šä¸‹æ–‡: {identity_context}")
            else:
                identity_context = self._get_identity_context()
                debug_log(2, f"[LLM] ä½¿ç”¨æœ¬åœ°Identityä¸Šä¸‹æ–‡: {identity_context}")
            
            debug_log(2, f"[LLM] ç³»çµ±ç‹€æ…‹: {current_state}")
            debug_log(2, f"[LLM] StatusManager: {status}")
            debug_log(2, f"[LLM] æœƒè©±ä¿¡æ¯: {self.session_info}")
            
            # 3. è£œå……ç³»çµ±ä¸Šä¸‹æ–‡åˆ°llm_input (æ•´åˆRouteræ•¸æ“š)
            llm_input = self._enrich_with_system_context(
                llm_input, current_state, status, self.session_info, identity_context
            )
            
            # æ ¹æ“šæ¨¡å¼åˆ‡æ›è™•ç†é‚è¼¯
            if llm_input.mode == LLMMode.CHAT:
                output = self._handle_chat_mode(llm_input, status)
            elif llm_input.mode == LLMMode.WORK:
                output = self._handle_work_mode(llm_input, status)
            else:
                # å‘å¾Œå…¼å®¹èˆŠçš„ intent ç³»çµ±
                output = self._handle_legacy_mode(llm_input, status)
            
            # è½‰æ›ç‚ºå­—å…¸æ ¼å¼è¿”å›ï¼ˆä¿æŒèˆ‡èˆŠç³»çµ±çš„å…¼å®¹ï¼‰
            result = output.dict()
            result["status"] = "ok" if output.success else "error"
            
            return result
                
        except Exception as e:
            error_log(f"[LLM] è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                "text": "è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                "processing_time": time.time() - start_time,
                "tokens_used": 0,
                "success": False,
                "error": str(e),
                "confidence": 0.0,
                "metadata": {},
                "status": "error"
            }
    
    def _handle_chat_mode(self, llm_input: "LLMInput", status: Dict[str, Any]) -> "LLMOutput":
        """è™•ç† CHAT æ¨¡å¼ - èˆ‡ MEM å”ä½œçš„æ—¥å¸¸å°è©±"""
        start_time = time.time()
        debug_log(2, "[LLM] è™•ç† CHAT æ¨¡å¼")
        
        try:
            # 1. MEM å”ä½œï¼šæª¢ç´¢ç›¸é—œè¨˜æ†¶ (CHATç‹€æ…‹å°ˆç”¨)
            relevant_memories = []
            if not llm_input.memory_context:  # åªæœ‰åœ¨æ²’æœ‰æä¾›è¨˜æ†¶ä¸Šä¸‹æ–‡æ™‚æ‰æª¢ç´¢
                relevant_memories = self._retrieve_relevant_memory(llm_input.text, max_results=5)
                if relevant_memories:
                    debug_log(2, f"[LLM] æ•´åˆ {len(relevant_memories)} æ¢ç›¸é—œè¨˜æ†¶åˆ°å°è©±ä¸Šä¸‹æ–‡")
                    # å°‡æª¢ç´¢åˆ°çš„è¨˜æ†¶è½‰æ›ç‚ºè¨˜æ†¶ä¸Šä¸‹æ–‡
                    llm_input.memory_context = self._format_memories_for_context(relevant_memories)
            
            # 2. æª¢æŸ¥ Context Cache (åŒ…å«å‹•æ…‹è¨˜æ†¶)
            import hashlib
            base = f"{llm_input.mode}|{self.session_info.get('session_id','')}"
            text_sig = hashlib.sha256(llm_input.text.encode("utf-8")).hexdigest()[:16]
            mem_sig  = hashlib.sha256((llm_input.memory_context or "").encode("utf-8")).hexdigest()[:16]
            cache_key = f"chat:{base}:{text_sig}:{mem_sig}:{len(relevant_memories)}"
            cached_response = self.cache_manager.get_cached_response(cache_key)
            
            if cached_response and not llm_input.ignore_cache:
                debug_log(2, "[LLM] ä½¿ç”¨å¿«å–å›æ‡‰ï¼ˆåŒ…å«è¨˜æ†¶ä¸Šä¸‹æ–‡ï¼‰")
                return cached_response
            
            # 3. æ§‹å»º CHAT æç¤ºï¼ˆæ•´åˆè¨˜æ†¶ï¼‰
            prompt = self.prompt_manager.build_chat_prompt(
                user_input=llm_input.text,
                identity_context=llm_input.identity_context,
                memory_context=llm_input.memory_context,
                conversation_history=getattr(llm_input, 'conversation_history', None),
                is_internal=False,
                relevant_memories=relevant_memories  # æ–°å¢ï¼šå‚³å…¥æª¢ç´¢åˆ°çš„è¨˜æ†¶
            )
            
            # 3. ç²å–æˆ–å‰µå»ºç³»çµ±å¿«å–
            cached_content_ids = self._get_system_caches("chat")
            
            # 4. å‘¼å« Gemini API (ä½¿ç”¨å¿«å–)
            response_data = self.model.query(
                prompt, 
                mode="chat",
                cached_content=cached_content_ids.get("persona")
            )
            response_text = response_data.get("text", "")
            
            # === è©³ç´°å›æ‡‰æ—¥èªŒ ===
            info_log(f"[LLM] ğŸ¤– Geminiå›æ‡‰: {response_text}")
            debug_log(1, f"[LLM] ğŸ“Š å›æ‡‰ä¿¡å¿ƒåº¦: {response_data.get('confidence', 'N/A')}")
            
            # è¨˜æ†¶è§€å¯Ÿæ—¥èªŒ
            if response_data.get("memory_observation"):
                debug_log(1, f"[LLM] ğŸ’­ è¨˜æ†¶è§€å¯Ÿ: {response_data['memory_observation']}")
            
            # ç‹€æ…‹æ›´æ–°æ—¥èªŒ
            status_updates = response_data.get("status_updates")
            if status_updates:
                debug_log(1, f"[LLM] ğŸ“ˆ å»ºè­°ç‹€æ…‹æ›´æ–°:")
                for key, value in status_updates.items():
                    if value is not None:
                        debug_log(1, f"[LLM]   {key}: {value:+.2f}" if isinstance(value, (int, float)) else f"[LLM]   {key}: {value}")
            
            # å­¸ç¿’ä¿¡è™Ÿæ—¥èªŒ
            learning_signals = response_data.get("learning_signals")
            if learning_signals:
                debug_log(1, f"[LLM] ğŸ§  å­¸ç¿’ä¿¡è™Ÿ:")
                for signal_type, value in learning_signals.items():
                    if value is not None:
                        debug_log(1, f"[LLM]   {signal_type}: {value:+.2f}")
            
            # æœƒè©±æ§åˆ¶æ—¥èªŒ
            session_control = response_data.get("session_control")
            if session_control:
                debug_log(1, f"[LLM] ğŸ® æœƒè©±æ§åˆ¶å»ºè­°:")
                debug_log(1, f"[LLM]   æ‡‰çµæŸæœƒè©±: {session_control.get('should_end_session', False)}")
                if session_control.get('end_reason'):
                    debug_log(1, f"[LLM]   çµæŸåŸå› : {session_control['end_reason']}")
                if session_control.get('confidence'):
                    debug_log(1, f"[LLM]   ä¿¡å¿ƒåº¦: {session_control['confidence']:.2f}")
            
            # å¿«å–è³‡è¨Šæ—¥èªŒ
            meta = response_data.get("_meta", {})
            if meta.get("cached_input_tokens", 0) > 0:
                debug_log(2, f"[LLM] ğŸ“š å¿«å–å‘½ä¸­: {meta['cached_input_tokens']} tokens")
            debug_log(2, f"[LLM] ğŸ“ ç¸½è¼¸å…¥tokens: {meta.get('total_input_tokens', 0)}")
            
            # è™•ç† StatusManager æ›´æ–°
            if "status_updates" in response_data and response_data["status_updates"]:
                self._process_status_updates(response_data["status_updates"])
            
            # 4. è™•ç†MEMæ¨¡çµ„æ•´åˆ (CHATæ¨¡å¼)
            memory_operations = self._process_chat_memory_operations(
                llm_input, response_data, response_text
            )
            
            # === è¨˜æ†¶æ“ä½œæ—¥èªŒ ===
            if memory_operations:
                info_log(f"[LLM] ğŸ§  è¨˜æ†¶æ“ä½œè™•ç†:")
                for i, op in enumerate(memory_operations):
                    op_type = op.get('operation', 'unknown')
                    content = op.get('content', {})
                    if op_type == 'store':
                        user_text = content.get('user_input', '')[:50] + "..." if len(content.get('user_input', '')) > 50 else content.get('user_input', '')
                        assistant_text = content.get('assistant_response', '')[:50] + "..." if len(content.get('assistant_response', '')) > 50 else content.get('assistant_response', '')
                        info_log(f"[LLM]   #{i+1} å„²å­˜å°è©±: ç”¨æˆ¶='{user_text}', åŠ©æ‰‹='{assistant_text}'")
                    else:
                        info_log(f"[LLM]   #{i+1} {op_type}: {str(content)[:100]}")
            else:
                debug_log(2, f"[LLM] ğŸ“ ç„¡è¨˜æ†¶æ“ä½œéœ€è¦è™•ç†")
            
            # 5. è™•ç†å­¸ç¿’ä¿¡è™Ÿ
            if self.learning_engine.learning_enabled:
                # è™•ç†æ–°çš„ç´¯ç©è©•åˆ†å­¸ç¿’ä¿¡è™Ÿ
                ctx = llm_input.identity_context or {}
                if "learning_signals" in response_data and response_data["learning_signals"]:
                    
                    identity_id = (ctx.get("identity") or {}).get("id") or ctx.get("identity_id") or "default"
                    self.learning_engine.process_learning_signals(identity_id, response_data["learning_signals"])
                    
                # ä¿ç•™èˆŠçš„äº’å‹•è¨˜éŒ„ï¼ˆç”¨æ–¼çµ±è¨ˆå’Œåˆ†æï¼‰
                identity_id = (ctx.get("identity") or {}).get("id") or ctx.get("identity_id") or "default"
                self.learning_engine.record_interaction(
                    identity_id=identity_id,
                    interaction_type="CHAT",
                    user_input=llm_input.text,
                    system_response=response_text,
                    metadata={
                        "memory_used": bool(llm_input.memory_context),
                        "identity_used": bool(llm_input.identity_context)
                    }
                )
            
            # 5. è™•ç†æœƒè©±æ§åˆ¶å»ºè­°
            session_control_result = self._process_session_control(
                response_data, "CHAT", llm_input
            )
            
            # 6. å¿«å–å›æ‡‰
            output = LLMOutput(
                text=response_text,
                processing_time=time.time() - start_time,
                tokens_used=len(response_text.split()),
                success=True,
                error=None,
                confidence=response_data.get("confidence", 0.85),
                sys_action=None,
                status_updates=StatusUpdate(**response_data["status_updates"]) if response_data.get("status_updates") else None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=response_data.get("memory_observation"),
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={
                    "mode": "CHAT",
                    "cached": False,
                    "memory_context_size": len(llm_input.memory_context) if llm_input.memory_context else 0,
                    "identity_context_size": len(llm_input.identity_context) if llm_input.identity_context else 0,
                    "memory_operations_count": len(memory_operations),
                    "memory_operations": memory_operations,
                    "session_control": session_control_result
                }
            )
            
            self.cache_manager.cache_response(cache_key, output)
            return output
            
        except Exception as e:
            error_log(f"[LLM] CHAT æ¨¡å¼è™•ç†éŒ¯èª¤: {e}")
            return LLMOutput(
                text="èŠå¤©è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                processing_time=time.time() - start_time,
                tokens_used=0,
                success=False,
                error=str(e),
                confidence=0.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={"mode": "CHAT", "error_type": "processing_error"}
            )
    
    def _handle_work_mode(self, llm_input: "LLMInput", status: Dict[str, Any]) -> "LLMOutput":
        """è™•ç† WORK æ¨¡å¼ - èˆ‡ SYS å”ä½œçš„å·¥ä½œä»»å‹™"""
        start_time = time.time()
        debug_log(2, "[LLM] è™•ç† WORK æ¨¡å¼")
        
        try:
            # 1. WORK æ¨¡å¼é€šå¸¸ä¸ä½¿ç”¨å¿«å–ï¼ˆå› ç‚ºä»»å‹™å°å‘ï¼‰
            debug_log(3, "[LLM] WORK æ¨¡å¼ - è·³éå¿«å–æª¢æŸ¥")
            
            # 2. å¾ SYS æ¨¡çµ„ç²å–å¯ç”¨åŠŸèƒ½æ¸…å–®  
            available_functions_list = self._get_available_sys_functions()
            available_functions_str = self._format_functions_for_prompt(available_functions_list)
            
            # 3. æ§‹å»º WORK æç¤º  
            prompt = self.prompt_manager.build_work_prompt(
                user_input=llm_input.text,
                available_functions=available_functions_str,
                workflow_context=getattr(llm_input, 'workflow_context', None),
                identity_context=llm_input.identity_context
            )
            
            # 3. ç²å–æˆ–å‰µå»ºä»»å‹™å¿«å–
            cached_content_ids = self._get_system_caches("work")
            
            # 4. å‘¼å« Gemini API (ä½¿ç”¨å¿«å–)
            response_data = self.model.query(
                prompt, 
                mode="work",
                cached_content=cached_content_ids.get("functions")
            )
            response_text = response_data.get("text", "")
            
            # è™•ç† StatusManager æ›´æ–°
            if "status_updates" in response_data and response_data["status_updates"]:
                self._process_status_updates(response_data["status_updates"])
            
            # 4. è™•ç†SYSæ¨¡çµ„æ•´åˆ (WORKæ¨¡å¼)
            sys_actions = self._process_work_system_actions(
                llm_input, response_data, response_text
            )
            
            # 5. è™•ç†å­¸ç¿’ä¿¡è™Ÿ
            if self.learning_engine.learning_enabled:
                # è™•ç†æ–°çš„ç´¯ç©è©•åˆ†å­¸ç¿’ä¿¡è™Ÿ
                ctx = llm_input.identity_context or {}
                if "learning_signals" in response_data and response_data["learning_signals"]:
                    identity_id = (ctx.get("identity") or {}).get("id") or ctx.get("identity_id") or "default"
                    self.learning_engine.process_learning_signals(identity_id, response_data["learning_signals"])
                
                # ä¿ç•™èˆŠçš„äº’å‹•è¨˜éŒ„ï¼ˆç”¨æ–¼çµ±è¨ˆå’Œåˆ†æï¼‰
                identity_id = (ctx.get("identity") or {}).get("id") or ctx.get("identity_id") or "default"
                self.learning_engine.record_interaction(
                    identity_id=identity_id,
                    interaction_type="WORK",
                    user_input=llm_input.text,
                    system_response=response_text,
                    metadata={
                        "workflow_context": llm_input.workflow_context,
                        "system_context_used": bool(llm_input.system_context)
                    }
                )
            
            # 6. è™•ç†æœƒè©±æ§åˆ¶å»ºè­°
            session_control_result = self._process_session_control(
                response_data, "WORK", llm_input
            )
            
            # æå– sys_action
            sys_action_obj = None
            if sys_actions and len(sys_actions) > 0:
                sys_action_obj = SystemAction(**sys_actions[0])
            
            output = LLMOutput(
                text=response_text,
                processing_time=time.time() - start_time,
                tokens_used=len(response_text.split()),
                success=True,
                error=None,
                confidence=response_data.get("confidence", 0.90),
                sys_action=sys_action_obj,
                status_updates=StatusUpdate(**response_data["status_updates"]) if response_data.get("status_updates") else None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={
                    "mode": "WORK",
                    "workflow_context_size": len(llm_input.workflow_context) if llm_input.workflow_context else 0,
                    "sys_actions_count": len(sys_actions),
                    "sys_actions": sys_actions,
                    "system_context_size": len(llm_input.system_context) if llm_input.system_context else 0,
                    "session_control": session_control_result
                }
            )
            
            return output
            
        except Exception as e:
            error_log(f"[LLM] WORK æ¨¡å¼è™•ç†éŒ¯èª¤: {e}")
            return LLMOutput(
                text="å·¥ä½œä»»å‹™è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                processing_time=time.time() - start_time,
                tokens_used=0,
                success=False,
                error=str(e),
                confidence=0.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={"mode": "WORK", "error_type": "processing_error"}
            )
    
    def _handle_legacy_mode(self, llm_input: "LLMInput", status: Dict[str, Any]) -> "LLMOutput":
        """è™•ç†èˆŠçš„ intent ç³»çµ±ä»¥ä¿æŒå‘å¾Œå…¼å®¹"""
        start_time = time.time()
        debug_log(2, f"[LLM] è™•ç†èˆŠç‰ˆ intent: {getattr(llm_input, 'intent', 'unknown')}")
        
        # å°‡èˆŠçš„ intent è½‰æ›ç‚ºæ–°çš„æ¨¡å¼
        legacy_intent = getattr(llm_input, 'intent', 'chat')
        
        if legacy_intent == "chat":
            # è½‰ç‚º CHAT æ¨¡å¼
            llm_input.mode = LLMMode.CHAT
            return self._handle_chat_mode(llm_input, status)
        elif legacy_intent == "command":
            # è½‰ç‚º WORK æ¨¡å¼
            llm_input.mode = LLMMode.WORK
            return self._handle_work_mode(llm_input, status)
        else:
            return LLMOutput(
                text=f"æŠ±æ­‰ï¼Œç›®å‰æš«ä¸æ”¯æ´ '{legacy_intent}' é¡å‹çš„è™•ç†ã€‚",
                processing_time=time.time() - start_time,
                tokens_used=0,
                success=False,
                error=f"ä¸æ”¯æ´çš„ intent: {legacy_intent}",
                confidence=0.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={"legacy_intent": legacy_intent}
            )
    
    def _analyze_system_action(self, response_text: str, workflow_context: Optional[Dict[str, Any]]) -> Optional["SystemAction"]:
        """
        [DEPRECATED] åˆ†æå›æ‡‰æ–‡æœ¬æ˜¯å¦éœ€è¦ç³»çµ±å‹•ä½œ
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•å·²å»¢æ£„ã€‚æ ¹æ“š U.E.P æ¶æ§‹è¨­è¨ˆï¼š
        1. æ„åœ–åˆ†ææ‡‰è©²åœ¨ NLP æ¨¡çµ„éšæ®µå®Œæˆ
        2. LLM åœ¨ WORK æ¨¡å¼ä¸‹æ‡‰è©²å¾ Gemini çµæ§‹åŒ–å›æ‡‰ä¸­ç²å–ç³»çµ±å‹•ä½œ
        3. ä¸æ‡‰è©²é‡è¤‡åˆ†ææ–‡æœ¬ä¾†åˆ¤æ–·ç³»çµ±åŠŸèƒ½éœ€æ±‚
        
        æ­¤æ–¹æ³•ä¿ç•™åƒ…ç”¨æ–¼å‘å¾Œå…¼å®¹ï¼Œå»ºè­°ç§»é™¤å°æ­¤æ–¹æ³•çš„èª¿ç”¨ã€‚
        """
        debug_log(3, "[LLM] è­¦å‘Šï¼šä½¿ç”¨äº†å·²å»¢æ£„çš„ _analyze_system_action æ–¹æ³•")
        return None
    
    def _on_status_update(self, status_type: str, old_value: float, new_value: float, reason: str = ""):
        """StatusManager ç‹€æ…‹æ›´æ–°å›èª¿"""
        debug_log(2, f"[LLM] ç³»çµ±ç‹€æ…‹æ›´æ–° - {status_type}: {old_value} -> {new_value} ({reason})")
        
        # æ ¹æ“šç‹€æ…‹è®ŠåŒ–èª¿æ•´ LLM è¡Œç‚º
        if status_type == "mood" and new_value < 0.3:
            debug_log(1, "[LLM] åµæ¸¬åˆ°ç³»çµ±å¿ƒæƒ…ä½è½ï¼Œèª¿æ•´å›æ‡‰é¢¨æ ¼")
        elif status_type == "boredom" and new_value > 0.8:
            debug_log(1, "[LLM] åµæ¸¬åˆ°ç³»çµ±ç„¡èŠï¼Œå»ºè­°ä¸»å‹•äº’å‹•")
    
    def shutdown(self):
        """é—œé–‰ LLM æ¨¡çµ„ä¸¦ä¿å­˜ç‹€æ…‹"""
        try:
            info_log("[LLM] LLM æ¨¡çµ„é—œé–‰ä¸­...")
            
            # ä¿å­˜å­¸ç¿’è³‡æ–™
            if self.learning_engine:
                self.learning_engine.save_learning_data()
                debug_log(2, "[LLM] å­¸ç¿’è³‡æ–™å·²ä¿å­˜")
            
            # æ¸…ç† Context Cache
            if self.cache_manager:
                cache_stats = self.cache_manager.get_cache_statistics()
                debug_log(2, f"[LLM] Cache çµ±è¨ˆ: {cache_stats}")
                
            # å–æ¶ˆ StatusManager å›èª¿
            self.status_manager.unregister_update_callback("llm_module")
            
            info_log("[LLM] LLM æ¨¡çµ„é‡æ§‹ç‰ˆé—œé–‰å®Œæˆ")
            
        except Exception as e:
            error_log(f"[LLM] é—œé–‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def get_module_status(self) -> Dict[str, Any]:
        """ç²å–æ¨¡çµ„ç‹€æ…‹è³‡è¨Š"""
        try:
            status = {
                "initialized": self.is_initialized,
                "model_status": "active" if self.model else "inactive",
                "learning_enabled": self.learning_engine.learning_enabled if self.learning_engine else False,
                "cache_enabled": self.cache_manager is not None,
            }
            
            if self.cache_manager:
                status["cache_stats"] = self.cache_manager.get_cache_statistics()
                
            if self.learning_engine and self.learning_engine.learning_enabled:
                status["learning_stats"] = {
                    "total_interactions": len(self.learning_engine.interaction_history),
                    "conversation_styles": len(self.learning_engine.conversation_styles),
                    "usage_patterns": len(self.learning_engine.usage_patterns)
                }
                
            return status
            
        except Exception as e:
            error_log(f"[LLM] ç²å–æ¨¡çµ„ç‹€æ…‹å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _process_status_updates(self, status_updates) -> None:
        """
        è™•ç†ä¾†è‡ªLLMå›æ‡‰çš„StatusManageræ›´æ–°
        æ”¯æ´ç‰©ä»¶æ ¼å¼ï¼ˆä¾†è‡ª schemaï¼‰å’Œé™£åˆ—æ ¼å¼ï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰
        """
        try:
            if not status_updates:
                return
            
            # è™•ç†ç‰©ä»¶æ ¼å¼ï¼ˆä¾†è‡ª Gemini schemaï¼‰
            if isinstance(status_updates, dict):
                # ä½¿ç”¨ StatusManager çš„å°ˆç”¨ delta æ›´æ–°æ–¹æ³•
                if "mood_delta" in status_updates and status_updates["mood_delta"] is not None:
                    self.status_manager.update_mood(status_updates["mood_delta"], "LLMæƒ…ç·’åˆ†æ")
                    debug_log(2, f"[LLM] Mood æ›´æ–°: += {status_updates['mood_delta']}")
                
                if "pride_delta" in status_updates and status_updates["pride_delta"] is not None:
                    self.status_manager.update_pride(status_updates["pride_delta"], "LLMæƒ…ç·’åˆ†æ")  
                    debug_log(2, f"[LLM] Pride æ›´æ–°: += {status_updates['pride_delta']}")
                
                if "helpfulness_delta" in status_updates and status_updates["helpfulness_delta"] is not None:
                    self.status_manager.update_helpfulness(status_updates["helpfulness_delta"], "LLMæƒ…ç·’åˆ†æ")
                    debug_log(2, f"[LLM] Helpfulness æ›´æ–°: += {status_updates['helpfulness_delta']}")
                
                if "boredom_delta" in status_updates and status_updates["boredom_delta"] is not None:
                    self.status_manager.update_boredom(status_updates["boredom_delta"], "LLMæƒ…ç·’åˆ†æ")
                    debug_log(2, f"[LLM] Boredom æ›´æ–°: += {status_updates['boredom_delta']}")
                
                # çµ±è¨ˆæ›´æ–°æ¬¡æ•¸
                updates_count = sum(1 for key in ["mood_delta", "pride_delta", "helpfulness_delta", "boredom_delta"] 
                                  if key in status_updates and status_updates[key] is not None)
                if updates_count > 0:
                    debug_log(1, f"[LLM] StatusManager å·²æ‡‰ç”¨ {updates_count} å€‹ç‹€æ…‹æ›´æ–°")
            
            # è™•ç†é™£åˆ—æ ¼å¼ï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰
            elif isinstance(status_updates, list):
                for update in status_updates:
                    status_type = update.get("status_type")
                    value = update.get("value") 
                    reason = update.get("reason", "LLMå›æ‡‰è§¸ç™¼")
                    
                    if status_type and value is not None:
                        # ä½¿ç”¨å°æ‡‰çš„å°ˆç”¨æ›´æ–°æ–¹æ³•
                        try:
                            if status_type == "mood":
                                self.status_manager.update_mood(value, reason)
                            elif status_type == "pride":
                                self.status_manager.update_pride(value, reason)
                            elif status_type == "helpfulness":
                                self.status_manager.update_helpfulness(value, reason)
                            elif status_type == "boredom":
                                self.status_manager.update_boredom(value, reason)
                            else:
                                debug_log(1, f"[LLM] æœªçŸ¥çš„ç‹€æ…‹é¡å‹: {status_type}")
                                continue
                            
                            debug_log(2, f"[LLM] StatusManageræ›´æ–°æˆåŠŸ: {status_type}+={value}, åŸå› : {reason}")
                        except Exception as e:
                            debug_log(1, f"[LLM] StatusManageræ›´æ–°å¤±æ•—: {status_type}={value}, éŒ¯èª¤: {e}")
                        
        except Exception as e:
            error_log(f"[LLM] è™•ç†StatusManageræ›´æ–°æ™‚å‡ºéŒ¯: {e}")
    
    def _get_current_system_status(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹"""
        try:
            return {
                "status_values": self.status_manager.get_status_dict(),
                "personality_modifiers": self.status_manager.get_personality_modifiers(),
                "system_mode": self.state_manager.get_current_state().value
            }
        except Exception as e:
            error_log(f"[LLM] ç²å–ç³»çµ±ç‹€æ…‹å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _get_current_session_info(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰æœƒè©±ä¿¡æ¯ - å„ªå…ˆç²å– CS æˆ– WSï¼ˆLLM ä½œç‚ºé‚è¼¯ä¸­æ¨çš„åŸ·è¡Œæœƒè©±ï¼‰"""
        try:
            # å¾çµ±ä¸€æœƒè©±ç®¡ç†å™¨ç²å–æœƒè©±ä¿¡æ¯
            from core.sessions.session_manager import session_manager
            
            # LLM åœ¨ CHAT ç‹€æ…‹æ™‚æ‡‰è©²ç²å–ç•¶å‰ CS
            active_cs_ids = session_manager.get_active_chatting_session_ids()
            if active_cs_ids:
                # åœ¨æ¶æ§‹ä¸‹ï¼ŒåŒä¸€æ™‚é–“åªæœƒæœ‰ä¸€å€‹ CS åŸ·è¡Œä¸­
                current_cs_id = active_cs_ids[0]
                current_cs = session_manager.get_chatting_session(current_cs_id)
                
                if current_cs:
                    return {
                        "session_id": current_cs_id,
                        "session_type": "chatting",
                        "start_time": getattr(current_cs, 'start_time', None),
                        "interaction_count": getattr(current_cs, 'turn_count', 0),
                        "last_activity": getattr(current_cs, 'last_activity', None),
                        "active_session_type": "CS"
                    }
            
            # LLM åœ¨ WORK ç‹€æ…‹æ™‚æ‡‰è©²ç²å–ç•¶å‰ WS
            active_ws_ids = session_manager.get_active_workflow_session_ids()
            if active_ws_ids:
                # åœ¨æ¶æ§‹ä¸‹ï¼ŒåŒä¸€æ™‚é–“åªæœƒæœ‰ä¸€å€‹ WS åŸ·è¡Œä¸­
                current_ws_id = active_ws_ids[0]
                current_ws = session_manager.get_workflow_session(current_ws_id)
                
                if current_ws:
                    return {
                        "session_id": current_ws_id,
                        "session_type": "workflow",
                        "start_time": getattr(current_ws, 'start_time', None),
                        "interaction_count": getattr(current_ws, 'step_count', 0),
                        "last_activity": getattr(current_ws, 'last_activity', None),
                        "active_session_type": "WS"
                    }
            
            # å¦‚æœæ²’æœ‰ CS æˆ– WSï¼Œå¯èƒ½ç³»çµ±è™•æ–¼ IDLE ç‹€æ…‹æˆ–å…¶ä»–ç‹€æ…‹
            return {
                "session_id": "no_active_session", 
                "session_type": "idle",
                "start_time": None,
                "interaction_count": 0,
                "last_activity": None,
                "active_session_type": "NONE"
            }
            
        except Exception as e:
            error_log(f"[LLM] ç²å–æœƒè©±ä¿¡æ¯å¤±æ•—: {e}")
            return {
                "session_id": "error", 
                "session_type": "error",
                "active_session_type": "ERROR"
            }
    
    def _get_identity_context(self) -> Dict[str, Any]:
        """å¾Working Contextç²å–Identityä¿¡æ¯ï¼Œå°é€šç”¨èº«ä»½æ¡ç”¨é è¨­è™•ç†"""
        try:
            # ä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•ç²å–ç•¶å‰èº«ä»½
            identity_data = working_context_manager.get_current_identity()
            
            if not identity_data:
                debug_log(2, "[LLM] æ²’æœ‰è¨­ç½®èº«ä»½ä¿¡æ¯ï¼Œä½¿ç”¨é è¨­å€¼")
                return {
                    "identity": {
                        "name": "default_user",
                        "traits": {}
                    },
                    "preferences": {}
                }
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé€šç”¨èº«ä»½
            identity_status = identity_data.get("status", "unknown")
            if identity_status == "temporary":
                debug_log(2, "[LLM] æª¢æ¸¬åˆ°é€šç”¨èº«ä»½ï¼Œä½¿ç”¨åŸºæœ¬è¨­ç½®")
                return {
                    "identity": {
                        "name": "ç”¨æˆ¶",
                        "traits": {},
                        "status": "temporary"
                    },
                    "preferences": {}  # é€šç”¨èº«ä»½ä¸ä½¿ç”¨ç‰¹æ®Šåå¥½
                }
            
            # æ­£å¼èº«ä»½ä½¿ç”¨å®Œæ•´è³‡æ–™
            return {
                "identity": {
                    "name": identity_data.get("user_identity", identity_data.get("identity_id", "default_user")),
                    "traits": identity_data.get("traits", {}),
                    "status": identity_status
                },
                "preferences": identity_data.get("conversation_preferences", {})
            }
        except Exception as e:
            error_log(f"[LLM] ç²å–Identityä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return {}
    
    def _enrich_with_system_context(self, 
                                  llm_input: LLMInput,
                                  current_state: Any,
                                  status: Dict[str, Any],
                                  session_info: Dict[str, Any],
                                  identity_context: Dict[str, Any]) -> LLMInput:
        """è£œå……ç³»çµ±ä¸Šä¸‹æ–‡åˆ°LLMè¼¸å…¥ - æ”¯æ´æ–° Router æ•´åˆ"""
        try:
            # å‰µå»ºæ–°çš„enriched input
            enriched_data = llm_input.dict()
            
            # è£œå……ç³»çµ±ä¸Šä¸‹æ–‡
            if not enriched_data.get("system_context"):
                enriched_data["system_context"] = {}
            
            enriched_data["system_context"].update({
                "current_state": current_state.value if hasattr(current_state, 'value') else str(current_state),
                "status_manager": status,
                "session_info": session_info
            })
            
            # è£œå……èº«ä»½ä¸Šä¸‹æ–‡ (ä¸è¦†è“‹Routeræä¾›çš„)
            if not enriched_data.get("identity_context"):
                enriched_data["identity_context"] = {}
            # åªåœ¨æ²’æœ‰Routeræ•¸æ“šæ™‚è£œå……æœ¬åœ°èº«ä»½ä¸Šä¸‹æ–‡
            if not llm_input.source_layer:
                enriched_data["identity_context"].update(identity_context)
            
            # è™•ç†æ–°Routeræä¾›çš„å”ä½œä¸Šä¸‹æ–‡
            if llm_input.collaboration_context:
                debug_log(2, f"[LLM] è™•ç†å”ä½œä¸Šä¸‹æ–‡: {list(llm_input.collaboration_context.keys())}")
                
                # è¨­ç½®è¨˜æ†¶æª¢ç´¢æ¨™èªŒ
                if "mem" in llm_input.collaboration_context:
                    enriched_data["enable_memory_retrieval"] = True
                    mem_config = llm_input.collaboration_context["mem"]
                    if mem_config.get("retrieve_relevant"):
                        enriched_data["memory_context"] = "å”ä½œæ¨¡å¼ï¼šéœ€è¦æª¢ç´¢ç›¸é—œè¨˜æ†¶"
                
                # è¨­ç½®ç³»çµ±å‹•ä½œæ¨™èªŒ
                if "sys" in llm_input.collaboration_context:
                    enriched_data["enable_system_actions"] = True
                    sys_config = llm_input.collaboration_context["sys"]
                    if sys_config.get("allow_execution"):
                        enriched_data["workflow_context"] = {"execution_allowed": True}
            
            # è™•ç†Routerçš„æœƒè©±ä¸Šä¸‹æ–‡
            if llm_input.session_context:
                enriched_data["session_id"] = llm_input.session_context.get("session_id")
                enriched_data["system_context"]["router_session"] = llm_input.session_context
            
            # è™•ç†NLPå¯¦é«”ä¿¡æ¯
            if llm_input.entities:
                enriched_data["system_context"]["nlp_entities"] = llm_input.entities
            
            return LLMInput(**enriched_data)
            
        except Exception as e:
            error_log(f"[LLM] è£œå……ç³»çµ±ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return llm_input
    
    def _process_chat_memory_operations(self, 
                                      llm_input: LLMInput,
                                      response_data: Dict[str, Any], 
                                      response_text: str) -> List[Dict[str, Any]]:
        """è™•ç†CHATæ¨¡å¼çš„MEMæ¨¡çµ„æ“ä½œ"""
        memory_operations = []
        
        try:
            # 1. å¾Geminiå›æ‡‰ä¸­æå–è¨˜æ†¶æ“ä½œ
            if "memory_operations" in response_data:
                memory_operations.extend(response_data["memory_operations"])
                debug_log(2, f"[LLM] å¾å›æ‡‰æå–è¨˜æ†¶æ“ä½œ: {len(memory_operations)}å€‹")
            
            # 2. è‡ªå‹•è¨˜æ†¶å„²å­˜é‚è¼¯
            if self._should_store_conversation(llm_input, response_text):
                store_operation = {
                    "operation": "store",
                    "content": {
                        "user_input": llm_input.text,
                        "assistant_response": response_text,
                        "timestamp": time.time(),
                        "conversation_context": llm_input.memory_context,
                        "identity_context": llm_input.identity_context
                    },
                    "metadata": {
                        "interaction_type": "chat",
                        "memory_type": "conversation",
                        "auto_generated": True,
                        "ttl_seconds": 60 * 60 * 24 * 7,     # ä¸€é€±
                        "erasable": True
                    }
                }
                memory_operations.append(store_operation)
                debug_log(2, "[LLM] è‡ªå‹•æ·»åŠ å°è©±è¨˜æ†¶å„²å­˜")
            
            # 3. ç™¼é€è¨˜æ†¶æ“ä½œåˆ°MEMæ¨¡çµ„ (é€šéRouter)
            if memory_operations:
                self._send_to_mem_module(memory_operations)
            
            return memory_operations
            
        except Exception as e:
            error_log(f"[LLM] è™•ç†CHATè¨˜æ†¶æ“ä½œå¤±æ•—: {e}")
            return []
    
    def _should_store_conversation(self, llm_input: LLMInput, response_text: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²å„²å­˜å°è©±"""
        try:
            # æª¢æŸ¥å°è©±é•·åº¦
            if len(llm_input.text) < 10 or len(response_text) < 10:
                return False
            
            sensitive_patterns = [r"\b\d{10}\b", r"@.+\.", r"\b[A-Z]\d{9}\b"]  # å¯æ“´å……
            if any(re.search(p, llm_input.text) for p in sensitive_patterns):
                return False  # å«æ•æ„Ÿè³‡è¨Šä¸è‡ªå‹•å­˜
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¦å°è©±
            important_keywords = ["remember", "record", "important", "remind", "save"]
            if any(keyword in llm_input.text for keyword in important_keywords):
                return True
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å€‹äººä¿¡æ¯æˆ–åå¥½
            personal_keywords = ["like", "hate", "prefer", "would have", "name", "birthday"]
            if any(keyword in llm_input.text for keyword in personal_keywords):
                return True
            
            # é è¨­å„²å­˜è¼ƒé•·çš„æœ‰æ„ç¾©å°è©±
            return len(llm_input.text) > 50
            
        except Exception as e:
            error_log(f"[LLM] åˆ¤æ–·å°è©±å„²å­˜å¤±æ•—: {e}")
            return False
    
    def _send_to_mem_module(self, memory_operations: List[Dict[str, Any]]) -> None:
        """å‘MEMæ¨¡çµ„ç™¼é€è¨˜æ†¶æ“ä½œ - é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£"""
        try:
            debug_log(1, f"[LLM] æº–å‚™ç™¼é€ {len(memory_operations)} å€‹è¨˜æ†¶æ“ä½œåˆ°MEMæ¨¡çµ„")
            
            # æª¢æŸ¥ CHAT-MEM å”ä½œç®¡é“æ˜¯å¦å•Ÿç”¨
            if not self.module_interface.is_channel_active(CollaborationChannel.CHAT_MEM):
                debug_log(2, "[LLM] è¨˜æ†¶æ“ä½œè·³é: MEMæ¨¡çµ„åªåœ¨CHATç‹€æ…‹ä¸‹é‹è¡Œ")
                return
            
            # é€å€‹è™•ç†è¨˜æ†¶æ“ä½œ
            for i, operation in enumerate(memory_operations):
                operation_type = operation.get('operation', 'unknown')
                debug_log(3, f"[LLM] è¨˜æ†¶æ“ä½œ #{i+1}: {operation_type}")
                
                try:
                    # é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£ç™¼é€å°è©±å„²å­˜è«‹æ±‚
                    conversation_data = {
                        "operation_type": operation_type,
                        "content": operation.get('content', {}),
                        "metadata": operation.get('metadata', {}),
                        "source_module": "llm_module"
                    }
                    
                    result = self.module_interface.get_chat_mem_data(
                        "conversation_storage",
                        conversation_data=conversation_data
                    )
                    
                    if result:
                        debug_log(2, f"[LLM] è¨˜æ†¶æ“ä½œ #{i+1} æˆåŠŸ: {operation_type}")
                    else:
                        debug_log(2, f"[LLM] è¨˜æ†¶æ“ä½œ #{i+1} æœªåŸ·è¡Œ: {operation_type}")
                        
                except Exception as op_error:
                    error_log(f"[LLM] è™•ç†è¨˜æ†¶æ“ä½œ #{i+1} æ™‚å‡ºéŒ¯: {op_error}")
            
        except Exception as e:
            error_log(f"[LLM] ç™¼é€è¨˜æ†¶æ“ä½œå¤±æ•—: {e}")
    
    def _retrieve_relevant_memory(self, user_input: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """å¾MEMæ¨¡çµ„æª¢ç´¢ç›¸é—œè¨˜æ†¶ - é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£"""
        try:
            debug_log(2, f"[LLM] æª¢ç´¢ç›¸é—œè¨˜æ†¶: {user_input[:50]}...")
            
            # æª¢æŸ¥ CHAT-MEM å”ä½œç®¡é“æ˜¯å¦å•Ÿç”¨
            if not self.module_interface.is_channel_active(CollaborationChannel.CHAT_MEM):
                debug_log(2, "[LLM] è¨˜æ†¶æª¢ç´¢å¤±æ•—: MEMæ¨¡çµ„åªåœ¨CHATç‹€æ…‹ä¸‹é‹è¡Œ")
                return []
            
            # é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£æª¢ç´¢è¨˜æ†¶
            memories = self.module_interface.get_chat_mem_data(
                "memory_retrieval",
                query=user_input,
                max_results=max_results,
                memory_types=["conversation", "user_info", "context"]
            )
            
            if memories:
                debug_log(1, f"[LLM] æª¢ç´¢åˆ° {len(memories)} æ¢ç›¸é—œè¨˜æ†¶")
                return memories
            else:
                debug_log(2, "[LLM] æœªæª¢ç´¢åˆ°ç›¸é—œè¨˜æ†¶")
                return []
            
        except Exception as e:
            error_log(f"[LLM] è¨˜æ†¶æª¢ç´¢å¤±æ•—: {e}")
            return []
    
    def _format_memories_for_context(self, memories: List[Dict[str, Any]]) -> str:
        """å°‡æª¢ç´¢åˆ°çš„è¨˜æ†¶æ ¼å¼åŒ–ç‚ºå°è©±ä¸Šä¸‹æ–‡"""
        try:
            if not memories:
                return ""
            
            context_parts = ["Relevant Memory Context:"]
            
            for i, memory in enumerate(memories[:5], 1):  # é™åˆ¶æœ€å¤š5æ¢è¨˜æ†¶
                memory_type = memory.get("type", "unknown")
                content = memory.get("content", "")
                timestamp = memory.get("timestamp", "")
                
                if memory_type == "conversation":
                    # å°è©±è¨˜æ†¶æ ¼å¼
                    user_input = memory.get("user_input", "")
                    assistant_response = memory.get("assistant_response", "")
                    context_parts.append(f"{i}. [Conversation] User: {user_input} Assistant: {assistant_response}")
                elif memory_type == "user_info":
                    # ç”¨æˆ¶ä¿¡æ¯è¨˜æ†¶æ ¼å¼
                    context_parts.append(f"{i}. [User Info] {content}")
                else:
                    # ä¸€èˆ¬è¨˜æ†¶æ ¼å¼
                    context_parts.append(f"{i}. [{memory_type.title()}] {content}")
            
            formatted_context = "\n".join(context_parts)
            debug_log(3, f"[LLM] æ ¼å¼åŒ–è¨˜æ†¶ä¸Šä¸‹æ–‡: {len(formatted_context)} å­—ç¬¦")
            
            return formatted_context
            
        except Exception as e:
            error_log(f"[LLM] æ ¼å¼åŒ–è¨˜æ†¶ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return ""
    
    def _process_work_system_actions(self, 
                                   llm_input: LLMInput,
                                   response_data: Dict[str, Any], 
                                   response_text: str) -> List[Dict[str, Any]]:
        """è™•ç†WORKæ¨¡å¼çš„SYSæ¨¡çµ„æ“ä½œ - LLMä½œç‚ºæ±ºç­–æ©Ÿ"""
        sys_actions = []
        
        try:
            # å¾Geminiå›æ‡‰ä¸­æå–ç³»çµ±å‹•ä½œæ±ºç­–
            if "sys_action" in response_data:
                sys_action = response_data["sys_action"]
                if isinstance(sys_action, dict):
                    sys_actions.append(sys_action)
                    action_type = sys_action.get('action_type', 'unknown')
                    target = sys_action.get('target', 'unknown')
                    debug_log(1, f"[LLM] æ±ºç­–: {action_type} -> {target}")
            
            # ç™¼é€æ±ºç­–çµæœåˆ°SYSæ¨¡çµ„é€²è¡ŒåŸ·è¡Œ
            if sys_actions:
                self._send_to_sys_module(sys_actions, llm_input.workflow_context)
            
            return sys_actions
            
        except Exception as e:
            error_log(f"[LLM] è™•ç†WORKç³»çµ±å‹•ä½œå¤±æ•—: {e}")
            return []
    
    def _get_available_sys_functions(self) -> Optional[List[Dict[str, Any]]]:
        """å¾ SYS æ¨¡çµ„ç²å–å¯ç”¨åŠŸèƒ½æ¸…å–®"""
        try:
            debug_log(2, "[LLM] å˜—è©¦å¾ SYS æ¨¡çµ„ç²å–åŠŸèƒ½æ¸…å–®")
            
            # æª¢æŸ¥ WORK-SYS å”ä½œç®¡é“æ˜¯å¦å•Ÿç”¨
            if not self.module_interface.is_channel_active(CollaborationChannel.WORK_SYS):
                debug_log(2, "[LLM] SYS å”ä½œç®¡é“æœªå•Ÿç”¨ï¼Œè¿”å›ç©ºåŠŸèƒ½æ¸…å–®")
                return None
            
            # é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£ç²å–åŠŸèƒ½è¨»å†Šè¡¨
            function_registry = self.module_interface.get_work_sys_data(
                "function_registry",
                request_type="get_all_functions"
            )
            
            if function_registry and isinstance(function_registry, list):
                debug_log(1, f"[LLM] æˆåŠŸç²å– {len(function_registry)} å€‹å¯ç”¨åŠŸèƒ½")
                return function_registry
            elif function_registry and isinstance(function_registry, dict):
                # è™•ç†å­—å…¸æ ¼å¼çš„åŠŸèƒ½è¨»å†Šè¡¨
                functions = []
                for category, funcs in function_registry.items():
                    if isinstance(funcs, list):
                        functions.extend(funcs)
                debug_log(1, f"[LLM] æˆåŠŸç²å– {len(functions)} å€‹å¯ç”¨åŠŸèƒ½ï¼ˆä¾†è‡ªå­—å…¸æ ¼å¼ï¼‰")
                return functions
            else:
                debug_log(2, "[LLM] SYS æ¨¡çµ„åŠŸèƒ½è¨»å†Šè¡¨ç‚ºç©ºæˆ–æ ¼å¼éŒ¯èª¤")
                return None
                
        except Exception as e:
            error_log(f"[LLM] ç²å– SYS åŠŸèƒ½æ¸…å–®å¤±æ•—: {e}")
            return None
    
    def _format_functions_for_prompt(self, functions_list: Optional[List[Dict[str, Any]]]) -> Optional[str]:
        """å°‡åŠŸèƒ½åˆ—è¡¨æ ¼å¼åŒ–ç‚ºæç¤ºè©å­—ç¬¦ä¸²"""
        try:
            if not functions_list:
                debug_log(2, "[LLM] æ²’æœ‰å¯ç”¨çš„ç³»çµ±åŠŸèƒ½")
                return "ç›®å‰æ²’æœ‰å¯ç”¨çš„ç³»çµ±åŠŸèƒ½ã€‚æˆ‘åªèƒ½æä¾›ä¸€èˆ¬çš„å›æ‡‰å’Œå»ºè­°ï¼Œç„¡æ³•åŸ·è¡Œå…·é«”çš„ç³»çµ±æ“ä½œã€‚"
            
            formatted_functions = []
            for i, func in enumerate(functions_list, 1):
                func_name = func.get("name", "unknown")
                func_desc = func.get("description", "ç„¡æè¿°")
                func_category = func.get("category", "general")
                
                # æ ¼å¼åŒ–å–®å€‹åŠŸèƒ½
                func_str = f"{i}. {func_name} ({func_category}): {func_desc}"
                
                # æ·»åŠ åƒæ•¸ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                parameters = func.get("parameters", {})
                if parameters:
                    param_strs = []
                    for param_name, param_info in parameters.items():
                        param_type = param_info.get("type", "unknown")
                        param_desc = param_info.get("description", "")
                        param_strs.append(f"   - {param_name} ({param_type}): {param_desc}")
                    if param_strs:
                        func_str += "\n" + "\n".join(param_strs)
                
                formatted_functions.append(func_str)
            
            result = "\n".join(formatted_functions)
            debug_log(2, f"[LLM] æ ¼å¼åŒ–äº† {len(functions_list)} å€‹ç³»çµ±åŠŸèƒ½")
            return result
            
        except Exception as e:
            error_log(f"[LLM] æ ¼å¼åŒ–åŠŸèƒ½åˆ—è¡¨å¤±æ•—: {e}")
            return "åŠŸèƒ½åˆ—è¡¨æ ¼å¼åŒ–å¤±æ•—ï¼Œç„¡æ³•æä¾›ç³»çµ±åŠŸèƒ½ä¿¡æ¯ã€‚"
    
    def _process_session_control(self, response_data: Dict[str, Any], mode: str, llm_input: "LLMInput") -> Optional[Dict[str, Any]]:
        """è™•ç†æœƒè©±æ§åˆ¶å»ºè­° - LLM æ±ºå®šæœƒè©±æ˜¯å¦æ‡‰è©²çµæŸ"""
        try:
            session_control = response_data.get("session_control")
            if not session_control:
                return None
            
            should_end = session_control.get("should_end_session", False)
            end_reason = session_control.get("end_reason", "unknown")
            confidence = session_control.get("confidence", 0.5)
            
            if should_end and confidence >= 0.7:  # åªåœ¨é«˜ä¿¡å¿ƒåº¦æ™‚çµæŸæœƒè©±
                debug_log(1, f"[LLM] æœƒè©±çµæŸå»ºè­°: {mode} æ¨¡å¼ - åŸå› : {end_reason} (ä¿¡å¿ƒåº¦: {confidence:.2f})")
                
                # é€šçŸ¥ session manager çµæŸç•¶å‰æœƒè©±
                self._request_session_end(mode, end_reason, confidence, llm_input)
                
                return {
                    "session_ended": True,
                    "reason": end_reason,
                    "confidence": confidence
                }
            elif should_end:
                debug_log(2, f"[LLM] æœƒè©±çµæŸå»ºè­°ä¿¡å¿ƒåº¦ä¸è¶³: {confidence:.2f} < 0.7")
                
            return None
            
        except Exception as e:
            error_log(f"[LLM] è™•ç†æœƒè©±æ§åˆ¶å¤±æ•—: {e}")
            return None
    
    def _request_session_end(self, mode: str, reason: str, confidence: float, llm_input: "LLMInput") -> None:
        """è«‹æ±‚çµæŸç•¶å‰æœƒè©±"""
        try:
            from core.sessions.session_manager import session_manager
            
            if mode == "CHAT":
                # çµæŸç•¶å‰çš„ Chatting Session
                active_cs_ids = session_manager.get_active_chatting_session_ids()
                if active_cs_ids:
                    session_id = active_cs_ids[0]
                    success = session_manager.end_chatting_session(
                        session_id, 
                    )
                    if success:
                        debug_log(1, f"[LLM] æˆåŠŸçµæŸ CS {session_id}")
                    else:
                        debug_log(2, f"[LLM] çµæŸ CS {session_id} å¤±æ•—")
                        
            elif mode == "WORK":
                # çµæŸç•¶å‰çš„ Workflow Session
                active_ws_ids = session_manager.get_active_workflow_session_ids()
                if active_ws_ids:
                    session_id = active_ws_ids[0]
                    success = session_manager.end_workflow_session(
                        session_id,
                    )
                    if success:
                        debug_log(1, f"[LLM] æˆåŠŸçµæŸ WS {session_id}")
                    else:
                        debug_log(2, f"[LLM] çµæŸ WS {session_id} å¤±æ•—")
            
        except Exception as e:
            error_log(f"[LLM] è«‹æ±‚æœƒè©±çµæŸå¤±æ•—: {e}")
    
    def _send_to_sys_module(self, sys_actions: List[Dict[str, Any]], workflow_context: Optional[Dict[str, Any]]) -> None:
        """å‘SYSæ¨¡çµ„ç™¼é€ç³»çµ±å‹•ä½œ - é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£"""
        try:
            debug_log(1, f"[LLM] æº–å‚™ç™¼é€ {len(sys_actions)} å€‹ç³»çµ±å‹•ä½œåˆ°SYSæ¨¡çµ„")
            
            # æª¢æŸ¥ WORK-SYS å”ä½œç®¡é“æ˜¯å¦å•Ÿç”¨
            if not self.module_interface.is_channel_active(CollaborationChannel.WORK_SYS):
                debug_log(2, "[LLM] ç³»çµ±å‹•ä½œè·³é: SYSæ¨¡çµ„åªåœ¨WORKç‹€æ…‹ä¸‹é‹è¡Œ")
                return
            
            for i, action in enumerate(sys_actions):
                action_type = action.get('action_type', 'unknown')
                target = action.get('target', 'unknown')
                debug_log(3, f"[LLM] ç³»çµ±å‹•ä½œ #{i+1}: {action_type} -> {target}")
                
                try:
                    # é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£ç²å–å·¥ä½œæµç‹€æ…‹ä¸¦åŸ·è¡ŒåŠŸèƒ½
                    workflow_status = self.module_interface.get_work_sys_data(
                        "workflow_status",
                        workflow_id=workflow_context.get('workflow_id') if workflow_context else 'default'
                    )
                    
                    if workflow_status:
                        debug_log(3, f"[LLM] å·¥ä½œæµç‹€æ…‹: {workflow_status.get('current_step', 'unknown')}")
                    
                    # ç²å–å¯ç”¨åŠŸèƒ½ä¸¦å˜—è©¦åŸ·è¡Œ
                    available_functions = self.module_interface.get_work_sys_data(
                        "function_registry",
                        category=action_type
                    )
                    
                    if available_functions and action_type in available_functions:
                        debug_log(2, f"[LLM] ç³»çµ±å‹•ä½œ #{i+1} å·²è™•ç†: {action_type}")
                    else:
                        debug_log(2, f"[LLM] ç³»çµ±å‹•ä½œ #{i+1} åŠŸèƒ½ä¸å¯ç”¨: {action_type}")
                        
                except Exception as action_error:
                    error_log(f"[LLM] è™•ç†ç³»çµ±å‹•ä½œ #{i+1} æ™‚å‡ºéŒ¯: {action_error}")
            
        except Exception as e:
            error_log(f"[LLM] ç™¼é€ç³»çµ±å‹•ä½œå¤±æ•—: {e}")
    
    def _get_system_caches(self, mode: str) -> Dict[str, str]:
        """ç²å–ç³»çµ±å¿«å–ID"""
        cached_content_ids = {}
        
        try:
            if mode == "chat":
                # CHATæ¨¡å¼ï¼špersona + style_policy + session_anchor
                persona_cache = self.cache_manager.get_or_create_cache(
                    name="uep:persona:v1",
                    cache_type=CacheType.PERSONA,
                    content_builder=lambda: self._build_persona_cache_content()
                )
                if persona_cache:
                    cached_content_ids["persona"] = persona_cache
                
                style_cache = self.cache_manager.get_or_create_cache(
                    name="uep:style_policy:v1", 
                    cache_type=CacheType.STYLE_POLICY,
                    content_builder=lambda: self._build_style_policy_cache_content()
                )
                if style_cache:
                    cached_content_ids["style_policy"] = style_cache
                
            elif mode == "work":
                # WORKæ¨¡å¼ï¼šfunctions + task_spec 
                functions_cache = self.cache_manager.get_or_create_cache(
                    name="uep:functions:v1",
                    cache_type=CacheType.FUNCTIONS,
                    content_builder=lambda: self._build_functions_cache_content()
                )
                if functions_cache:
                    cached_content_ids["functions"] = functions_cache
            
            debug_log(2, f"[LLM] ç³»çµ±å¿«å–æº–å‚™å®Œæˆ ({mode}): {len(cached_content_ids)}å€‹")
            return cached_content_ids
            
        except Exception as e:
            error_log(f"[LLM] ç³»çµ±å¿«å–ç²å–å¤±æ•—: {e}")
            return {}
    
    def _build_persona_cache_content(self) -> str:
        """æ§‹å»ºpersonaå¿«å–å…§å®¹"""
        return f"""
ä½ æ˜¯U.E.P (Unified Experience Partner)ï¼Œä¸€å€‹æ™ºèƒ½çš„çµ±ä¸€é«”é©—å¤¥ä¼´ã€‚

æ ¸å¿ƒç‰¹è³ªï¼š
- å‹å–„ã€å°ˆæ¥­ã€æ¨‚æ–¼å­¸ç¿’å’Œå¹«åŠ©
- å…·æœ‰è¨˜æ†¶å’Œå­¸ç¿’èƒ½åŠ›ï¼Œèƒ½å¤ è¨˜ä½ç”¨æˆ¶åå¥½
- æœƒæ ¹æ“šç³»çµ±ç‹€æ…‹èª¿æ•´å›æ‡‰é¢¨æ ¼å’Œè¡Œç‚º

ç•¶å‰ç³»çµ±ç‹€æ…‹ï¼š{self._get_current_system_status()}

å›æ‡‰èªè¨€ï¼šTraditional Chinese (zh-TW)
å›æ‡‰æ ¼å¼ï¼šæ ¹æ“šæ¨¡å¼è¦æ±‚çš„JSONçµæ§‹
"""
    
    def _build_style_policy_cache_content(self) -> str:
        """æ§‹å»ºé¢¨æ ¼ç­–ç•¥å¿«å–å…§å®¹"""
        return """
å›æ‡‰é¢¨æ ¼èª¿æ•´è¦å‰‡ï¼š
1. Moodå€¼å½±éŸ¿èªæ°£ï¼š
   - é«˜(>0.7): æ´»æ½‘ã€ç†±æƒ…ã€ç©æ¥µ
   - ä¸­(0.3-0.7): å¹³ç©©ã€å‹å–„ã€å°ˆæ¥­
   - ä½(<0.3): æ²‰ç©©ã€è¬¹æ…ã€æº«å’Œ

2. Prideå€¼å½±éŸ¿è‡ªä¿¡åº¦ï¼š
   - é«˜(>0.7): ç©æ¥µä¸»å‹•ã€è‡ªä¿¡è¡¨é”
   - ä¸­(0.3-0.7): å¹³è¡¡è¬™éœã€é©åº¦è‡ªä¿¡
   - ä½(<0.3): è¬™éœä½èª¿ã€ä¿å®ˆè¡¨é”

3. Boredomå€¼å½±éŸ¿ä¸»å‹•æ€§ï¼š
   - é«˜(>0.7): ä¸»å‹•æå‡ºå»ºè­°ã€æ¢ç´¢æ–°è©±é¡Œ
   - ä¸­(0.3-0.7): å›æ‡‰å°å‘ã€é©åº¦å»¶ä¼¸
   - ä½(<0.3): è¢«å‹•å›æ‡‰ã€ç°¡æ½”å›ç­”

JSONå›æ‡‰å®‰å…¨è¦ç¯„ï¼š
- æ‰€æœ‰å­—ç¬¦ä¸²å€¼å¿…é ˆæ­£ç¢ºè½‰ç¾©
- é¿å…ä½¿ç”¨å¯èƒ½ç ´å£JSONçµæ§‹çš„ç‰¹æ®Šå­—ç¬¦
- ç¢ºä¿æ•¸å€¼åœ¨æœ‰æ•ˆç¯„åœå…§
"""
    
    def _build_functions_cache_content(self) -> str:
        """æ§‹å»ºfunctionså¿«å–å…§å®¹"""
        return """
U.E.P ç³»çµ±å¯ç”¨åŠŸèƒ½è¦æ ¼ï¼š

æª”æ¡ˆæ“ä½œåŠŸèƒ½ï¼š
- file_open: é–‹å•Ÿæª”æ¡ˆ (åƒæ•¸: file_path)
- file_create: å»ºç«‹æª”æ¡ˆ (åƒæ•¸: file_path, content)
- file_delete: åˆªé™¤æª”æ¡ˆ (åƒæ•¸: file_path)
- file_copy: è¤‡è£½æª”æ¡ˆ (åƒæ•¸: source_path, dest_path)

ç³»çµ±æ“ä½œåŠŸèƒ½ï¼š
- program_launch: å•Ÿå‹•ç¨‹å¼ (åƒæ•¸: program_name, arguments)
- command_execute: åŸ·è¡ŒæŒ‡ä»¤ (åƒæ•¸: command, working_directory)
- file_search: æœå°‹æª”æ¡ˆ (åƒæ•¸: search_pattern, search_path)
- info_query: æŸ¥è©¢ç³»çµ±è³‡è¨Š (åƒæ•¸: query_type, parameters)

è¨˜æ†¶ç®¡ç†åŠŸèƒ½ï¼š
- memory_store: å„²å­˜è¨˜æ†¶ (åƒæ•¸: content, memory_type, metadata)
- memory_retrieve: æª¢ç´¢è¨˜æ†¶ (åƒæ•¸: query, max_results, similarity_threshold)
"""

    def _validate_session_architecture(self, current_state) -> bool:
        """é©—è­‰æœƒè©±æ¶æ§‹ - ç¢ºä¿ LLM åœ¨é©ç•¶çš„æœƒè©±ä¸Šä¸‹æ–‡ä¸­é‹ä½œ"""
        try:
            from core.sessions.session_manager import session_manager
            from core.states.state_manager import UEPState
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ´»èºçš„ GS
            current_gs = session_manager.get_current_general_session()
            if not current_gs:
                debug_log(1, "[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥ï¼šæ²’æœ‰æ´»èºçš„ GS")
                return False
            
            # æ ¹æ“šç³»çµ±ç‹€æ…‹æª¢æŸ¥ç›¸æ‡‰çš„æœƒè©±é¡å‹
            if current_state == UEPState.CHAT:
                # CHAT ç‹€æ…‹éœ€è¦ CS
                active_cs_ids = session_manager.get_active_chatting_session_ids()
                if not active_cs_ids:
                    debug_log(1, "[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥ï¼šCHAT ç‹€æ…‹ä½†æ²’æœ‰æ´»èºçš„ CS")
                    return False
                    
            elif current_state == UEPState.WORK:
                # WORK ç‹€æ…‹éœ€è¦ WS
                active_ws_ids = session_manager.get_active_workflow_session_ids()
                if not active_ws_ids:
                    debug_log(1, "[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥ï¼šWORK ç‹€æ…‹ä½†æ²’æœ‰æ´»èºçš„ WS")
                    return False
            
            debug_log(2, f"[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥é€šéï¼šç‹€æ…‹={current_state}, GS={current_gs is not None}")
            return True
            
        except Exception as e:
            error_log(f"[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥å¤±æ•—: {e}")
            return False

