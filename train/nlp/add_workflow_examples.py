"""
æ·»åŠ å¯¦éš›å·¥ä½œæµç›¸é—œçš„è¨“ç·´æ•¸æ“š
é‡é»åŠ å¼· summarize å’Œ archive ç­‰é—œéµå·¥ä½œæµå‹•è©çš„è­˜åˆ¥
"""

import json
from datetime import datetime
from pathlib import Path

# æ–°çš„è¨“ç·´ç¯„ä¾‹ - é‡å°å¯¦éš›å·¥ä½œæµ
new_examples = [
    # Summarize ç›¸é—œ - éƒ½æ‡‰è©²æ˜¯ DIRECT_WORK
    {
        "text": "Summarize the document",
        "segments": [{"text": "Summarize the document", "label": "DIRECT_WORK"}]
    },
    {
        "text": "Summarize all documents with research tag",
        "segments": [{"text": "Summarize all documents with research tag", "label": "DIRECT_WORK"}]
    },
    {
        "text": "Summarize the meeting notes",
        "segments": [{"text": "Summarize the meeting notes", "label": "DIRECT_WORK"}]
    },
    {
        "text": "Summarize this file for me",
        "segments": [{"text": "Summarize this file for me", "label": "DIRECT_WORK"}]
    },
    
    # Archive ç›¸é—œ - éƒ½æ‡‰è©²æ˜¯ DIRECT_WORK
    {
        "text": "Archive old documents",
        "segments": [{"text": "Archive old documents", "label": "DIRECT_WORK"}]
    },
    {
        "text": "Archive documents from last year",
        "segments": [{"text": "Archive documents from last year", "label": "DIRECT_WORK"}]
    },
    {
        "text": "Archive all files from 2023",
        "segments": [{"text": "Archive all files from 2023", "label": "DIRECT_WORK"}]
    },
    
    # å…¶ä»–å·¥ä½œæµæŒ‡ä»¤ - DIRECT_WORK
    {
        "text": "Search for python files",
        "segments": [{"text": "Search for python files", "label": "DIRECT_WORK"}]
    },
    {
        "text": "Create a backup of the database",
        "segments": [{"text": "Create a backup of the database", "label": "DIRECT_WORK"}]
    },
    {
        "text": "Generate a report for last month",
        "segments": [{"text": "Generate a report for last month", "label": "DIRECT_WORK"}]
    },
    {
        "text": "Analyze the error logs",
        "segments": [{"text": "Analyze the error logs", "label": "DIRECT_WORK"}]
    },
    
    # èŠå¤©é¡å‹ - ç¢ºä¿ä¸è¢«èª¤åˆ¤ç‚º WORK
    {
        "text": "Tell me a joke",
        "segments": [{"text": "Tell me a joke", "label": "CHAT"}]
    },
    {
        "text": "Tell me about yourself",
        "segments": [{"text": "Tell me about yourself", "label": "CHAT"}]
    },
    {
        "text": "Can you explain how this works",
        "segments": [{"text": "Can you explain how this works", "label": "CHAT"}]
    },
    {
        "text": "What do you think about this",
        "segments": [{"text": "What do you think about this", "label": "CHAT"}]
    },
    
    # è¤‡åˆæ„åœ– - å¤šå€‹ç¨ç«‹ä»»å‹™
    {
        "text": "Hey, can you summarize this document",
        "segments": [
            {"text": "Hey", "label": "CALL"},
            {"text": "can you summarize this document", "label": "DIRECT_WORK"}
        ]
    },
    {
        "text": "Backup the files and then send me a report",
        "segments": [
            {"text": "Backup the files", "label": "DIRECT_WORK"},
            {"text": "and then", "label": "UNKNOWN"},  # é€£æ¥è©
            {"text": "send me a report", "label": "DIRECT_WORK"}
        ]
    },
    {
        "text": "Archive old data then generate a summary",
        "segments": [
            {"text": "Archive old data", "label": "DIRECT_WORK"},
            {"text": "then", "label": "UNKNOWN"},  # é€£æ¥è©
            {"text": "generate a summary", "label": "DIRECT_WORK"}
        ]
    },
]

def create_bio_format(example):
    """å°‡ segment æ ¼å¼è½‰æ›ç‚º BIO æ¨™è¨»æ ¼å¼"""
    text = example["text"]
    segments = example["segments"]
    
    # Tokenize (ç°¡å–®æŒ‰ç©ºæ ¼åˆ†å‰²)
    tokens = text.split()
    bio_labels = []
    
    # è¿½è¹¤ç•¶å‰ä½ç½®
    current_pos = 0
    segment_idx = 0
    
    for token in tokens:
        # æ‰¾åˆ°é€™å€‹ token åœ¨åŸæ–‡ä¸­çš„ä½ç½®
        token_start = text.find(token, current_pos)
        token_end = token_start + len(token)
        
        # æ‰¾åˆ°é€™å€‹ token å±¬æ–¼å“ªå€‹ segment
        label = "O"
        for seg in segments:
            seg_text = seg["text"]
            seg_start = text.find(seg_text)
            seg_end = seg_start + len(seg_text)
            
            if token_start >= seg_start and token_end <= seg_end:
                # åˆ¤æ–·æ˜¯ B- é‚„æ˜¯ I-
                if token_start == seg_start or (bio_labels and not bio_labels[-1].endswith(seg["label"])):
                    label = f"B-{seg['label']}"
                else:
                    label = f"I-{seg['label']}"
                break
        
        bio_labels.append(label)
        current_pos = token_end
    
    return tokens, bio_labels

def main():
    """ç”Ÿæˆä¸¦æ·»åŠ æ–°çš„è¨“ç·´æ•¸æ“š"""
    project_root = Path(__file__).parent.parent.parent
    output_file = project_root / "train" / "nlp" / "workflow_additional_examples.jsonl"
    
    print(f"ğŸ“ æº–å‚™ç”Ÿæˆ {len(new_examples)} å€‹æ–°è¨“ç·´ç¯„ä¾‹...")
    
    # è½‰æ›ç‚ºå®Œæ•´æ ¼å¼
    formatted_examples = []
    for i, example in enumerate(new_examples):
        tokens, bio_labels = create_bio_format(example)
        
        formatted = {
            "id": f"workflow_additional_{i:04d}",
            "text": example["text"],
            "tokens": tokens,
            "bio_labels": bio_labels,
            "segments": [
                {
                    "text": seg["text"],
                    "label": seg["label"],
                    "start": example["text"].find(seg["text"]),
                    "end": example["text"].find(seg["text"]) + len(seg["text"]),
                    "confidence": 1.0,
                    "annotator_notes": "Manual workflow example"
                }
                for seg in example["segments"]
            ],
            "metadata": {
                "source": "workflow_manual_addition",
                "scenario": "single_intent" if len(example["segments"]) == 1 else "compound_intent",
                "created_date": datetime.now().isoformat(),
                "annotated": True,
                "quality_checked": True,
                "annotator": "human"
            }
        }
        formatted_examples.append(formatted)
    
    # å¯«å…¥æ–‡ä»¶
    with open(output_file, "w", encoding="utf-8") as f:
        for example in formatted_examples:
            f.write(json.dumps(example, ensure_ascii=False) + "\n")
    
    print(f"âœ… å·²ç”Ÿæˆ {len(formatted_examples)} å€‹ç¯„ä¾‹åˆ°: {output_file}")
    print("\nğŸ“Š ç¯„ä¾‹çµ±è¨ˆ:")
    
    # çµ±è¨ˆå„é¡å‹æ•¸é‡
    label_counts = {}
    for ex in formatted_examples:
        for seg in ex["segments"]:
            label = seg["label"]
            label_counts[label] = label_counts.get(label, 0) + 1
    
    for label, count in sorted(label_counts.items()):
        print(f"  - {label}: {count} å€‹ segment")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("1. æª¢æŸ¥ç”Ÿæˆçš„ç¯„ä¾‹æ˜¯å¦æ­£ç¢º")
    print("2. åˆä½µåˆ°ä¸»è¨“ç·´æ•¸æ“š: python train/nlp/merge_training_data.py")
    print("3. é‡æ–°è¨“ç·´æ¨¡å‹: python train/nlp/train_bio_model.py")

if __name__ == "__main__":
    main()
