#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èªéŸ³æ´»å‹•æª¢æ¸¬ (VAD) æ¨¡çµ„
"""

import numpy as np
import time
from typing import Dict, List, Tuple, Optional

try:
    import torch
    import torchaudio
    from pyannote.audio import Model
    VAD_AVAILABLE = True
except ImportError:
    VAD_AVAILABLE = False

from utils.debug_helper import debug_log, info_log, error_log

class VoiceActivityDetection:
    """èªéŸ³æ´»å‹•æª¢æ¸¬é¡"""
    
    def __init__(self, sample_rate: int = 16000, sensitivity: float = 0.7, min_speech_duration: float = 0.3):
        self.sample_rate = sample_rate
        self.vad_model = None
        
        # VAD åƒæ•¸ï¼ˆå¯é€é user_settings èª¿æ•´ï¼‰
        self.sensitivity = sensitivity  # éˆæ•åº¦ 0.0-1.0
        self.energy_threshold = 0.0005 * (2.0 - sensitivity)  # éˆæ•åº¦è¶Šé«˜ï¼Œé–¾å€¼è¶Šä½
        self.silence_duration_threshold = 0.8  # éœéŸ³æŒçºŒæ™‚é–“é–¾å€¼ï¼ˆç§’ï¼‰
        self.speech_duration_threshold = min_speech_duration  # æœ€å°èªéŸ³æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
        
        info_log(f"[VAD] èªéŸ³æ´»å‹•æª¢æ¸¬æ¨¡çµ„åˆå§‹åŒ– (éˆæ•åº¦: {sensitivity:.2f}, æœ€å°èªéŸ³æ™‚é•·: {min_speech_duration:.2f}s)")
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ– VAD æ¨¡å‹"""
        try:
            if VAD_AVAILABLE:
                info_log("[VAD] å˜—è©¦è¼‰å…¥ pyannote VAD æ¨¡å‹...")
                # å¯ä»¥å˜—è©¦è¼‰å…¥ pyannote çš„ VAD æ¨¡å‹
                # self.vad_model = Model.from_pretrained("pyannote/voice-activity-detection")
                # æš«æ™‚å…ˆç”¨ç°¡åŒ–å¯¦ç¾
                pass
            
            info_log("[VAD] ä½¿ç”¨åŸºæ–¼èƒ½é‡çš„ VAD å¯¦ç¾")
            return True
            
        except Exception as e:
            error_log(f"[VAD] åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def detect_voice_activity(self, audio_data: np.ndarray, window_size: float = 0.025) -> List[Dict]:
        """æª¢æ¸¬èªéŸ³æ´»å‹•
        
        Args:
            audio_data: éŸ³é »æ•¸æ“š
            window_size: çª—å£å¤§å°ï¼ˆç§’ï¼‰
            
        Returns:
            èªéŸ³æ´»å‹•äº‹ä»¶åˆ—è¡¨
        """
        try:
            debug_log(3, "[VAD] é–‹å§‹èªéŸ³æ´»å‹•æª¢æ¸¬...")
            
            # æ­£è¦åŒ–éŸ³é »
            audio_float = audio_data.astype(np.float32) / 32768.0
            
            # è¨ˆç®—çª—å£å¤§å°
            window_samples = int(window_size * self.sample_rate)
            n_windows = len(audio_float) // window_samples
            
            events = []
            current_state = "silence"
            current_start_time = 0.0
            
            for i in range(n_windows):
                start_idx = i * window_samples
                end_idx = min((i + 1) * window_samples, len(audio_float))
                window = audio_float[start_idx:end_idx]
                
                # è¨ˆç®—çª—å£èƒ½é‡
                energy = self._compute_energy(window)
                time_stamp = i * window_size
                
                # åˆ¤æ–·èªéŸ³æ´»å‹•
                is_speech = self._is_speech(energy)
                
                if current_state == "silence" and is_speech:
                    # å¾éœéŸ³è½‰ç‚ºèªéŸ³
                    events.append({
                        'event_type': 'speech_start',
                        'timestamp': time_stamp,
                        'confidence': min(energy / self.energy_threshold, 1.0),
                        'energy_level': energy
                    })
                    current_state = "speech"
                    current_start_time = time_stamp
                    
                elif current_state == "speech" and not is_speech:
                    # å¾èªéŸ³è½‰ç‚ºéœéŸ³
                    speech_duration = time_stamp - current_start_time
                    if speech_duration >= self.speech_duration_threshold:
                        events.append({
                            'event_type': 'speech_end',
                            'timestamp': time_stamp,
                            'confidence': 0.8,
                            'energy_level': energy,
                            'duration': speech_duration
                        })
                    current_state = "silence"
                    current_start_time = time_stamp
            
            # ğŸ”§ BUGFIX: å¦‚æœéŸ³é »æœ«å°¾é‚„åœ¨èªªè©±ï¼Œè£œå……ä¸€å€‹ speech_end äº‹ä»¶
            if current_state == "speech":
                final_duration = (n_windows * window_size) - current_start_time
                if final_duration >= self.speech_duration_threshold:
                    events.append({
                        'event_type': 'speech_end',
                        'timestamp': n_windows * window_size,
                        'confidence': 0.8,
                        'energy_level': 0.0,
                        'duration': final_duration,
                        'is_incomplete': True  # æ¨™è¨˜ç‚ºæœªå®Œæˆçš„èªéŸ³ç‰‡æ®µ
                    })
                    debug_log(3, f"[VAD] æª¢æ¸¬åˆ°æœªå®Œæˆçš„èªéŸ³ç‰‡æ®µ: {final_duration:.3f}s")
            
            debug_log(3, f"[VAD] æª¢æ¸¬åˆ° {len(events)} å€‹èªéŸ³äº‹ä»¶")
            return events
            
        except Exception as e:
            error_log(f"[VAD] èªéŸ³æ´»å‹•æª¢æ¸¬å¤±æ•—: {str(e)}")
            return []
    
    def _compute_energy(self, window: np.ndarray) -> float:
        """è¨ˆç®—çª—å£èƒ½é‡"""
        if len(window) == 0:
            return 0.0
        return np.mean(window ** 2) # type: ignore
    
    def _is_speech(self, energy: float) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºèªéŸ³"""
        return energy > self.energy_threshold
    
    def has_sufficient_speech(self, audio_data: np.ndarray, min_duration: float = 0.05) -> bool:
        """æª¢æŸ¥éŸ³é »æ˜¯å¦åŒ…å«è¶³å¤ çš„èªéŸ³å…§å®¹
        
        Args:
            audio_data: éŸ³é »æ•¸æ“š
            min_duration: æœ€å°èªéŸ³æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦åŒ…å«è¶³å¤ èªéŸ³
        """
        try:
            events = self.detect_voice_activity(audio_data)
            
            # è¨ˆç®—ç¸½èªéŸ³æ™‚é–“
            total_speech_time = 0.0
            for event in events:
                if event['event_type'] == 'speech_end':
                    total_speech_time += event.get('duration', 0.0)
            
            has_speech = total_speech_time >= min_duration
            debug_log(3, f"[VAD] èªéŸ³æ™‚é–“: {total_speech_time:.2f}s, éœ€æ±‚: {min_duration:.2f}s, é€šé: {has_speech}")
            
            return has_speech
            
        except Exception as e:
            error_log(f"[VAD] èªéŸ³æª¢æŸ¥å¤±æ•—: {str(e)}")
            return False
    
    def extract_speech_segments(self, audio_data: np.ndarray) -> List[Tuple[float, float, np.ndarray]]:
        """æå–èªéŸ³ç‰‡æ®µ
        
        Returns:
            List[(start_time, end_time, audio_segment)]
        """
        try:
            events = self.detect_voice_activity(audio_data)
            segments = []
            
            speech_start = None
            for event in events:
                if event['event_type'] == 'speech_start':
                    speech_start = event['timestamp']
                elif event['event_type'] == 'speech_end' and speech_start is not None:
                    speech_end = event['timestamp']
                    
                    # æå–éŸ³é »ç‰‡æ®µ
                    start_idx = int(speech_start * self.sample_rate)
                    end_idx = int(speech_end * self.sample_rate)
                    segment = audio_data[start_idx:end_idx]
                    
                    segments.append((speech_start, speech_end, segment))
                    speech_start = None
            
            debug_log(3, f"[VAD] æå– {len(segments)} å€‹èªéŸ³ç‰‡æ®µ")
            return segments
            
        except Exception as e:
            error_log(f"[VAD] èªéŸ³ç‰‡æ®µæå–å¤±æ•—: {str(e)}")
            return []
    
    def update_thresholds(self, energy_threshold: Optional[float] = None, 
                         silence_duration: Optional[float] = None,
                         speech_duration: Optional[float] = None):
        """æ›´æ–° VAD é–¾å€¼"""
        if energy_threshold is not None:
            self.energy_threshold = max(0.001, energy_threshold)
            info_log(f"[VAD] æ›´æ–°èƒ½é‡é–¾å€¼: {self.energy_threshold}")
            
        if silence_duration is not None:
            self.silence_duration_threshold = max(0.1, silence_duration)
            info_log(f"[VAD] æ›´æ–°éœéŸ³é–¾å€¼: {self.silence_duration_threshold}")
            
        if speech_duration is not None:
            self.speech_duration_threshold = max(0.1, speech_duration)
            info_log(f"[VAD] æ›´æ–°èªéŸ³é–¾å€¼: {self.speech_duration_threshold}")
    
    def get_audio_quality_score(self, audio_data: np.ndarray) -> float:
        """è©•ä¼°éŸ³é »å“è³ªåˆ†æ•¸ (0-1)"""
        try:
            audio_float = audio_data.astype(np.float32) / 32768.0
            
            # è¨ˆç®—ä¿¡å™ªæ¯”ç›¸é—œæŒ‡æ¨™
            energy = np.mean(audio_float ** 2)
            peak_energy = np.max(np.abs(audio_float))
            zero_crossing_rate = np.mean(np.diff(np.sign(audio_float)) != 0)
            
            # ç°¡å–®çš„å“è³ªè©•åˆ†
            energy_score = min(energy * 100, 1.0)  # èƒ½é‡åˆ†æ•¸
            peak_score = min(peak_energy * 2, 1.0)  # å³°å€¼åˆ†æ•¸
            clarity_score = max(0, 1 - zero_crossing_rate * 2)  # æ¸…æ™°åº¦åˆ†æ•¸
            
            # åŠ æ¬Šå¹³å‡
            quality_score = (energy_score * 0.4 + peak_score * 0.3 + clarity_score * 0.3)
            
            debug_log(3, f"[VAD] éŸ³é »å“è³ª: {quality_score:.3f} (èƒ½é‡:{energy_score:.3f}, å³°å€¼:{peak_score:.3f}, æ¸…æ™°åº¦:{clarity_score:.3f})")
            
            return quality_score
            
        except Exception as e:
            error_log(f"[VAD] å“è³ªè©•ä¼°å¤±æ•—: {str(e)}")
            return 0.5  # ä¸­ç­‰å“è³ª
    
    def shutdown(self):
        """é—œé–‰ VAD æ¨¡çµ„"""
        info_log("[VAD] èªéŸ³æ´»å‹•æª¢æ¸¬æ¨¡çµ„å·²é—œé–‰")
