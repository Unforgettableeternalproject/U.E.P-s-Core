# modules/llm_module/llm_module.py
"""
LLM æ¨¡çµ„é‡æ§‹ç‰ˆæœ¬

æ–°åŠŸèƒ½ï¼š
1. æ”¯æ´ CHAT å’Œ WORK ç‹€æ…‹åˆ†é›¢è™•ç†
2. æ•´åˆ StatusManager ç³»çµ±æ•¸å€¼ç®¡ç†
3. Context Caching ä¸Šä¸‹æ–‡å¿«å–
4. å­¸ç¿’åŠŸèƒ½ï¼šè¨˜éŒ„ä½¿ç”¨è€…åå¥½å’Œå°è©±é¢¨æ ¼
5. èˆ‡ Working Context å’Œèº«ä»½ç®¡ç†ç³»çµ±æ•´åˆ
6. å…§å»º Prompt ç®¡ç†ï¼Œä¸å†ä¾è³´å¤–éƒ¨ prompt_builder
"""

import re
import time
import json
from typing import Dict, Any, Optional, List
from pathlib import Path

from core.bases.module_base import BaseModule
from core.working_context import working_context_manager, ContextType
from core.status_manager import status_manager
from core.states.state_manager import state_manager, UEPState

from .schemas import (
    LLMInput, LLMOutput, SystemAction, LLMMode, SystemState,
    ConversationEntry, LearningData, StatusUpdate
)
from .gemini_client import GeminiWrapper
from .prompt_manager import PromptManager
from .learning_engine import LearningEngine
from .cache_manager import cache_manager, CacheType
from .mcp_client import MCPClient
from .module_interfaces import (
    state_aware_interface, CollaborationChannel, set_collaboration_state
)

from configs.config_loader import load_module_config
from utils.debug_helper import debug_log, info_log, error_log


class LLMModule(BaseModule):
    def __init__(self, config=None):
        super().__init__()
        self.config = config or load_module_config("llm_module")
        
        # æ ¸å¿ƒçµ„ä»¶
        self.model = GeminiWrapper(self.config)
        self.prompt_manager = PromptManager(self.config)
        self.learning_engine = LearningEngine(self.config.get("learning", {}))
        
        # çµ±ä¸€å¿«å–ç®¡ç†å™¨ (æ•´åˆGeminié¡¯æ€§å¿«å– + æœ¬åœ°å¿«å–)
        self.cache_manager = cache_manager
        
        # ç‹€æ…‹å’Œæœƒè©±ç®¡ç†
        self.state_manager = state_manager
        self.status_manager = status_manager
        self.session_info = {}
        
        # ç‹€æ…‹æ„ŸçŸ¥æ¨¡çµ„æ¥å£
        self.module_interface = state_aware_interface
        
        # MCP å®¢æˆ¶ç«¯ (ç”¨æ–¼èˆ‡ SYS æ¨¡çµ„çš„ MCP Server é€šè¨Š)
        # âœ… å‚³é self ä»¥ä¾¿ MCP Client å¯ä»¥ç²å–ç•¶å‰æœƒè©±ä¿¡æ¯
        self.mcp_client = MCPClient(llm_module=self)
        
        # ğŸ”§ å·¥ä½œæµäº‹ä»¶éšŠåˆ—ï¼ˆåˆå§‹åŒ–ç‚ºç©ºåˆ—è¡¨ï¼Œé˜²æ­¢éºç•™èˆŠäº‹ä»¶ï¼‰
        self._pending_workflow_events = []
        
        # ğŸ”§ å·¥ä½œæµå®Œæˆäº‹ä»¶å»é‡ï¼ˆè¿½è¹¤å·²è™•ç†çš„ (session_id, complete=True) äº‹ä»¶ï¼‰
        self._processed_workflow_completions = set()
        
        # ç›£è½ç³»çµ±ç‹€æ…‹è®ŠåŒ–ä»¥è‡ªå‹•åˆ‡æ›å”ä½œç®¡é“
        self._setup_state_listener()
        
        # çµ±è¨ˆæ•¸æ“š
        self.processing_stats = {
            "total_requests": 0,
            "chat_requests": 0, 
            "work_requests": 0,
            "total_processing_time": 0.0,
            "cache_hits": 0
        }

    def debug(self):
        # Debug level = 1
        debug_log(1, "[LLM] Debug æ¨¡å¼å•Ÿç”¨ - é‡æ§‹ç‰ˆæœ¬")
        # Debug level = 2  
        debug_log(2, f"[LLM] æ¨¡å‹åç¨±: {self.model.model_name}")
        debug_log(2, f"[LLM] æº«åº¦: {self.model.temperature}")
        debug_log(2, f"[LLM] Top P: {self.model.top_p}")
        debug_log(2, f"[LLM] æœ€å¤§è¼¸å‡ºå­—å…ƒæ•¸: {self.model.max_tokens}")
        debug_log(2, f"[LLM] çµ±ä¸€å¿«å–ç®¡ç†å™¨: å•Ÿç”¨ (Gemini + æœ¬åœ°å¿«å–)")
        debug_log(2, f"[LLM] Learning Engine: {'å•Ÿç”¨' if self.learning_engine.learning_enabled else 'åœç”¨'}")
        debug_log(2, f"[LLM] MCP Client: {'å·²é€£æ¥' if self.mcp_client.mcp_server else 'æœªé€£æ¥'}")
        # Debug level = 4
        debug_log(4, f"[LLM] å®Œæ•´æ¨¡çµ„è¨­å®š: {self.config}")
    
    def _setup_state_listener(self):
        """è¨­å®šç³»çµ±ç‹€æ…‹ç›£è½å™¨ï¼Œè‡ªå‹•åˆ‡æ›å”ä½œç®¡é“"""
        try:
            # ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹ä¸¦è¨­å®šåˆå§‹å”ä½œç®¡é“
            current_state = self.state_manager.get_current_state()
            set_collaboration_state(current_state)
            
            debug_log(2, f"[LLM] ç‹€æ…‹æ„ŸçŸ¥æ¨¡çµ„æ¥å£è¨­å®šå®Œæˆï¼Œåˆå§‹ç‹€æ…‹: {current_state}")
            debug_log(3, f"[LLM] ç®¡é“ç‹€æ…‹: {self.module_interface.get_channel_status()}")
            
        except Exception as e:
            error_log(f"[LLM] ç‹€æ…‹ç›£è½å™¨è¨­å®šå¤±æ•—: {e}")
    
    def _update_collaboration_channels(self, new_state: UEPState):
        """æ ¹æ“šç³»çµ±ç‹€æ…‹æ›´æ–°å”ä½œç®¡é“"""
        try:
            old_status = self.module_interface.get_channel_status()
            set_collaboration_state(new_state)
            new_status = self.module_interface.get_channel_status()
            
            if old_status != new_status:
                debug_log(2, f"[LLM] å”ä½œç®¡é“æ›´æ–°: {old_status} â†’ {new_status}")
                
        except Exception as e:
            error_log(f"[LLM] å”ä½œç®¡é“æ›´æ–°å¤±æ•—: {e}")
        
    def initialize(self):
        """åˆå§‹åŒ– LLM æ¨¡çµ„"""
        debug_log(1, "[LLM] åˆå§‹åŒ–ä¸­...")
        self.debug()
        
        try:
            # Gemini å®¢æˆ¶ç«¯åœ¨ __init__ ä¸­å·²ç¶“åˆå§‹åŒ–ï¼Œæª¢æŸ¥æ˜¯å¦æ­£å¸¸
            if not hasattr(self.model, 'client') or self.model.client is None:
                error_log("[LLM] Gemini æ¨¡å‹åˆå§‹åŒ–å¤±æ•—")
                return False
            
            # è¨»å†Š StatusManager å›èª¿
            self.status_manager.register_update_callback("llm_module", self._on_status_update)
            
            # ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹
            current_state = self.state_manager.get_current_state()
            debug_log(2, f"[LLM] ç•¶å‰ç³»çµ±ç‹€æ…‹: {current_state}")
            
            # âœ… é€£æ¥ event_bus ä¸¦è¨‚é–±å·¥ä½œæµäº‹ä»¶
            try:
                from core.event_bus import event_bus, SystemEvent
                self.event_bus = event_bus
                # è¨‚é–±å·¥ä½œæµæ­¥é©Ÿå®Œæˆäº‹ä»¶
                self.event_bus.subscribe(
                    SystemEvent.WORKFLOW_STEP_COMPLETED,
                    self._handle_workflow_step_completed,
                    handler_name="LLM.workflow_step_handler"
                )
                # ğŸ†• è¨‚é–±å·¥ä½œæµå¤±æ•—äº‹ä»¶
                self.event_bus.subscribe(
                    SystemEvent.WORKFLOW_FAILED,
                    self._handle_workflow_failed,
                    handler_name="LLM.workflow_error_handler"
                )
                # ğŸ†• è¨‚é–±è¼¸å‡ºå±¤å®Œæˆäº‹ä»¶ï¼ˆç”¨æ–¼è™•ç†å¾…è™•ç†çš„äº’å‹•æ­¥é©Ÿæç¤ºï¼‰
                self.event_bus.subscribe(
                    SystemEvent.OUTPUT_LAYER_COMPLETE,
                    self._handle_output_complete,
                    handler_name="LLM.output_complete_handler"
                )
                debug_log(2, "[LLM] Event bus å·²é€£æ¥ï¼Œå·²è¨‚é–± WORKFLOW_STEP_COMPLETED, WORKFLOW_FAILED å’Œ OUTPUT_LAYER_COMPLETE äº‹ä»¶")
            except Exception as e:
                error_log(f"[LLM] ç„¡æ³•é€£æ¥ event bus: {e}")
                self.event_bus = None
            
            self.is_initialized = True
            info_log("[LLM] LLM æ¨¡çµ„é‡æ§‹ç‰ˆåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            error_log(f"[LLM] åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def _handle_workflow_step_completed(self, event):
        """
        âœ… è™•ç†å·¥ä½œæµæ­¥é©Ÿå®Œæˆäº‹ä»¶
        
        ç•¶ SYS åœ¨èƒŒæ™¯å®Œæˆä¸€å€‹æ­¥é©Ÿå¾Œï¼Œæ­¤æ–¹æ³•æœƒè¢«èª¿ç”¨ï¼š
        1. å¯©æ ¸æ­¥é©Ÿçµæœ
        2. æ±ºå®šæ˜¯å¦æ‰¹å‡†ã€ä¿®æ”¹æˆ–å–æ¶ˆ
        3. èª¿ç”¨ç›¸æ‡‰çš„ MCP å·¥å…·
        
        Args:
            event: Event object containing step completion data
        """
        try:
            debug_log(2, f"[LLM] æ”¶åˆ°å·¥ä½œæµæ­¥é©Ÿå®Œæˆäº‹ä»¶: {event.event_id}")
            
            data = event.data
            session_id = data.get("session_id")
            workflow_type = data.get("workflow_type")
            step_result = data.get("step_result", {})
            requires_review = data.get("requires_llm_review", False)
            review_data = data.get("llm_review_data")
            
            # ğŸ”§ å»é‡æª¢æŸ¥ï¼šå¦‚æœæ˜¯å®Œæˆäº‹ä»¶ä¸”å·²è™•ç†éï¼Œè·³é
            is_complete = step_result.get('complete', False)
            if is_complete and session_id in self._processed_workflow_completions:
                debug_log(2, f"[LLM] âš ï¸ è·³éé‡è¤‡çš„å·¥ä½œæµå®Œæˆäº‹ä»¶: {session_id}")
                return
            
            # ğŸ”§ æ¨™è¨˜ç‚ºå·²è™•ç†ï¼ˆåœ¨åŠ å…¥éšŠåˆ—å‰å°±æ¨™è¨˜ï¼Œé¿å…é‡è¤‡åŠ å…¥ï¼‰
            if is_complete:
                self._processed_workflow_completions.add(session_id)
                debug_log(2, f"[LLM] âœ… å·²æ¨™è¨˜å·¥ä½œæµå®Œæˆäº‹ä»¶: {session_id}")
            
            debug_log(2, f"[LLM] å·¥ä½œæµ {workflow_type} ({session_id}) æ­¥é©Ÿå®Œæˆ")
            debug_log(3, f"[LLM] éœ€è¦å¯©æ ¸: {requires_review}, çµæœ: {step_result.get('success')}")
            debug_log(2, f"[LLM] æ¥æ”¶åˆ°çš„ review_data keys: {list(review_data.keys()) if review_data else 'None'}")
            
            # ğŸ”§ æª¢æŸ¥æ˜¯å¦ç‚º LLM_PROCESSING è«‹æ±‚
            requires_llm_processing = data.get('requires_llm_processing', False)
            if requires_llm_processing:
                debug_log(2, f"[LLM] æª¢æ¸¬åˆ° LLM_PROCESSING è«‹æ±‚")
                llm_request_data = data.get('llm_request_data', {})
                
                # ğŸ”§ å»é‡æª¢æŸ¥ï¼šé¿å…é‡è¤‡è™•ç†åŒä¸€å€‹ LLM_PROCESSING æ­¥é©Ÿ
                step_id = llm_request_data.get('step_id', '')
                processing_key = f"{session_id}:{step_id}"
                
                if not hasattr(self, '_processed_llm_steps'):
                    self._processed_llm_steps = set()
                
                if processing_key in self._processed_llm_steps:
                    debug_log(2, f"[LLM] âš ï¸ è·³éé‡è¤‡çš„ LLM_PROCESSING è«‹æ±‚: {processing_key}")
                    return
                
                # æ¨™è¨˜ç‚ºå·²è™•ç†
                self._processed_llm_steps.add(processing_key)
                debug_log(2, f"[LLM] âœ… é–‹å§‹è™•ç† LLM_PROCESSING è«‹æ±‚: {processing_key}")
                
                self._handle_llm_processing_request(session_id, workflow_type, llm_request_data)
                return
            
            # ğŸ†• æª¢æŸ¥æ˜¯å¦ç‚ºå·¥ä½œæµå®Œæˆï¼ˆæœ€å¾Œä¸€æ­¥ï¼‰
            is_workflow_complete = step_result.get('complete', False)
            debug_log(2, f"[LLM] æ­¥é©Ÿçµæœæ•¸æ“š: {step_result}")
            debug_log(2, f"[LLM] å·¥ä½œæµå®Œæˆæ¨™è¨˜: {is_workflow_complete}")
            should_respond_to_user = review_data and review_data.get('requires_user_response', False) if review_data else False
            should_end_session = review_data and review_data.get('should_end_session', False) if review_data else False
            
            # ğŸ†• ç²å–ç•¶å‰æ­¥é©Ÿå’Œä¸‹ä¸€æ­¥è³‡è¨Š
            # ğŸ”§ å„ªå…ˆå¾ review_data ä¸­è®€å–ï¼Œå¦å‰‡å¾ data ä¸­è®€å–
            current_step_info = (review_data.get('current_step_info') if review_data else None) or data.get('current_step_info')
            next_step_info = (review_data.get('next_step_info') if review_data else None) or data.get('next_step_info')
            
            # ğŸ”§ ä¿®æ­£ï¼šæª¢æŸ¥ç•¶å‰æ­¥é©Ÿæ˜¯å¦ç‚º Interactiveï¼ˆç­‰å¾…è¼¸å…¥ï¼‰ï¼Œä¸”ä¸æœƒè¢«è·³é
            # âš ï¸ é‡è¦ï¼šå¦‚æœæ­¥é©Ÿæœƒè¢«è·³éï¼ˆstep_will_be_skipped=Trueï¼‰ï¼Œä¸æ‡‰è¦–ç‚ºéœ€è¦äº’å‹•
            current_step_is_interactive = (
                current_step_info 
                and current_step_info.get('step_type') == 'interactive' 
                and not current_step_info.get('step_will_be_skipped', False)
            ) if current_step_info else False
            
            next_step_is_interactive = (
                next_step_info 
                and next_step_info.get('step_type') == 'interactive'
                and not next_step_info.get('step_will_be_skipped', False)
            ) if next_step_info else False
            
            debug_log(3, f"[LLM] ç•¶å‰æ­¥é©Ÿè³‡è¨Š: {current_step_info}")
            debug_log(3, f"[LLM] ä¸‹ä¸€æ­¥è³‡è¨Š: {next_step_info}")
            debug_log(3, f"[LLM] ç•¶å‰æ­¥é©Ÿæ˜¯äº’å‹•æ­¥é©Ÿ: {current_step_is_interactive}")
            debug_log(3, f"[LLM] ä¸‹ä¸€æ­¥æ˜¯äº’å‹•æ­¥é©Ÿ: {next_step_is_interactive}")
            
            # ğŸ”§ éæ¿¾æ¢ä»¶ï¼šå¦‚æœä¸éœ€è¦å¯©æ ¸ä¸”å·¥ä½œæµæœªå®Œæˆ
            # âš ï¸ é‡è¦ï¼šå·¥ä½œæµå®Œæˆæ™‚å³ä½¿ä¸éœ€è¦å¯©æ ¸ä¹Ÿè¦ç”Ÿæˆæœ€çµ‚ç¸½çµ
            if not requires_review and not is_workflow_complete:
                debug_log(2, f"[LLM] æ­¥é©Ÿä¸éœ€è¦å¯©æ ¸ä¸”å·¥ä½œæµæœªå®Œæˆ")
                return
            
            # ğŸ”§ å¯¦æ–½ 3 æ™‚åˆ»å›æ‡‰æ¨¡å¼ï¼š
            # 1. å·¥ä½œæµè§¸ç™¼ - ç”± start_workflow MCP å·¥å…·è™•ç†ï¼ˆä¸åœ¨é€™è£¡ï¼‰
            # 2. ç•¶å‰æ­¥é©Ÿç‚ºäº’å‹•æ­¥é©Ÿï¼Œæˆ–ä¸‹ä¸€æ­¥ç‚ºäº’å‹•æ­¥é©Ÿ - éœ€è¦ç”Ÿæˆæç¤ºçµ¦ä½¿ç”¨è€…
            # 3. å·¥ä½œæµå®Œæˆ - éœ€è¦ç”Ÿæˆæœ€çµ‚çµæœ
            should_generate_response = is_workflow_complete or current_step_is_interactive or next_step_is_interactive
            
            if not should_generate_response:
                debug_log(2, f"[LLM] æ­¥é©Ÿå®Œæˆï¼Œä¸‹ä¸€æ­¥éäº’å‹•æ­¥é©Ÿï¼Œéœé»˜æ‰¹å‡†ä¸¦æ¨é€²")
                # âœ… éœé»˜æ‰¹å‡†ï¼šProcessing æ­¥é©Ÿå¾Œè‡ªå‹•æ¨é€²ï¼Œä¸ç”Ÿæˆå›æ‡‰
                self._approve_workflow_step(session_id, None)
                return
            
            # âœ… éœ€è¦ç”Ÿæˆå›æ‡‰ï¼šå°‡å·¥ä½œæµäº‹ä»¶åŠ å…¥å¾…è™•ç†éšŠåˆ—
            if not hasattr(self, '_pending_workflow_events'):
                self._pending_workflow_events = []
            
            debug_log(2, f"[LLM] å°‡å·¥ä½œæµäº‹ä»¶åŠ å…¥å¾…è™•ç†éšŠåˆ—ï¼Œreview_data keys: {list(review_data.keys()) if review_data else 'None'}")
            
            self._pending_workflow_events.append({
                "type": "workflow_step_completed" if not is_workflow_complete else "workflow_completed",
                "session_id": session_id,
                "workflow_type": workflow_type,
                "step_result": step_result,
                "review_data": review_data,
                "is_complete": is_workflow_complete,
                "should_respond": should_respond_to_user,
                "should_end_session": should_end_session,
                "current_step_info": current_step_info,  # ğŸ†• å‚³éç•¶å‰æ­¥é©Ÿè³‡è¨Š
                "next_step_info": next_step_info,  # ğŸ†• å‚³éä¸‹ä¸€æ­¥è³‡è¨Š
                "timestamp": time.time()
            })
            
            info_log(f"[LLM] å·¥ä½œæµäº‹ä»¶å·²åŠ å…¥éšŠåˆ—: {workflow_type}, is_complete={is_workflow_complete}, current_interactive={current_step_is_interactive}")
            
            # ğŸ†• è™•ç†éœ€è¦ç”Ÿæˆå›æ‡‰çš„æƒ…æ³
            # ğŸ”§ ä¿®æ­£ï¼šå·¥ä½œæµå®Œæˆæ™‚ä¸æª¢æŸ¥äº’å‹•æ­¥é©Ÿï¼Œç›´æ¥è™•ç†å®Œæˆé‚è¼¯
            if is_workflow_complete:
                debug_log(2, f"[LLM] å·¥ä½œæµå®Œæˆï¼Œç«‹å³ç”Ÿæˆæœ€çµ‚ç¸½çµå›æ‡‰")
                self._process_workflow_completion(session_id, workflow_type, step_result, review_data)
                return  # âš ï¸ é‡è¦ï¼šå·¥ä½œæµå®Œæˆå¾Œç›´æ¥è¿”å›ï¼Œä¸å†è™•ç†å¾ŒçºŒé‚è¼¯
            elif current_step_is_interactive or next_step_is_interactive:
                # âš ï¸ é—œéµä¿®æ­£ï¼šè¨‚é–± OUTPUT_LAYER_COMPLETEï¼Œåœ¨ç•¶å‰ cycle çš„è¼¸å‡ºå®Œæˆå¾Œå†è™•ç†
                # é€™æ¨£å¯ä»¥ç¢ºä¿äº’å‹•æ­¥é©Ÿæç¤ºåœ¨æ­£ç¢ºçš„é †åºç”Ÿæˆ
                debug_log(2, f"[LLM] ç•¶å‰æˆ–ä¸‹ä¸€æ­¥æ˜¯äº’å‹•æ­¥é©Ÿï¼Œè¨‚é–± OUTPUT_LAYER_COMPLETE ç­‰å¾…ç•¶å‰è¼¸å‡ºå®Œæˆ")
                
                # ä¿å­˜å¾…è™•ç†çš„äº’å‹•æ­¥é©Ÿä¿¡æ¯
                if not hasattr(self, '_pending_interactive_prompts'):
                    self._pending_interactive_prompts = []
                
                self._pending_interactive_prompts.append({
                    'session_id': session_id,
                    'workflow_type': workflow_type,
                    'step_result': step_result,
                    'review_data': review_data,
                    'next_step_info': next_step_info,
                    'current_cycle_session': self._get_current_gs_id()  # è¨˜éŒ„ç•¶å‰ cycle çš„ session
                })
                
                # ğŸ”§ ä¿®æ­£ï¼šä¸è¦ç«‹å³æ‰¹å‡†æ­¥é©Ÿï¼
                # ç•¶ä¸‹ä¸€æ­¥æ˜¯äº’å‹•æ­¥é©Ÿæ™‚ï¼ŒLLM æ‡‰è©²ç”Ÿæˆæç¤ºçµ¦ç”¨æˆ¶ï¼Œä½†ä¸æ‰¹å‡†ç•¶å‰æ­¥é©Ÿ
                # å·¥ä½œæµæœƒè‡ªå‹•é€²å…¥ç­‰å¾…è¼¸å…¥ç‹€æ…‹ï¼ˆå› ç‚º _auto_advance æª¢æ¸¬åˆ° InteractiveStepï¼‰
                # åªæœ‰ç•¶ LLM éœ€è¦æ‰¹å‡†ç•¶å‰æ­¥é©Ÿçš„çµæœæ™‚æ‰èª¿ç”¨ approve_step
                # åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼ˆLLM_PROCESSING â†’ INTERACTIVEï¼‰ï¼ŒLLM åªæ˜¯æä¾›æç¤ºï¼Œä¸æ‰¹å‡†
                
                # âš ï¸ é‡è¦ï¼šç«‹å³è™•ç†äº’å‹•æ­¥é©Ÿæç¤ºï¼Œä¸ç­‰å¾… OUTPUT å®Œæˆ
                # å› ç‚ºåœ¨æŸäº›æƒ…æ³ä¸‹ï¼ˆå¦‚æ­¥é©Ÿå®Œæˆäº‹ä»¶æ™šæ–¼è¼¸å‡ºå®Œæˆï¼‰ï¼ŒOUTPUT å¯èƒ½å·²ç¶“å®Œæˆ
                # æ­¤æ™‚ä¸æœƒå†è§¸ç™¼ OUTPUT_LAYER_COMPLETE äº‹ä»¶è™•ç†ï¼Œå°è‡´æç¤ºæ°¸é ä¸æœƒç”Ÿæˆ
                debug_log(2, f"[LLM] ç«‹å³ç”Ÿæˆäº’å‹•æ­¥é©Ÿæç¤º: {workflow_type}")
                self._process_interactive_step_prompt(
                    session_id,
                    workflow_type,
                    step_result,
                    review_data,
                    next_step_info
                )
                # ğŸ”§ ä¿®æ­£ï¼šå¾å…©å€‹éšŠåˆ—ä¸­ç§»é™¤ï¼ˆå·²ç¶“è™•ç†ï¼‰
                # âš ï¸ é‡è¦ï¼šå¿…é ˆå¾ _pending_workflow_events ä¸­ç§»é™¤ï¼Œå¦å‰‡æœƒåœ¨ handle() ä¸­è¢«å†æ¬¡è™•ç†
                # å°è‡´ LLM ç”Ÿæˆæ±ºç­–ä¸¦éŒ¯èª¤åœ°èª¿ç”¨ approve_step
                self._pending_interactive_prompts.pop()
                # å¾ _pending_workflow_events ä¸­æ‰¾åˆ°ä¸¦ç§»é™¤å°æ‡‰çš„äº‹ä»¶
                if hasattr(self, '_pending_workflow_events') and self._pending_workflow_events:
                    # æ‰¾åˆ°åŒ¹é…çš„äº‹ä»¶ï¼ˆsession_id ç›¸åŒï¼‰
                    self._pending_workflow_events = [
                        e for e in self._pending_workflow_events 
                        if e.get('session_id') != session_id
                    ]
                    debug_log(2, f"[LLM] å·²å¾å¾…è™•ç†éšŠåˆ—ä¸­ç§»é™¤äº’å‹•æ­¥é©Ÿäº‹ä»¶: {session_id}")
            else:
                # ğŸ”§ å…¶ä»–æƒ…æ³ï¼šç­‰å¾…ä¸‹æ¬¡ handle() èª¿ç”¨
                debug_log(2, f"[LLM] å·¥ä½œæµäº‹ä»¶å·²æº–å‚™å¥½ï¼Œç­‰å¾…ä¸‹æ¬¡ handle() èª¿ç”¨ç”Ÿæˆå›æ‡‰")
            
        except Exception as e:
            import traceback
            error_log(f"[LLM] è™•ç†å·¥ä½œæµæ­¥é©Ÿå®Œæˆäº‹ä»¶å¤±æ•—: {e}")
            error_log(f"[LLM] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
    
    def _handle_llm_processing_request(self, session_id: str, workflow_type: str, llm_request_data: dict):
        """
        è™•ç†å·¥ä½œæµä¸­çš„ LLM_PROCESSING è«‹æ±‚
        
        ç•¶å·¥ä½œæµæ­¥é©Ÿé¡å‹ç‚º STEP_TYPE_LLM_PROCESSING æ™‚ï¼Œæœƒèª¿ç”¨æ­¤æ–¹æ³•ä¾†ï¼š
        1. æå– LLM è«‹æ±‚æ•¸æ“šï¼ˆprompt, output_keyï¼‰
        2. ç”Ÿæˆ LLM å›æ‡‰
        3. å°‡çµæœå¯«å…¥å·¥ä½œæµæœƒè©±æ•¸æ“š
        4. è§¸ç™¼å·¥ä½œæµç¹¼çºŒåŸ·è¡Œ
        
        Args:
            session_id: å·¥ä½œæµæœƒè©±ID
            workflow_type: å·¥ä½œæµé¡å‹
            llm_request_data: åŒ…å« prompt, output_data_key ç­‰çš„è«‹æ±‚æ•¸æ“š
        """
        try:
            debug_log(2, f"[LLM] é–‹å§‹è™•ç† LLM_PROCESSING è«‹æ±‚: {workflow_type}")
            
            # æå–è«‹æ±‚æ•¸æ“š
            prompt = llm_request_data.get('prompt')
            output_key = llm_request_data.get('output_data_key')
            task_description = llm_request_data.get('task_description', '')
            
            if not prompt:
                error_log(f"[LLM] LLM_PROCESSING è«‹æ±‚ç¼ºå°‘ prompt")
                return
            
            if not output_key:
                error_log(f"[LLM] LLM_PROCESSING è«‹æ±‚ç¼ºå°‘ output_data_key")
                return
            
            debug_log(3, f"[LLM] ä»»å‹™æè¿°: {task_description}")
            debug_log(3, f"[LLM] è¼¸å‡ºéµ: {output_key}")
            debug_log(3, f"[LLM] Prompt é•·åº¦: {len(prompt)} å­—ç¬¦")
            
            # ğŸ”§ ä½¿ç”¨ internal æ¨¡å¼ç”Ÿæˆ LLM å›æ‡‰ï¼ˆç¯€çœ tokenï¼‰
            # internal æ¨¡å¼ï¼šä¸ä½¿ç”¨ UEP ç³»çµ±æç¤ºè©ã€ä¸ä½¿ç”¨å¿«å–ã€ä¸ä½¿ç”¨ MCP å·¥å…·
            debug_log(2, f"[LLM] æ­£åœ¨èª¿ç”¨ Gemini APIï¼ˆinternal æ¨¡å¼ï¼‰...")
            
            # æ§‹å»ºç°¡æ½”çš„ç³»çµ±æç¤ºè©ï¼ˆåƒ…é‡å°å·¥ä½œæµä»»å‹™ï¼‰
            workflow_system_prompt = (
                "You are a helpful assistant processing workflow tasks. "
                "Provide clear, concise responses based on the given instructions. "
                "Follow the format requirements strictly. And ALWAYS respond in English"
            )
            
            response_data = self.model.query(
                prompt, 
                mode="internal",  # ğŸ”§ ä½¿ç”¨ internal æ¨¡å¼
                cached_content=None,  # ä¸ä½¿ç”¨å¿«å–
                tools=None,  # ä¸ä½¿ç”¨ MCP å·¥å…·
                system_instruction=workflow_system_prompt  # ä½¿ç”¨ç°¡æ½”çš„ç³»çµ±æç¤ºè©
            )
            
            if not response_data or 'text' not in response_data:
                error_log(f"[LLM] Gemini API å›æ‡‰ç„¡æ•ˆ: {response_data}")
                return
            
            llm_result = response_data['text']
            debug_log(2, f"[LLM] LLM å›æ‡‰å·²ç”Ÿæˆ (é•·åº¦: {len(llm_result)})")
            debug_log(3, f"[LLM] å›æ‡‰å…§å®¹é è¦½: {llm_result[:200]}...")
            
            # å¯«å…¥å·¥ä½œæµæœƒè©±æ•¸æ“š
            from core.sessions.session_manager import session_manager
            workflow_session = session_manager.get_workflow_session(session_id)
            
            if not workflow_session:
                error_log(f"[LLM] æ‰¾ä¸åˆ°å·¥ä½œæµæœƒè©±: {session_id}")
                return
            
            workflow_session.add_data(output_key, llm_result)
            debug_log(2, f"[LLM] å·²å°‡ LLM çµæœå¯«å…¥æœƒè©±æ•¸æ“šéµ: {output_key}")
            
            # ğŸ”§ è§¸ç™¼å·¥ä½œæµç¹¼çºŒåŸ·è¡Œ
            # ä½¿ç”¨ provide_workflow_input MCP å·¥å…·ä¾†æ¨é€²å·¥ä½œæµ
            debug_log(2, f"[LLM] èª¿ç”¨ provide_workflow_input æ¨é€²å·¥ä½œæµ...")
            
            import asyncio
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # èª¿ç”¨ MCP å·¥å…·æ¨é€²å·¥ä½œæµï¼ˆä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œç‚ºè¼¸å…¥ï¼Œå› ç‚ºLLMçµæœå·²å¯«å…¥æœƒè©±ï¼‰
            continue_result = loop.run_until_complete(
                self.mcp_client.call_tool(
                    "provide_workflow_input",
                    {
                        "session_id": session_id,
                        "user_input": "",  # ç©ºè¼¸å…¥ï¼Œå› ç‚ºæ•¸æ“šå·²åœ¨æœƒè©±ä¸­
                        "use_fallback": True  # ä½¿ç”¨ fallback æ¨¡å¼ç›´æ¥æ¨é€²
                    }
                )
            )
            
            debug_log(2, f"[LLM] å·¥ä½œæµæ¨é€²çµæœ: {continue_result.get('status')}")
            debug_log(2, f"[LLM] LLM_PROCESSING è«‹æ±‚è™•ç†å®Œæˆï¼Œå·¥ä½œæµå·²ç¹¼çºŒ")
            
        except Exception as e:
            import traceback
            error_log(f"[LLM] è™•ç† LLM_PROCESSING è«‹æ±‚å¤±æ•—: {e}")
            error_log(f"[LLM] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
    
    def _handle_workflow_failed(self, event):
        """
        âœ… è™•ç†å·¥ä½œæµå¤±æ•—äº‹ä»¶
        
        ç•¶å·¥ä½œæµåŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤æ™‚ï¼š
        1. ç”Ÿæˆè‡ªç„¶èªè¨€çš„éŒ¯èª¤èªªæ˜
        2. èª¿ç”¨ cancel_workflow MCP å·¥å…·å„ªé›…çµ‚æ­¢å·¥ä½œæµ
        3. é€šçŸ¥ä½¿ç”¨è€…éŒ¯èª¤æƒ…æ³
        
        Args:
            event: Event object containing error data
        """
        try:
            debug_log(2, f"[LLM] æ”¶åˆ°å·¥ä½œæµå¤±æ•—äº‹ä»¶: {event.event_id}")
            
            data = event.data
            session_id = data.get("session_id")
            workflow_type = data.get("workflow_type")
            error_message = data.get("error_message")
            current_step = data.get("current_step")
            
            error_log(f"[LLM] å·¥ä½œæµå¤±æ•—: {workflow_type} ({session_id}) - {error_message}")
            
            # âœ… å°‡éŒ¯èª¤äº‹ä»¶åŠ å…¥å¾…è™•ç†éšŠåˆ—
            if not hasattr(self, '_pending_workflow_events'):
                self._pending_workflow_events = []
            
            self._pending_workflow_events.append({
                "type": "workflow_failed",
                "session_id": session_id,
                "workflow_type": workflow_type,
                "error_message": error_message,
                "current_step": current_step,
                "timestamp": time.time()
            })
            
            info_log(f"[LLM] å·¥ä½œæµéŒ¯èª¤äº‹ä»¶å·²åŠ å…¥éšŠåˆ—: {workflow_type}")
            
        except Exception as e:
            error_log(f"[LLM] è™•ç†å·¥ä½œæµå¤±æ•—äº‹ä»¶éŒ¯èª¤: {e}")
    
    def _handle_output_complete(self, event):
        """
        è™•ç†è¼¸å‡ºå±¤å®Œæˆäº‹ä»¶
        
        ç•¶ TTS è¼¸å‡ºå®Œæˆå¾Œï¼Œæª¢æŸ¥æ˜¯å¦æœ‰å¾…è™•ç†çš„äº’å‹•æ­¥é©Ÿæç¤ºéœ€è¦ç”Ÿæˆ
        é€™ç¢ºä¿äº’å‹•æ­¥é©Ÿæç¤ºåœ¨æ­£ç¢ºçš„æ™‚åºç”Ÿæˆï¼ˆåœ¨ç•¶å‰ cycle çš„è¼¸å‡ºä¹‹å¾Œï¼‰
        
        Args:
            event: OUTPUT_LAYER_COMPLETE äº‹ä»¶
        """
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰å¾…è™•ç†çš„äº’å‹•æ­¥é©Ÿæç¤º
            if not hasattr(self, '_pending_interactive_prompts') or not self._pending_interactive_prompts:
                return
            
            current_gs = self._get_current_gs_id()
            
            # è™•ç†æ‰€æœ‰å¾…è™•ç†çš„æç¤ºï¼ˆåªè™•ç†ç•¶å‰ cycle çš„ï¼‰
            prompts_to_process = []
            remaining_prompts = []
            
            for prompt_info in self._pending_interactive_prompts:
                if prompt_info['current_cycle_session'] == current_gs:
                    prompts_to_process.append(prompt_info)
                else:
                    remaining_prompts.append(prompt_info)
            
            self._pending_interactive_prompts = remaining_prompts
            
            # è™•ç†æ¯å€‹å¾…è™•ç†çš„æç¤º
            for prompt_info in prompts_to_process:
                debug_log(2, f"[LLM] OUTPUT å®Œæˆå¾Œè™•ç†äº’å‹•æ­¥é©Ÿæç¤º: {prompt_info['workflow_type']}")
                self._process_interactive_step_prompt(
                    prompt_info['session_id'],
                    prompt_info['workflow_type'],
                    prompt_info['step_result'],
                    prompt_info['review_data'],
                    prompt_info['next_step_info']
                )
        
        except Exception as e:
            import traceback
            error_log(f"[LLM] è™•ç†è¼¸å‡ºå®Œæˆäº‹ä»¶å¤±æ•—: {e}")
            error_log(f"[LLM] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
    
    def _submit_workflow_review_request(self, session_id: str, workflow_type: str, is_complete: bool):
        """
        æäº¤å·¥ä½œæµå¯©æ ¸è«‹æ±‚åˆ° ModuleCoordinator
        
        é€šé ModuleCoordinator æäº¤ä¸€å€‹å…§éƒ¨è™•ç†è«‹æ±‚ï¼Œè§¸ç™¼æ–°çš„ PROCESSING â†’ OUTPUT å¾ªç’°ï¼Œ
        è®“ LLM ç”Ÿæˆå·¥ä½œæµé€²åº¦/å®Œæˆå›æ‡‰ä¸¦é€šé TTS æ’­æ”¾
        
        Args:
            session_id: å·¥ä½œæµæœƒè©± ID
            workflow_type: å·¥ä½œæµé¡å‹
            is_complete: æ˜¯å¦ç‚ºå·¥ä½œæµå®Œæˆäº‹ä»¶
        """
        try:
            from core.module_coordinator import module_coordinator
            from core.sessions.session_manager import unified_session_manager
            
            # ç²å–ç•¶å‰æ´»èºçš„ GS
            all_sessions = unified_session_manager.get_all_active_session_ids()
            gs_id = all_sessions.get('general_session_id')
            
            debug_log(3, f"[LLM] æŸ¥æ‰¾ GS: all_sessions={all_sessions}, gs_id={gs_id}")
            
            if not gs_id:
                error_log(f"[LLM] ç„¡æ³•æ‰¾åˆ°æ´»èº GSï¼Œç„¡æ³•è§¸ç™¼å¯©æ ¸å¾ªç’°")
                # å¦‚æœæ²’æœ‰ GSï¼Œç›´æ¥æ‰¹å‡†æ­¥é©Ÿ
                if not is_complete:
                    self._approve_workflow_step(session_id, None)
                return
            
            # æ§‹å»ºå…§éƒ¨è™•ç†è«‹æ±‚
            # é€™å€‹è«‹æ±‚æœƒè¢«è·¯ç”±åˆ° LLMï¼ŒLLM æœƒçœ‹åˆ° _pending_workflow_events ä¸¦è™•ç†
            internal_request = {
                "session_id": gs_id,
                "cycle_index": getattr(module_coordinator, 'current_cycle_index', 0) + 1,
                "layer": "PROCESSING",
                "input_text": f"[WORKFLOW_EVENT] {workflow_type} - {'completed' if is_complete else 'step_completed'}",
                "metadata": {
                    "workflow_review": True,
                    "workflow_session_id": session_id,
                    "workflow_type": workflow_type,
                    "is_complete": is_complete
                }
            }
            
            debug_log(2, f"[LLM] ç”Ÿæˆå·¥ä½œæµå¯©æ ¸å›æ‡‰: {gs_id}")
            
            # ğŸ”§ ç”Ÿæˆå¯©æ ¸å›æ‡‰æ–‡æœ¬
            response_text = self._generate_workflow_review_text(is_complete)
            
            if response_text:
                # é€šé ModuleCoordinator æäº¤è™•ç†å±¤è«‹æ±‚
                # é€™æœƒè§¸ç™¼: Router â†’ TTS â†’ OUTPUT_LAYER_COMPLETE
                completion_data = {
                    "session_id": gs_id,
                    "cycle_index": internal_request.get("cycle_index", 0),
                    "layer": "PROCESSING",
                    "response": response_text,
                    "source_module": "llm",
                    "llm_output": {
                        "text": response_text,
                        "success": True,
                        "metadata": {
                            "workflow_review": True,
                            "workflow_session_id": session_id,
                            "session_control": {'action': 'end_session'} if is_complete else None
                        }
                    },
                    "timestamp": time.time(),
                    "completion_type": "processing_layer_finished",
                    "success": True
                }
                
                # æäº¤åˆ° ModuleCoordinator
                from core.event_bus import event_bus, SystemEvent
                event_bus.publish(
                    event_type=SystemEvent.PROCESSING_LAYER_COMPLETE,
                    data=completion_data,
                    source="llm"
                )
                
                debug_log(2, f"[LLM] å·²ç™¼å¸ƒå·¥ä½œæµå¯©æ ¸å›æ‡‰äº‹ä»¶")
            
            # è™•ç†å®Œæˆå¾Œï¼Œæ‰¹å‡†å·¥ä½œæµæ­¥é©Ÿï¼ˆå¦‚æœä¸æ˜¯å®Œæˆäº‹ä»¶ï¼‰
            if not is_complete:
                self._approve_workflow_step(session_id, None)
            
        except Exception as e:
            error_log(f"[LLM] æäº¤å·¥ä½œæµå¯©æ ¸è«‹æ±‚å¤±æ•—: {e}")
            import traceback
            debug_log(1, f"[LLM] éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
            # å¤±æ•—æ™‚ç›´æ¥æ‰¹å‡†æ­¥é©Ÿ
            if not is_complete:
                self._approve_workflow_step(session_id, None)
    
    def _generate_workflow_review_text(self, is_complete: bool) -> Optional[str]:
        """
        ç”Ÿæˆå·¥ä½œæµå¯©æ ¸å›æ‡‰æ–‡æœ¬
        
        å¾å¾…è™•ç†äº‹ä»¶éšŠåˆ—ä¸­å–å‡ºäº‹ä»¶ï¼Œç”Ÿæˆé©ç•¶çš„å¯©æ ¸å›æ‡‰æ–‡æœ¬
        
        Args:
            is_complete: æ˜¯å¦ç‚ºå·¥ä½œæµå®Œæˆäº‹ä»¶
            
        Returns:
            å¯©æ ¸å›æ‡‰æ–‡æœ¬
        """
        try:
            if not hasattr(self, '_pending_workflow_events') or not self._pending_workflow_events:
                return None
            
            # å–å‡ºç¬¬ä¸€å€‹å¾…è™•ç†äº‹ä»¶
            event = self._pending_workflow_events.pop(0)
            
            workflow_type = event.get('workflow_type', 'unknown')
            step_result = event.get('step_result', {})
            review_data = event.get('review_data', {})
            
            # æ ¹æ“šäº‹ä»¶é¡å‹ç”Ÿæˆå›æ‡‰
            if is_complete:
                # å·¥ä½œæµå®Œæˆï¼šç”Ÿæˆå®Œæˆå›æ‡‰
                if workflow_type == 'drop_and_read' and review_data:
                    file_name = review_data.get('file_name', 'æª”æ¡ˆ')
                    content = review_data.get('full_content', '')
                    content_length = review_data.get('content_length', 0)
                    
                    # ğŸ”§ ä½¿ç”¨ LLM æ™ºèƒ½è™•ç†æª”æ¡ˆå…§å®¹
                    if content_length > 500:
                        # å…§å®¹éé•·ï¼šå»ºè­°ä½¿ç”¨æ‘˜è¦åŠŸèƒ½ï¼Œåªæä¾›å‰100å­—ç¬¦é è¦½
                        preview = content[:100] if content else ""
                        
                        prompt = (
                            f"You are U.E.P., an interdimensional being. You've just read a file named '{file_name}' "
                            f"which contains {content_length} characters.\n\n"
                            f"Here's a brief preview of the beginning:\n{preview}...\n\n"
                            f"The content is quite long. Please respond to the user in English:\n"
                            f"1. Acknowledge that you've read the file\n"
                            f"2. Mention the file is long ({content_length} characters)\n"
                            f"3. Provide a very brief description of what you see in the preview (in English, even if the content is in another language)\n"
                            f"4. Suggest using the summary feature for detailed analysis\n\n"
                            f"Keep your response natural, friendly, and concise (2-3 sentences max)."
                        )
                    else:
                        # å…§å®¹é©ä¸­ï¼šç”¨è‹±æ–‡æè¿°/æ‘˜è¦å…§å®¹
                        prompt = (
                            f"You are U.E.P., an interdimensional being. You've just read a file named '{file_name}'.\n\n"
                            f"File content:\n{content}\n\n"
                            f"Please respond to the user in English:\n"
                            f"1. Acknowledge that you've read the file\n"
                            f"2. Provide a brief, natural description or summary of the content IN ENGLISH\n"
                            f"   - If the content is in another language (e.g., Chinese, Japanese), translate or explain it in English\n"
                            f"   - Focus on the main topic and key points\n"
                            f"3. Keep it conversational and concise (3-4 sentences max)\n\n"
                            f"IMPORTANT: Always respond in English, regardless of the original language of the content."
                        )
                    
                    # èª¿ç”¨ LLM ç”Ÿæˆæ™ºèƒ½å›æ‡‰
                    try:
                        response = self.model.query(prompt, mode="internal")
                        return response.get("text", f"I've read the file {file_name}.")
                    except Exception as e:
                        error_log(f"[LLM] ç”Ÿæˆæª”æ¡ˆå…§å®¹å›æ‡‰å¤±æ•—: {e}")
                        # é™ç´šæ–¹æ¡ˆ
                        if content_length > 500:
                            return f"I've read the file {file_name} ({content_length} characters). The content is quite long. I recommend using the summary feature."
                        else:
                            return f"I've read the file {file_name}. The file contains approximately {content_length} characters of content."
                
                return f"Workflow {workflow_type} has been completed successfully."
            else:
                # ä¸­é–“æ­¥é©Ÿï¼šç”Ÿæˆé€²åº¦å›æ‡‰
                if workflow_type == 'drop_and_read':
                    if review_data and 'file_path' in review_data:
                        return "å¥½çš„ï¼Œæˆ‘å·²ç¶“æ”¶åˆ°æª”æ¡ˆäº†ï¼Œæ­£åœ¨è®€å–å…§å®¹..."
                
                return f"å·¥ä½œæµ {workflow_type} æ­£åœ¨é€²è¡Œä¸­ï¼Œè«‹ç¨å€™..."
                
        except Exception as e:
            error_log(f"[LLM] ç”Ÿæˆå·¥ä½œæµå¯©æ ¸æ–‡æœ¬å¤±æ•—: {e}")
            return None
    
    def _get_pending_workflow_context(self) -> Optional[Dict[str, Any]]:
        """
        ç²å–å¾…è™•ç†çš„å·¥ä½œæµä¸Šä¸‹æ–‡æ•¸æ“š
        
        å¾å¾…è™•ç†äº‹ä»¶éšŠåˆ—ä¸­å–å‡ºå·¥ä½œæµäº‹ä»¶ï¼Œæ§‹å»ºç‚º workflow_context
        ä¾› handle() æ–¹æ³•ä½¿ç”¨
        
        Returns:
            å·¥ä½œæµä¸Šä¸‹æ–‡å­—å…¸ï¼Œå¦‚æœæ²’æœ‰å¾…è™•ç†äº‹ä»¶å‰‡è¿”å› None
        """
        try:
            if not hasattr(self, '_pending_workflow_events') or not self._pending_workflow_events:
                return None
            
            # å–å‡ºç¬¬ä¸€å€‹å¾…è™•ç†äº‹ä»¶
            event = self._pending_workflow_events.pop(0)
            
            event_type = event.get('type', 'workflow_step_completed')
            
            # ğŸ†• è™•ç†å·¥ä½œæµéŒ¯èª¤äº‹ä»¶
            if event_type == 'workflow_failed':
                workflow_context = {
                    'type': 'workflow_error',
                    'workflow_session_id': event.get('session_id'),
                    'workflow_type': event.get('workflow_type'),
                    'error_message': event.get('error_message'),
                    'current_step': event.get('current_step')
                }
                debug_log(2, f"[LLM] æ§‹å»ºå·¥ä½œæµéŒ¯èª¤ä¸Šä¸‹æ–‡: workflow={workflow_context['workflow_type']}, "
                            f"error={workflow_context['error_message']}")
            else:
                # æ§‹å»ºå·¥ä½œæµä¸Šä¸‹æ–‡
                workflow_context = {
                    'type': 'workflow_step_response',
                    'workflow_session_id': event.get('session_id'),
                    'workflow_type': event.get('workflow_type'),
                    'is_complete': event.get('is_complete', False),
                    'should_end_session': event.get('should_end_session', False),
                    'step_result': event.get('step_result', {}),
                    'review_data': event.get('review_data', {}),
                    'next_step_info': event.get('next_step_info')  # ğŸ†• åŒ…å«ä¸‹ä¸€æ­¥è³‡è¨Š
                }
                
                debug_log(2, f"[LLM] æ§‹å»ºå·¥ä½œæµä¸Šä¸‹æ–‡: type={workflow_context['type']}, "
                            f"workflow={workflow_context['workflow_type']}, "
                            f"complete={workflow_context['is_complete']}")
            
            return workflow_context
            
        except Exception as e:
            error_log(f"[LLM] ç²å–å·¥ä½œæµä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return None
    
    def _approve_workflow_step(self, session_id: str, modifications: Optional[Dict] = None):
        """æ‰¹å‡†å·¥ä½œæµæ­¥é©Ÿä¸¦ç¹¼çºŒ"""
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        loop.run_until_complete(
            self.mcp_client.call_tool("approve_step", {
                "session_id": session_id,
                "modifications": modifications or {}
            })
        )
        debug_log(2, f"[LLM] å·²æ‰¹å‡†å·¥ä½œæµæ­¥é©Ÿ: {session_id}")
        # æ­¥é©Ÿæ‰¹å‡†å¾Œæœƒè‡ªå‹•åŸ·è¡Œï¼Œä¸éœ€è¦é¡å¤–çš„äº‹ä»¶é€šçŸ¥
    
    def _modify_workflow_step(self, session_id: str, modifications: Dict[str, Any]):
        """ä¿®æ”¹å·¥ä½œæµæ­¥é©Ÿä¸¦é‡è©¦"""
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        loop.run_until_complete(
            self.mcp_client.call_tool("modify_step", {
                "session_id": session_id,
                "modifications": modifications
            })
        )
        debug_log(2, f"[LLM] å·²ä¿®æ”¹å·¥ä½œæµæ­¥é©Ÿ: {session_id}")
    
    def _cancel_workflow(self, session_id: str, reason: str):
        """å–æ¶ˆå·¥ä½œæµ"""
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        loop.run_until_complete(
            self.mcp_client.call_tool("cancel_workflow", {
                "session_id": session_id,
                "reason": reason
            })
        )
        debug_log(2, f"[LLM] å·²å–æ¶ˆå·¥ä½œæµ: {session_id}")
    
    def _handle_workflow_completion(self, session_id: str, workflow_type: str, 
                                    step_result: Dict[str, Any], review_data: Dict[str, Any],
                                    should_end_session: bool):
        """
        ğŸ†• è™•ç†å·¥ä½œæµå®Œæˆäº‹ä»¶
        
        ç•¶å·¥ä½œæµçš„æœ€å¾Œä¸€æ­¥å®Œæˆæ™‚ï¼š
        1. æå–å·¥ä½œæµçµæœæ•¸æ“š
        2. ç”Ÿæˆç”¨æˆ¶å›æ‡‰ï¼ˆå‘Šè¨´ç”¨æˆ¶çµæœï¼‰
        3. çµæŸæœƒè©±ï¼ˆå¦‚æœéœ€è¦ï¼‰
        
        Args:
            session_id: å·¥ä½œæµæœƒè©± ID
            workflow_type: å·¥ä½œæµé¡å‹
            step_result: æœ€å¾Œä¸€æ­¥çš„çµæœ
            review_data: LLM å¯©æ ¸æ•¸æ“šï¼ˆåŒ…å«æª”æ¡ˆå…§å®¹ç­‰ï¼‰
            should_end_session: æ˜¯å¦æ‡‰è©²çµæŸæœƒè©±
        """
        try:
            info_log(f"[LLM] è™•ç†å·¥ä½œæµå®Œæˆ: {workflow_type} ({session_id})")
            
            # æå–æª”æ¡ˆä¿¡æ¯
            file_name = review_data.get('file_name', 'unknown file')
            content = review_data.get('full_content', '')
            content_length = review_data.get('content_length', 0)
            
            # æ§‹å»º prompt è®“ LLM ç”Ÿæˆç”¨æˆ¶å›æ‡‰
            prompt = (
                f"A workflow has been completed successfully.\n\n"
                f"Workflow: {workflow_type}\n"
                f"File: {file_name}\n"
                f"Content Length: {content_length} characters\n\n"
                f"File Content:\n{content[:1000]}{'...' if len(content) > 1000 else ''}\n\n"
                f"Please generate a friendly response to the user in Traditional Chinese, "
                f"summarizing what was done and providing key insights from the file content. "
                f"Keep it concise and helpful."
            )
            
            # èª¿ç”¨ LLM ç”Ÿæˆå›æ‡‰
            # âš ï¸ é—œéµï¼šå·¥ä½œæµå®Œæˆå›æ‡‰æ™‚ä¸æä¾› MCP å·¥å…·ä¸”ä¸ä½¿ç”¨å¿«å–ï¼ˆé¿å… LLM å¾å¿«å–ä¸­èª¿ç”¨ approve_step ç­‰å·¥å…·ï¼‰
            debug_log(2, f"[LLM] ç”Ÿæˆå·¥ä½œæµå®Œæˆå›æ‡‰ï¼ˆä¸æä¾› MCP å·¥å…·ï¼Œä¸ä½¿ç”¨å¿«å–ï¼‰")
            response = self.model.query(prompt, mode="internal", tools=None, cached_content=None)
            
            if "text" in response:
                user_response = response["text"]
            else:
                user_response = f"å·²æˆåŠŸè®€å–æª”æ¡ˆ {file_name}ï¼Œå…§å®¹é•·åº¦: {content_length} å­—ç¬¦ã€‚"
            
            info_log(f"[LLM] å·¥ä½œæµå®Œæˆå›æ‡‰: {user_response[:100]}...")
            
            # ğŸ†• å°‡å›æ‡‰ç™¼é€åˆ°è™•ç†å±¤å®Œæˆäº‹ä»¶ï¼Œè§¸ç™¼ TTS è¼¸å‡º
            from core.event_bus import event_bus, SystemEvent
            import time
            
            # æº–å‚™ LLM è¼¸å‡ºæ•¸æ“š
            llm_output = {
                "text": user_response,
                "sys_action": None,
                "status_updates": None,
                "learning_data": None,
                "conversation_entry": None,
                "session_state": None,
                "memory_observation": None,
                "memory_summary": None,
                "emotion": "neutral",
                "confidence": 0.9,
                "processing_time": 0.0,
                "success": True,
                "error": None,
                "tokens_used": 0,
                "metadata": {
                    "mode": "WORK",
                    "workflow_type": workflow_type,
                    "workflow_session_id": session_id,
                    # ğŸ†• Task 5: çµæŸæœƒè©±æ§åˆ¶
                    "session_control": {"action": "end_session"} if should_end_session else None
                },
                "mood": "neutral",
                "status": "ok"
            }
            
            # ç™¼å¸ƒè™•ç†å±¤å®Œæˆäº‹ä»¶ï¼Œè§¸ç™¼ TTS è¼¸å‡º
            event_bus.publish(
                SystemEvent.PROCESSING_LAYER_COMPLETE,
                {
                    "session_id": "workflow_completion",  # è‡¨æ™‚æœƒè©± ID
                    "cycle_index": 0,
                    "layer": "PROCESSING",
                    "response": user_response,
                    "source_module": "llm",
                    "llm_output": llm_output,
                    "timestamp": time.time(),
                    "completion_type": "processing_layer_finished",
                    "mode": "WORK",
                    "success": True
                },
                source="llm"
            )
            
            info_log(f"[LLM] å·²ç™¼å¸ƒå·¥ä½œæµå®Œæˆå›æ‡‰åˆ°è™•ç†å±¤" + 
                    (f"ï¼Œå°‡çµæŸæœƒè©±" if should_end_session else ""))
            
            # âœ… æ¸…é™¤ workflow_processing æ¨™èªŒï¼Œå…è¨±ä¸‹ä¸€æ¬¡è¼¸å…¥å±¤é‹è¡Œ
            from core.working_context import working_context_manager
            working_context_manager.set_skip_input_layer(False, reason="workflow_completion_processed")
            debug_log(2, "[LLM] å·²æ¸…é™¤ workflow_processing æ¨™èªŒ")
            
            # ğŸ”§ æ¸…ç†è¿½è¹¤æ¨™è¨˜ï¼Œé˜²æ­¢å…§å­˜æ´©æ¼
            if session_id in self._processed_workflow_completions:
                self._processed_workflow_completions.discard(session_id)
                debug_log(2, f"[LLM] å·²ç§»é™¤å·¥ä½œæµå®Œæˆè¿½è¹¤: {session_id}")
            
            # ğŸ”§ æ¸…ç†è©²å·¥ä½œæµçš„æ‰€æœ‰ LLM_PROCESSING æ­¥é©Ÿæ¨™è¨˜
            if hasattr(self, '_processed_llm_steps'):
                steps_to_remove = {key for key in self._processed_llm_steps if key.startswith(f"{session_id}:")}
                for step_key in steps_to_remove:
                    self._processed_llm_steps.discard(step_key)
                if steps_to_remove:
                    debug_log(2, f"[LLM] å·²æ¸…ç† {len(steps_to_remove)} å€‹ LLM_PROCESSING æ­¥é©Ÿæ¨™è¨˜")
            
        except Exception as e:
            error_log(f"[LLM] è™•ç†å·¥ä½œæµå®Œæˆå¤±æ•—: {e}")
    
    def set_mcp_server(self, mcp_server):
        """
        è¨­ç½® MCP Server å¯¦ä¾‹
        
        Args:
            mcp_server: SYS æ¨¡çµ„çš„ MCP Server å¯¦ä¾‹
        """
        self.mcp_client.set_mcp_server(mcp_server)
        info_log("[LLM] MCP Server å·²è¨­ç½®ï¼ŒMCP å·¥å…·åŠŸèƒ½å·²å•Ÿç”¨")
        debug_log(2, f"[LLM] å¯ç”¨çš„ MCP å·¥å…·: {len(self.mcp_client.get_tools_for_llm())} å€‹")
    
    def get_mcp_tools_for_llm(self) -> List[Dict[str, Any]]:
        """
        ç²å– MCP å·¥å…·è¦ç¯„ä¾› LLM function calling ä½¿ç”¨
        
        Returns:
            å·¥å…·è¦ç¯„åˆ—è¡¨
        """
        return self.mcp_client.get_tools_for_llm()
    
    async def handle_mcp_tool_call(self, tool_name: str, tool_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        è™•ç† LLM çš„ MCP å·¥å…·å‘¼å«
        
        Args:
            tool_name: å·¥å…·åç¨±
            tool_params: å·¥å…·åƒæ•¸
            
        Returns:
            å·¥å…·åŸ·è¡Œçµæœ
        """
        debug_log(2, f"[LLM] è™•ç† MCP å·¥å…·å‘¼å«: {tool_name}")
        return await self.mcp_client.call_tool(tool_name, tool_params)
        
    def handle(self, data: dict) -> dict:
        """ä¸»è¦è™•ç†æ–¹æ³• - é‡æ§‹ç‰ˆæœ¬ï¼Œæ”¯æ´æ–°çš„ CHAT/WORK æ¨¡å¼å’Œæ–° Router æ•´åˆ"""
        start_time = time.time()
        
        try:
            # è§£æè¼¸å…¥ç‚ºæ–°æ¶æ§‹
            llm_input = LLMInput(**data)
            info_log(f"[LLM] é–‹å§‹è™•ç†è«‹æ±‚ - æ¨¡å¼: {llm_input.mode}, ç”¨æˆ¶è¼¸å…¥: {llm_input.text[:50]}...")
            debug_log(1, f"[LLM] è™•ç†è¼¸å…¥ - æ¨¡å¼: {llm_input.mode}, ç”¨æˆ¶è¼¸å…¥: {llm_input.text[:100]}...")
            
            # ğŸ”§ åœ¨è™•ç†é–‹å§‹æ™‚ç²å–ä¸¦ä¿å­˜ session_id å’Œ cycle_index
            # é¿å…åœ¨äº‹ä»¶ç™¼å¸ƒæ™‚å‹•æ…‹è®€å–å°è‡´cycleå·²éå¢çš„å•é¡Œ
            self._current_processing_session_id = self._get_current_gs_id()
            # âœ… å„ªå…ˆå¾è¼¸å…¥æ•¸æ“šä¸­è®€å– cycle_indexï¼ˆç”± MC å¾ STATE_ADVANCED æˆ– INPUT_LAYER_COMPLETE å‚³éï¼‰
            # å¦‚æœè¼¸å…¥æ•¸æ“šä¸­æ²’æœ‰ï¼Œæ‰å¾ working_context è®€å–
            if hasattr(llm_input, 'cycle_index') and llm_input.cycle_index is not None:
                self._current_processing_cycle_index = llm_input.cycle_index
                debug_log(3, f"[LLM] å¾è¼¸å…¥æ•¸æ“šè®€å– cycle_index: {self._current_processing_cycle_index}")
            else:
                self._current_processing_cycle_index = self._get_current_cycle_index()
                debug_log(3, f"[LLM] å¾ working_context è®€å– cycle_index: {self._current_processing_cycle_index}")
            debug_log(3, f"[LLM] è¨˜éŒ„è™•ç†ä¸Šä¸‹æ–‡: session={self._current_processing_session_id}, cycle={self._current_processing_cycle_index}")
            
            # æª¢æŸ¥æ˜¯å¦ä¾†è‡ªæ–° Router
            if llm_input.source_layer:
                info_log(f"[LLM] ä¾†è‡ªæ–°Router - ä¾†æºå±¤ç´š: {llm_input.source_layer}")
                debug_log(2, f"[LLM] ä¾†è‡ªæ–°Router - ä¾†æºå±¤ç´š: {llm_input.source_layer}")
                if llm_input.processing_context:
                    debug_log(3, f"[LLM] è™•ç†å±¤ä¸Šä¸‹æ–‡: {llm_input.processing_context}")
            
            # ğŸ”§ æª¢æŸ¥æ˜¯å¦ç‚ºå…§éƒ¨å‘¼å«ï¼ˆç¹éæœƒè©±æª¢æŸ¥å’Œç³»çµ±æç¤ºè©ï¼‰
            is_internal = getattr(llm_input, 'is_internal', False)
            
            if is_internal:
                debug_log(1, "[LLM] å…§éƒ¨å‘¼å«æ¨¡å¼ - ä½¿ç”¨ç°¡æ½”ç³»çµ±æç¤ºè©")
                # å…§éƒ¨å‘¼å«ï¼šä½¿ç”¨ç°¡æ½”æç¤ºè©ï¼Œä¸ä½¿ç”¨å¿«å–æˆ–æœƒè©±æª¢æŸ¥
                try:
                    # å…è¨±è‡ªå®šç¾©ç³»çµ±æç¤ºè©ï¼ˆç”¨æ–¼å·¥ä½œæµï¼‰ï¼Œå¦å‰‡ä½¿ç”¨é»˜èªç°¡æ½”ç‰ˆæœ¬
                    internal_system_prompt = getattr(llm_input, 'system_instruction', None)
                    if not internal_system_prompt:
                        internal_system_prompt = (
                            "You are a helpful assistant. "
                            "Provide clear and concise responses."
                        )
                    
                    response_data = self.model.query(
                        llm_input.text,
                        mode="internal",
                        cached_content=None,  # å…§éƒ¨å‘¼å«ä¸ä½¿ç”¨å¿«å–
                        system_instruction=internal_system_prompt
                    )
                    
                    response_text = response_data.get("content", response_data.get("text", ""))
                    
                    processing_time = time.time() - start_time
                    self.processing_stats["total_requests"] += 1
                    self.processing_stats["total_processing_time"] += processing_time
                    
                    return {
                        "status": "ok",
                        "text": response_text,
                        "mode": "internal",
                        "processing_time": processing_time,
                        "timestamp": time.time()
                    }
                except Exception as e:
                    error_log(f"[LLM] å…§éƒ¨å‘¼å«å¤±æ•—: {e}")
                    return {
                        "status": "error",
                        "message": f"å…§éƒ¨å‘¼å«å¤±æ•—: {str(e)}",
                        "timestamp": time.time()
                    }
            
            # 1. ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹å’Œæœƒè©±ä¿¡æ¯
            current_state = self.state_manager.get_current_state()
            info_log(f"[LLM] ç•¶å‰ç³»çµ±ç‹€æ…‹: {current_state}")
            
            # 1.1 æ›´æ–°å”ä½œç®¡é“ï¼ˆç¢ºä¿èˆ‡ç³»çµ±ç‹€æ…‹åŒæ­¥ï¼‰
            self._update_collaboration_channels(current_state)
            
            status = self._get_current_system_status()
            # ğŸ”§ å¦‚æœæœ‰å·¥ä½œæµæœƒè©±IDï¼Œå‚³éçµ¦ _get_current_session_info
            workflow_session_id = getattr(llm_input, 'workflow_session_id', None)
            self.session_info = self._get_current_session_info(workflow_session_id)
            
            # 1.2 æœƒè©±æ¶æ§‹æª¢æŸ¥ - LLM ä¸æ‡‰è©²åœ¨æ²’æœ‰é©ç•¶æœƒè©±çš„æƒ…æ³ä¸‹é‹ä½œ
            if not self._validate_session_architecture(current_state):
                error_log("[LLM] æœƒè©±æ¶æ§‹é•è¦ - æ‹’çµ•è™•ç†è«‹æ±‚")
                return {
                    "status": "error",
                    "message": "æœƒè©±æ¶æ§‹é•è¦ï¼šLLM éœ€è¦é©ç•¶çš„æœƒè©±ä¸Šä¸‹æ–‡",
                    "error_type": "session_architecture_violation",
                    "timestamp": time.time()
                }
            
            # 2. è™•ç†èº«ä»½ä¸Šä¸‹æ–‡ (å„ªå…ˆä½¿ç”¨ä¾†è‡ªRouterçš„)
            if llm_input.identity_context:
                identity_context = llm_input.identity_context
                debug_log(2, f"[LLM] ä½¿ç”¨Routeræä¾›çš„Identityä¸Šä¸‹æ–‡: {identity_context}")
            else:
                identity_context = self._get_identity_context()
                debug_log(2, f"[LLM] ä½¿ç”¨æœ¬åœ°Identityä¸Šä¸‹æ–‡: {identity_context}")
            
            debug_log(2, f"[LLM] ç³»çµ±ç‹€æ…‹: {current_state}")
            debug_log(2, f"[LLM] StatusManager: {status}")
            debug_log(2, f"[LLM] æœƒè©±ä¿¡æ¯: {self.session_info}")
            
            # 3. è£œå……ç³»çµ±ä¸Šä¸‹æ–‡åˆ°llm_input (æ•´åˆRouteræ•¸æ“š)
            llm_input = self._enrich_with_system_context(
                llm_input, current_state, status, self.session_info, identity_context
            )
            
            # ğŸ”§ æª¢æŸ¥æ˜¯å¦æœ‰å¾…è™•ç†çš„å·¥ä½œæµäº‹ä»¶
            # å¦‚æœæœ‰ï¼Œå°‡å·¥ä½œæµæ•¸æ“šæ³¨å…¥åˆ° llm_input.workflow_context
            pending_workflow = self._get_pending_workflow_context()
            if pending_workflow:
                info_log(f"[LLM] æª¢æ¸¬åˆ°å¾…è™•ç†å·¥ä½œæµäº‹ä»¶: {pending_workflow['workflow_type']}")
                # å°‡å·¥ä½œæµæ•¸æ“šåˆä½µåˆ° workflow_context
                if llm_input.workflow_context:
                    llm_input.workflow_context.update(pending_workflow)
                else:
                    llm_input.workflow_context = pending_workflow
                # ç¢ºä¿é€²å…¥ WORK æ¨¡å¼
                llm_input.mode = LLMMode.WORK
            
            # âœ… æª¢æŸ¥æ˜¯å¦ç‚ºå·¥ä½œæµè¼¸å…¥å ´æ™¯ï¼ˆInteractive Input Stepï¼‰
            # âœ… å„ªå…ˆç´šï¼šå¦‚æœå·¥ä½œæµæ­£åœ¨ç­‰å¾…è¼¸å…¥ï¼Œæ¸…é™¤èˆŠçš„ pending_workflow ä¸¦æ§‹å»ºæ–°çš„ workflow_input_required context
            from core.working_context import working_context_manager
            workflow_waiting_input = working_context_manager.is_workflow_waiting_input()
            
            if workflow_waiting_input and self.session_info and self.session_info.get('session_type') == 'workflow':
                # âœ… å¦‚æœæœ‰èˆŠçš„ pending_workflow æ•¸æ“šï¼Œæ¸…é™¤å®ƒï¼ˆå·¥ä½œæµç¾åœ¨éœ€è¦ç”¨æˆ¶è¼¸å…¥ï¼Œä¸å†æ˜¯å¯©æ ¸å ´æ™¯ï¼‰
                if pending_workflow:
                    debug_log(2, "[LLM] å·¥ä½œæµç­‰å¾…è¼¸å…¥ï¼Œæ¸…é™¤èˆŠçš„ pending_workflow æ•¸æ“š")
                    self._pending_workflow_events.clear()  # æ¸…é™¤å¾…è™•ç†äº‹ä»¶
                    pending_workflow = None
                info_log("[LLM] æª¢æ¸¬åˆ°å·¥ä½œæµè¼¸å…¥å ´æ™¯ - æ§‹å»º workflow_input_required context")
                
                # âœ… å¾ working_context_manager ç²å–å¯¦éš›çš„å·¥ä½œæµè¼¸å…¥ä¸Šä¸‹æ–‡
                saved_context = working_context_manager.get_context_data('workflow_input_context', {})
                workflow_session_id = saved_context.get('workflow_session_id') or self.session_info.get('session_id')
                
                # æ§‹å»º workflow_input_required contextï¼ˆä½¿ç”¨å¯¦éš›å€¼ï¼‰
                workflow_input_context = {
                    'type': 'workflow_input_required',
                    'workflow_session_id': workflow_session_id,
                    'workflow_type': saved_context.get('workflow_type', 'unknown'),
                    'step_id': saved_context.get('step_id', 'input_step'),
                    'step_type': saved_context.get('step_type', 'interactive'),
                    'prompt': saved_context.get('prompt', 'è«‹æä¾›è¼¸å…¥'),
                    'user_input': llm_input.text,  # ç”¨æˆ¶çš„è¼¸å…¥æ–‡æœ¬
                    'is_optional': saved_context.get('optional', False),
                    'fallback_value': ''  # ç©ºå­—ä¸²ä½œç‚º fallback
                }
                
                # åˆä½µåˆ° workflow_context
                if llm_input.workflow_context:
                    llm_input.workflow_context.update(workflow_input_context)
                else:
                    llm_input.workflow_context = workflow_input_context
                
                # ç¢ºä¿é€²å…¥ WORK æ¨¡å¼
                llm_input.mode = LLMMode.WORK
                
                debug_log(2, f"[LLM] workflow_input_context å·²æ§‹å»º: {workflow_input_context}")
            
            # æ ¹æ“šæ¨¡å¼åˆ‡æ›è™•ç†é‚è¼¯
            if llm_input.mode == LLMMode.CHAT:
                output = self._handle_chat_mode(llm_input, status)
            elif llm_input.mode == LLMMode.WORK:
                output = self._handle_work_mode(llm_input, status)
            else:
                # å‘å¾Œå…¼å®¹èˆŠçš„ intent ç³»çµ±
                output = self._handle_legacy_mode(llm_input, status)
            
            # è½‰æ›ç‚ºå­—å…¸æ ¼å¼è¿”å›ï¼ˆä¿æŒèˆ‡èˆŠç³»çµ±çš„å…¼å®¹ï¼‰
            result = output.dict()
            result["status"] = "ok" if output.success else "error"
            
            # âœ¨ å¦‚æœ metadata ä¸­æœ‰ workflow_decisionï¼Œæå–åˆ°é ‚å±¤
            if output.metadata and "workflow_decision" in output.metadata:
                result["workflow_decision"] = output.metadata["workflow_decision"]
            
            # âœ… äº‹ä»¶é©…å‹•ï¼šç™¼å¸ƒè™•ç†å±¤å®Œæˆäº‹ä»¶
            if output.success and result.get("text"):
                self._notify_processing_layer_completion(result)
            
            return result
                
        except Exception as e:
            error_log(f"[LLM] è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                "text": "è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                "processing_time": time.time() - start_time,
                "tokens_used": 0,
                "success": False,
                "error": str(e),
                "confidence": 0.0,
                "metadata": {},
                "status": "error"
            }
    
    def _handle_chat_mode(self, llm_input: "LLMInput", status: Dict[str, Any]) -> "LLMOutput":
        """è™•ç† CHAT æ¨¡å¼ - èˆ‡ MEM å”ä½œçš„æ—¥å¸¸å°è©±"""
        start_time = time.time()
        debug_log(2, "[LLM] è™•ç† CHAT æ¨¡å¼")
        
        try:
            # 1. MEM å”ä½œï¼šæª¢ç´¢ç›¸é—œè¨˜æ†¶ (CHATç‹€æ…‹å°ˆç”¨)
            relevant_memories = []
            if not llm_input.memory_context:  # åªæœ‰åœ¨æ²’æœ‰æä¾›è¨˜æ†¶ä¸Šä¸‹æ–‡æ™‚æ‰æª¢ç´¢
                relevant_memories = self._retrieve_relevant_memory(llm_input.text, max_results=5)
                if relevant_memories:
                    debug_log(2, f"[LLM] æ•´åˆ {len(relevant_memories)} æ¢ç›¸é—œè¨˜æ†¶åˆ°å°è©±ä¸Šä¸‹æ–‡")
                    # å°‡æª¢ç´¢åˆ°çš„è¨˜æ†¶è½‰æ›ç‚ºè¨˜æ†¶ä¸Šä¸‹æ–‡
                    llm_input.memory_context = self._format_memories_for_context(relevant_memories)
            
            # 2. æª¢æŸ¥ Context Cache (åŒ…å«å‹•æ…‹è¨˜æ†¶)
            import hashlib
            base = f"{llm_input.mode}|{self.session_info.get('session_id','')}"
            text_sig = hashlib.sha256(llm_input.text.encode("utf-8")).hexdigest()[:16]
            mem_sig  = hashlib.sha256((llm_input.memory_context or "").encode("utf-8")).hexdigest()[:16]
            cache_key = f"chat:{base}:{text_sig}:{mem_sig}:{len(relevant_memories)}"
            cached_response = self.cache_manager.get_cached_response(cache_key)
            
            if cached_response and not llm_input.ignore_cache:
                debug_log(2, "[LLM] ä½¿ç”¨å¿«å–å›æ‡‰ï¼ˆåŒ…å«è¨˜æ†¶ä¸Šä¸‹æ–‡ï¼‰")
                return cached_response
            
            # æå– intent metadataï¼ˆç”¨æ–¼æª¢æ¸¬é™ç´šçš„ WORK è«‹æ±‚ï¼‰
            intent_metadata = None
            if llm_input.entities and isinstance(llm_input.entities, dict):
                intent_metadata = llm_input.entities.get('intent_metadata')
            elif llm_input.processing_context and isinstance(llm_input.processing_context, dict):
                intent_metadata = llm_input.processing_context.get('intent_metadata')
            
            # 3. æ§‹å»º CHAT æç¤ºï¼ˆæ•´åˆè¨˜æ†¶å’Œé™ç´šè­¦å‘Šï¼‰
            prompt = self.prompt_manager.build_chat_prompt(
                user_input=llm_input.text,
                identity_context=llm_input.identity_context,
                memory_context=llm_input.memory_context,
                conversation_history=getattr(llm_input, 'conversation_history', None),
                is_internal=False,
                relevant_memories=relevant_memories,  # æ–°å¢ï¼šå‚³å…¥æª¢ç´¢åˆ°çš„è¨˜æ†¶
                intent_metadata=intent_metadata  # æ–°å¢ï¼šå‚³å…¥ intent metadata
            )
            
            # 3. ç²å–æˆ–å‰µå»ºç³»çµ±å¿«å–
            cached_content_ids = self._get_system_caches("chat")
            
            # 4. å‘¼å« Gemini API (ä½¿ç”¨å¿«å–)
            response_data = self.model.query(
                prompt, 
                mode="chat",
                cached_content=cached_content_ids.get("persona")
            )
            response_text = response_data.get("text", "")
            
            # === è©³ç´°å›æ‡‰æ—¥èªŒ ===
            info_log(f"[LLM] ğŸ¤– Geminiå›æ‡‰: {response_text}")
            debug_log(1, f"[LLM] ğŸ“Š å›æ‡‰ä¿¡å¿ƒåº¦: {response_data.get('confidence', 'N/A')}")
            
            # è¨˜æ†¶è§€å¯Ÿæ—¥èªŒ
            if response_data.get("memory_observation"):
                debug_log(1, f"[LLM] ğŸ’­ è¨˜æ†¶è§€å¯Ÿ: {response_data['memory_observation']}")
            
            # ç‹€æ…‹æ›´æ–°æ—¥èªŒ
            status_updates = response_data.get("status_updates")
            if status_updates:
                debug_log(1, f"[LLM] ğŸ“ˆ å»ºè­°ç‹€æ…‹æ›´æ–°:")
                for key, value in status_updates.items():
                    if value is not None:
                        debug_log(1, f"[LLM]   {key}: {value:+.2f}" if isinstance(value, (int, float)) else f"[LLM]   {key}: {value}")
            
            # å­¸ç¿’ä¿¡è™Ÿæ—¥èªŒ
            learning_signals = response_data.get("learning_signals")
            if learning_signals:
                debug_log(1, f"[LLM] ğŸ§  å­¸ç¿’ä¿¡è™Ÿ:")
                for signal_type, value in learning_signals.items():
                    if value is not None:
                        debug_log(1, f"[LLM]   {signal_type}: {value:+.2f}")
            
            # æœƒè©±æ§åˆ¶æ—¥èªŒ
            session_control = response_data.get("session_control")
            if session_control:
                debug_log(1, f"[LLM] ğŸ® æœƒè©±æ§åˆ¶å»ºè­°:")
                debug_log(1, f"[LLM]   æ‡‰çµæŸæœƒè©±: {session_control.get('should_end_session', False)}")
                if session_control.get('end_reason'):
                    debug_log(1, f"[LLM]   çµæŸåŸå› : {session_control['end_reason']}")
                if session_control.get('confidence'):
                    debug_log(1, f"[LLM]   ä¿¡å¿ƒåº¦: {session_control['confidence']:.2f}")
            
            # å¿«å–è³‡è¨Šæ—¥èªŒ
            meta = response_data.get("_meta", {})
            if meta.get("cached_input_tokens", 0) > 0:
                debug_log(2, f"[LLM] ğŸ“š å¿«å–å‘½ä¸­: {meta['cached_input_tokens']} tokens")
            debug_log(2, f"[LLM] ğŸ“ ç¸½è¼¸å…¥tokens: {meta.get('total_input_tokens', 0)}")
            
            # è™•ç† StatusManager æ›´æ–°
            if "status_updates" in response_data and response_data["status_updates"]:
                self._process_status_updates(response_data["status_updates"])
            
            # 4. è™•ç†MEMæ¨¡çµ„æ•´åˆ (CHATæ¨¡å¼)
            memory_operations = self._process_chat_memory_operations(
                llm_input, response_data, response_text
            )
            
            # === è¨˜æ†¶æ“ä½œæ—¥èªŒ ===
            if memory_operations:
                info_log(f"[LLM] ğŸ§  è¨˜æ†¶æ“ä½œè™•ç†:")
                for i, op in enumerate(memory_operations):
                    op_type = op.get('operation', 'unknown')
                    content = op.get('content', {})
                    if op_type == 'store':
                        user_text = content.get('user_input', '')[:50] + "..." if len(content.get('user_input', '')) > 50 else content.get('user_input', '')
                        assistant_text = content.get('assistant_response', '')[:50] + "..." if len(content.get('assistant_response', '')) > 50 else content.get('assistant_response', '')
                        info_log(f"[LLM]   #{i+1} å„²å­˜å°è©±: ç”¨æˆ¶='{user_text}', åŠ©æ‰‹='{assistant_text}'")
                    else:
                        info_log(f"[LLM]   #{i+1} {op_type}: {str(content)[:100]}")
            else:
                debug_log(2, f"[LLM] ğŸ“ ç„¡è¨˜æ†¶æ“ä½œéœ€è¦è™•ç†")
            
            # 5. è™•ç†å­¸ç¿’ä¿¡è™Ÿ
            if self.learning_engine.learning_enabled:
                # è™•ç†æ–°çš„ç´¯ç©è©•åˆ†å­¸ç¿’ä¿¡è™Ÿ
                ctx = llm_input.identity_context or {}
                if "learning_signals" in response_data and response_data["learning_signals"]:
                    
                    identity_id = (ctx.get("identity") or {}).get("id") or ctx.get("identity_id") or "default"
                    self.learning_engine.process_learning_signals(identity_id, response_data["learning_signals"])
                    
                # ä¿ç•™èˆŠçš„äº’å‹•è¨˜éŒ„ï¼ˆç”¨æ–¼çµ±è¨ˆå’Œåˆ†æï¼‰
                identity_id = (ctx.get("identity") or {}).get("id") or ctx.get("identity_id") or "default"
                self.learning_engine.record_interaction(
                    identity_id=identity_id,
                    interaction_type="CHAT",
                    user_input=llm_input.text,
                    system_response=response_text,
                    metadata={
                        "memory_used": bool(llm_input.memory_context),
                        "identity_used": bool(llm_input.identity_context)
                    }
                )
            
            # 5. è™•ç†æœƒè©±æ§åˆ¶å»ºè­°
            session_control_result = self._process_session_control(
                response_data, "CHAT", llm_input
            )
            
            # 6. å¿«å–å›æ‡‰
            output = LLMOutput(
                text=response_text,
                processing_time=time.time() - start_time,
                tokens_used=len(response_text.split()),
                success=True,
                error=None,
                confidence=response_data.get("confidence", 0.85),
                sys_action=None,
                status_updates=StatusUpdate(**response_data["status_updates"]) if response_data.get("status_updates") else None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=response_data.get("memory_observation"),
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={
                    "mode": "CHAT",
                    "cached": False,
                    "memory_context_size": len(llm_input.memory_context) if llm_input.memory_context else 0,
                    "identity_context_size": len(llm_input.identity_context) if llm_input.identity_context else 0,
                    "memory_operations_count": len(memory_operations),
                    "memory_operations": memory_operations,
                    "session_control": session_control_result
                }
            )
            
            self.cache_manager.cache_response(cache_key, output)
            
            # ç™¼å¸ƒ LLM å›æ‡‰ç”Ÿæˆäº‹ä»¶
            self._publish_llm_response_event(output, "CHAT", {
                "memory_context_used": bool(llm_input.memory_context),
                "relevant_memories_count": len(relevant_memories) if relevant_memories else 0
            })
            
            return output
            
        except Exception as e:
            error_log(f"[LLM] CHAT æ¨¡å¼è™•ç†éŒ¯èª¤: {e}")
            return LLMOutput(
                text="èŠå¤©è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                processing_time=time.time() - start_time,
                tokens_used=0,
                success=False,
                error=str(e),
                confidence=0.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={"mode": "CHAT", "error_type": "processing_error"}
            )
    
    def _handle_work_mode(self, llm_input: "LLMInput", status: Dict[str, Any]) -> "LLMOutput":
        """è™•ç† WORK æ¨¡å¼ - é€šé MCP èˆ‡ SYS å”ä½œçš„å·¥ä½œä»»å‹™
        
        MCP æ¶æ§‹æµç¨‹ï¼š
        
        Cycle 0ï¼ˆå•Ÿå‹•å·¥ä½œæµï¼‰ï¼š
        - LLM é€šé MCP function calling èª¿ç”¨ start_workflow
        - è¿”å›ï¼šã€Œå·¥ä½œæµå·²å•Ÿå‹•ï¼Œç¬¬ä¸€æ­¥æ˜¯...ã€
        
        Cycle 1+ï¼ˆå·¥ä½œæµæ­¥é©Ÿäº’å‹•ï¼‰ï¼š
        - SYS é€šé review_step è¿”å›ç•¶å‰æ­¥é©Ÿä¿¡æ¯
        - LLM å°‡æ­¥é©Ÿè½‰æ›ç‚ºç”¨æˆ¶å‹å¥½çš„æè¿°
        - ç”¨æˆ¶å›æ‡‰å¾Œï¼ŒLLM é€šé MCP èª¿ç”¨ approve_step/modify_step/cancel_workflow
        - é‡è¤‡ç›´åˆ°å·¥ä½œæµå®Œæˆ
        
        phase åƒæ•¸ï¼ˆå‘å¾Œå…¼å®¹ï¼‰:
        - decision: æ±ºç­–å·¥ä½œæµé¡å‹ï¼ˆå·²å»¢æ£„ï¼Œä½¿ç”¨ MCP function callingï¼‰
        - response: ç”Ÿæˆå·¥ä½œæµå›æ‡‰ï¼ˆé»˜èªï¼ŒåŒ…å« MCP èª¿ç”¨ï¼‰
        """
        start_time = time.time()
        phase = getattr(llm_input, 'phase', 'response')  # é»˜èªç‚º response æ¨¡å¼
        cycle_index = getattr(llm_input, 'cycle_index', 0)
        
        debug_log(2, f"[LLM] è™•ç† WORK æ¨¡å¼ (phase={phase}, cycle={cycle_index})")
        
        try:
            # âœ¨ Cycle 0 Decision Phase: æ±ºç­–å·¥ä½œæµé¡å‹
            if cycle_index == 0 and phase == 'decision':
                return self._decide_workflow(llm_input, start_time)
            
            # âœ¨ Response Phase: ç”Ÿæˆå·¥ä½œæµå›æ‡‰
            else:
                return self._generate_workflow_response(llm_input, status, start_time)
                
        except Exception as e:
            error_log(f"[LLM] WORK æ¨¡å¼è™•ç†éŒ¯èª¤: {e}")
            return LLMOutput(
                text="å·¥ä½œä»»å‹™è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                processing_time=time.time() - start_time,
                tokens_used=0,
                success=False,
                error=str(e),
                confidence=0.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={"mode": "WORK", "error_type": "processing_error", "phase": phase}
            )
    
    def _generate_system_report_response(self, llm_input: "LLMInput", status: Dict[str, Any], start_time: float) -> "LLMOutput":
        """ç”Ÿæˆç³»çµ±å ±å‘Šå›æ‡‰ï¼ˆç³»çµ±ä¸»å‹•é€šçŸ¥ï¼‰
        
        ç³»çµ±å ±å‘Šæ˜¯ç³»çµ±ä¸»å‹•ç™¼å‡ºçš„é€šçŸ¥ï¼ˆå¦‚å¾…è¾¦äº‹é …æé†’ã€æ—¥æ›†äº‹ä»¶ç­‰ï¼‰
        ä¸éœ€è¦å·¥ä½œæµå¼•æ“æˆ– MCP å·¥å…·ï¼Œç›´æ¥ç”Ÿæˆç°¡æ½”å‹å–„çš„è¨Šæ¯
        
        Args:
            llm_input: LLM è¼¸å…¥ï¼ˆåŒ…å«é€šçŸ¥å…§å®¹ï¼‰
            status: ç³»çµ±ç‹€æ…‹
            start_time: é–‹å§‹æ™‚é–“
            
        Returns:
            LLMOutput: ç”Ÿæˆçš„é€šçŸ¥è¨Šæ¯
        """
        try:
            info_log("[LLM] ğŸ”” ç”Ÿæˆç³»çµ±é€šçŸ¥è¨Šæ¯")
            
            # Build system report prompt (request concise, friendly notification)
            notification_content = llm_input.text
            debug_log(2, f"[LLM] ğŸ“‹ é€šçŸ¥å…§å®¹ï¼ˆè¼¸å…¥ï¼‰: {notification_content}")
            
            system_report_prompt = f"""You are U.E.P., an interdimensional being. Your system has detected an event that you need to inform the user about.

System detected event:
{notification_content}

Your task: Convert this system-detected event into a friendly, natural spoken message to inform the user.
Think of it as: "I (U.E.P.) noticed this and want to let you know."

Requirements:
1. Use first person ("I") to show you're informing them (e.g., "I noticed...", "Just wanted to let you know...")
2. Keep it brief (1-2 sentences)
3. Friendly, conversational tone
4. Include all the important details from the notification
5. Don't ask questions - you're informing, not requesting

Examples:

System event: "Reminder: Todo item 'Complete Report' is due in one hour"
Your message: "Hey, just wanted to remind you - 'Complete Report' is due in an hour."

System event: "Reminder: 'Team Meeting' is about to start, Location: Meeting Room A"
Your message: "Heads up, 'Team Meeting' is starting soon in Meeting Room A."

System event: "Alert: Todo item 'Submit Proposal' is overdue"
Your message: "Just letting you know, 'Submit Proposal' is overdue."

Now convert the system event above into your spoken message:"""

            # èª¿è©¦ï¼šè¨˜éŒ„å®Œæ•´ Prompt
            debug_log(3, f"[LLM] ğŸ“ ç³»çµ±é€šçŸ¥ Promptï¼ˆå‰200å­—ç¬¦ï¼‰:\n{system_report_prompt[:200]}...")
            debug_log(3, f"[LLM] ğŸ“ ç³»çµ±é€šçŸ¥ Promptï¼ˆå¾Œ200å­—ç¬¦ï¼‰:\n...{system_report_prompt[-200:]}")

            # èª¿ç”¨ Gemini APIï¼ˆä¸ä½¿ç”¨ MCP å·¥å…·å’Œ function callingï¼‰
            response_data = self.model.query(
                system_report_prompt,
                mode="chat"  # ä½¿ç”¨ chat æ¨¡å¼é¿å… function calling
            )
            
            # èª¿è©¦ï¼šè¨˜éŒ„ LLM åŸå§‹å›æ‡‰
            debug_log(3, f"[LLM] ğŸ¤– Gemini åŸå§‹å›æ‡‰: {response_data.get('text', '')[:200]}")
            
            response_text = response_data.get("text", "").strip()
            
            if not response_text:
                # å¦‚æœ LLM æ²’æœ‰ç”Ÿæˆå›æ‡‰ï¼Œä½¿ç”¨åŸå§‹é€šçŸ¥å…§å®¹
                response_text = notification_content
                info_log("[LLM] âš ï¸ LLM æœªç”Ÿæˆå›æ‡‰ï¼Œä½¿ç”¨åŸå§‹é€šçŸ¥å…§å®¹")
            
            info_log(f"[LLM] âœ… ç³»çµ±é€šçŸ¥è¨Šæ¯å·²ç”Ÿæˆï¼š{response_text[:50]}...")
            
            # ğŸ”‘ è¨­ç½® session_controlï¼šç³»çµ±é€šçŸ¥å®Œæˆå¾Œæ‡‰è©²çµæŸæœƒè©±
            # é€šçŸ¥æ˜¯ä¸€æ¬¡æ€§çš„ï¼Œä¸éœ€è¦æŒçºŒå°è©±
            session_control = {
                "should_end_session": True,
                "end_reason": "system_notification_complete",
                "confidence": 1.0
            }
            debug_log(2, f"[LLM] ğŸ”š ç³»çµ±é€šçŸ¥å®Œæˆï¼Œè¨­ç½®æœƒè©±çµæŸæ¨™è¨˜")
            
            return LLMOutput(
                text=response_text,
                processing_time=time.time() - start_time,
                tokens_used=response_data.get("_meta", {}).get("total_input_tokens", 0),
                success=True,
                error=None,
                confidence=1.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="cheerful",
                metadata={
                    "mode": "WORK",
                    "phase": "response",
                    "system_report": True,
                    "notification_type": getattr(llm_input, 'metadata', {}).get('notification_type', 'unknown'),
                    "session_control": session_control  # âœ… æ·»åŠ  session_control
                }
            )
            
        except Exception as e:
            error_log(f"[LLM] ç”Ÿæˆç³»çµ±é€šçŸ¥è¨Šæ¯å¤±æ•—: {e}")
            # å¤±æ•—æ™‚è¿”å›åŸå§‹é€šçŸ¥å…§å®¹
            return LLMOutput(
                text=llm_input.text,
                processing_time=time.time() - start_time,
                tokens_used=0,
                success=False,
                error=str(e),
                confidence=0.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={"mode": "WORK", "error_type": "system_report_error", "system_report": True}
            )
    
    def _decide_workflow(self, llm_input: "LLMInput", start_time: float) -> "LLMOutput":
        """æ±ºç­–å·¥ä½œæµé¡å‹ï¼ˆCycle 0, phase=decisionï¼‰
        
        ä½¿ç”¨ LLM + MCP å·¥å…·ä¾†ç†è§£ç”¨æˆ¶æ„åœ–ä¸¦æ±ºå®šé©ç•¶çš„å·¥ä½œæµ
        ç”¨æˆ¶è¼¸å…¥ç‚ºè‹±æ–‡ï¼Œç³»çµ±å…§éƒ¨æºé€šä¹Ÿä½¿ç”¨è‹±æ–‡
        """
        debug_log(2, "[LLM] ğŸ¯ Using LLM with MCP tools to decide workflow")
        
        try:
            text = llm_input.text
            
            # æ§‹å»º decision æç¤ºï¼ˆè‹±æ–‡ï¼‰
            # LLM ä½¿ç”¨è‡ªç„¶èªè¨€ç†è§£ä¾†æ±ºå®šå·¥ä½œæµï¼Œä¸ä¾è³´é—œéµè©åŒ¹é…
            decision_prompt = f"""
You are analyzing user intent to determine the appropriate workflow.

User input: "{text}"

Available workflows:
1. drop_and_read - Read file content via drag-and-drop interface
2. intelligent_archive - Archive and organize files intelligently  
3. summarize_tag - Generate summary and tags for files
4. file_selection - Let user choose specific file operations

Based on the user's input, determine which workflow is most appropriate.
Provide your analysis in JSON format:
{{
    "workflow_type": "<workflow_name>",
    "params": {{}},
    "reasoning": "<brief explanation in English>"
}}

Note: You have access to system functions via MCP tools. The SYS module will execute the chosen workflow.
"""
            
            # èª¿ç”¨ Gemini API é€²è¡Œæ±ºç­–
            # æ³¨æ„ï¼šMCP å·¥å…·åœ¨ workflow åŸ·è¡Œæ™‚ä½¿ç”¨ï¼Œdecision éšæ®µåªéœ€è¦ LLM ç†è§£æ„åœ–
            response_data = self.model.query(
                decision_prompt,
                mode="work"
            )
            
            response_text = response_data.get("text", "")
            
            # è§£æ LLM çš„æ±ºç­–çµæœ
            workflow_decision = self._parse_workflow_decision(response_text)
            
            if not workflow_decision:
                # å¦‚æœè§£æå¤±æ•—ï¼Œä½¿ç”¨é»˜èªæ±ºç­–
                workflow_decision = {
                    "workflow_type": "file_selection",
                    "params": {},
                    "reasoning": "Unable to determine specific operation, let user choose"
                }
            
            info_log(f"[LLM] Decision result: {workflow_decision['workflow_type']} - {workflow_decision['reasoning']}")
            
            return LLMOutput(
                text="",  # decision phase doesn't return user-facing text
                processing_time=time.time() - start_time,
                tokens_used=response_data.get("_meta", {}).get("total_input_tokens", 0),
                success=True,
                error=None,
                confidence=0.85,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={
                    "mode": "WORK",
                    "phase": "decision",
                    "workflow_decision": workflow_decision
                }
            )
            
        except Exception as e:
            error_log(f"[LLM] Workflow decision error: {e}")
            raise
    
    def _parse_workflow_decision(self, response_text: str) -> Optional[Dict[str, Any]]:
        """è§£æ LLM è¿”å›çš„å·¥ä½œæµæ±ºç­–
        
        Args:
            response_text: LLM çš„åŸå§‹éŸ¿æ‡‰æ–‡æœ¬
            
        Returns:
            è§£æå¾Œçš„ workflow_decisionï¼Œå¤±æ•—æ™‚è¿”å› None
        """
        try:
            import json
            import re
            
            # å˜—è©¦ç›´æ¥è§£æ JSON
            try:
                decision = json.loads(response_text)
                if "workflow_type" in decision:
                    return decision
            except json.JSONDecodeError:
                pass
            
            # å˜—è©¦å¾æ–‡æœ¬ä¸­æå– JSON
            json_match = re.search(r'\{[^{}]*"workflow_type"[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                try:
                    decision = json.loads(json_match.group())
                    return decision
                except json.JSONDecodeError:
                    pass
            
            # å¦‚æœç„¡æ³•è§£æï¼Œè¨˜éŒ„éŒ¯èª¤
            debug_log(2, f"[LLM] Unable to parse workflow decision from: {response_text[:200]}")
            return None
            
        except Exception as e:
            error_log(f"[LLM] Error parsing workflow decision: {e}")
            return None
    
    def _handle_workflow_input_fast_path(self, llm_input: "LLMInput", workflow_context: Dict[str, Any], start_time: float) -> "LLMOutput":
        """
        å¿«é€Ÿè·¯å¾‘è™•ç†å·¥ä½œæµè¼¸å…¥å ´æ™¯
        ç•¶æª¢æ¸¬åˆ° workflow_input_required æ™‚ï¼Œç›´æ¥èª¿ç”¨ provide_workflow_input å·¥å…·
        é¿å…é€šé Gemini API ç†è§£ç”¨æˆ¶æ„åœ–ï¼ŒåŠ å¿«éŸ¿æ‡‰é€Ÿåº¦ä¸¦é¿å…è¶…æ™‚
        
        âš ï¸ æ³¨æ„ï¼šå¦‚æœè¼¸å…¥éœ€è¦ LLM è§£æï¼ˆå¦‚è‡ªç„¶èªè¨€è½‰çµæ§‹åŒ–æ•¸æ“šï¼‰ï¼Œè¿”å› None è®“æ­£å¸¸æµç¨‹è™•ç†
        
        Args:
            llm_input: LLM è¼¸å…¥
            workflow_context: å·¥ä½œæµä¸Šä¸‹æ–‡ï¼ˆåŒ…å« workflow_session_idã€user_input ç­‰ï¼‰
            start_time: é–‹å§‹æ™‚é–“
            
        Returns:
            LLMOutput: è™•ç†çµæœï¼Œæˆ– None è¡¨ç¤ºéœ€è¦æ­£å¸¸æµç¨‹è™•ç†
        """
        try:
            import asyncio
            
            # æå–å·¥ä½œæµè³‡è¨Š
            workflow_session_id = workflow_context.get('workflow_session_id', 'unknown')
            user_input = workflow_context.get('user_input', llm_input.text)
            is_optional = workflow_context.get('is_optional', False)
            step_id = workflow_context.get('step_id', 'unknown')
            prompt = workflow_context.get('prompt', '')
            
            # ğŸ”§ æª¢æ¸¬æ˜¯å¦éœ€è¦ LLM è§£æ
            # å¦‚æœæç¤ºè¦æ±‚çµæ§‹åŒ–æ•¸æ“šï¼ˆåŒ…å« JSONã€task_nameã€priority ç­‰é—œéµå­—ï¼‰ï¼Œ
            # ä¸”ç”¨æˆ¶è¼¸å…¥æ˜¯è‡ªç„¶èªè¨€ï¼ˆä¸æ˜¯ JSON æˆ– key=value æ ¼å¼ï¼‰ï¼Œ
            # å‰‡ä¸ä½¿ç”¨å¿«é€Ÿè·¯å¾‘ï¼Œè®“ LLM è§£æ
            requires_structured_data = any(keyword in prompt.lower() for keyword in [
                'json', 'task_name', 'task_description', 'priority', 'deadline'
            ])
            
            is_natural_language = not (
                user_input.strip().startswith('{') or  # JSON æ ¼å¼
                '=' in user_input  # key=value æ ¼å¼
            )
            
            if requires_structured_data and is_natural_language:
                debug_log(2, f"[LLM] å¿«é€Ÿè·¯å¾‘è·³éï¼šè¼¸å…¥éœ€è¦ LLM è§£æï¼ˆstep={step_id}ï¼‰")
                return None  # è¿”å› None è®“æ­£å¸¸æµç¨‹è™•ç†
            
            info_log(f"[LLM] å¿«é€Ÿè·¯å¾‘ï¼šç›´æ¥æäº¤å·¥ä½œæµè¼¸å…¥ '{user_input}' åˆ°æ­¥é©Ÿ {step_id}")
            
            # ç›´æ¥èª¿ç”¨ provide_workflow_input å·¥å…·
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # èª¿ç”¨ MCP å·¥å…·
            function_call_result = loop.run_until_complete(
                self.mcp_client.call_tool("provide_workflow_input", {
                    "session_id": workflow_session_id,
                    "user_input": user_input,
                    "use_fallback": False  # ç”¨æˆ¶æä¾›äº†æ˜ç¢ºè¼¸å…¥
                })
            )
            
            debug_log(2, f"[LLM] å¿«é€Ÿè·¯å¾‘åŸ·è¡Œçµæœ: {function_call_result.get('status')}")
            
            # ğŸ”§ å¿«é€Ÿè·¯å¾‘çš„è·è²¬ï¼š
            # 1. å¿«é€Ÿæäº¤ç”¨æˆ¶è¼¸å…¥åˆ°å·¥ä½œæµï¼ˆé¿å… Gemini API è¶…æ™‚ï¼‰
            # 2. ä¸ç”Ÿæˆä»»ä½•å›æ‡‰æ–‡æœ¬ï¼ˆè¿”å›ç©ºå­—ç¬¦ä¸²ï¼‰
            # 3. è®“å·¥ä½œæµæ­¥é©Ÿå®Œæˆäº‹ä»¶ï¼ˆWORKFLOW_STEP_COMPLETEDï¼‰è§¸ç™¼æ­£å¸¸çš„ LLM å¯©æ ¸æµç¨‹
            #    - å·¥ä½œæµæ¨é€²åˆ°ä¸‹ä¸€æ­¥ï¼ˆå¦‚ archive_confirmï¼‰å¾Œæœƒç™¼å‡ºäº‹ä»¶
            #    - LLM è¨‚é–±è©²äº‹ä»¶ï¼Œå°‡å…¶åŠ å…¥ _pending_workflow_events éšŠåˆ—
            #    - ä¸‹æ¬¡å¾ªç’°æ™‚ï¼ŒLLM æœƒèª¿ç”¨ Gemini ç”Ÿæˆè‡ªç„¶èªè¨€å›æ‡‰
            
            # âœ… æª¢æŸ¥å·¥å…·èª¿ç”¨æ˜¯å¦æˆåŠŸ
            result_status = function_call_result.get("status", "unknown")
            
            if result_status == "success":
                info_log(f"[LLM] å¿«é€Ÿè·¯å¾‘ï¼šå·¥ä½œæµè¼¸å…¥å·²æˆåŠŸæäº¤ï¼Œç­‰å¾…å·¥ä½œæµäº‹ä»¶è§¸ç™¼å¾ŒçºŒå›æ‡‰ç”Ÿæˆ")
                # è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè®“å·¥ä½œæµäº‹ä»¶é©…å‹•å¾ŒçºŒæµç¨‹
                response_text = ""
            else:
                # éŒ¯èª¤æƒ…æ³ï¼šæä¾›ç°¡å–®éŒ¯èª¤è¨Šæ¯
                error_msg = function_call_result.get("error", "Unknown error")
                response_text = f"è™•ç†æ™‚ç™¼ç”Ÿå•é¡Œï¼š{error_msg}ã€‚è«‹å†è©¦ä¸€æ¬¡ã€‚"
                error_log(f"[LLM] å¿«é€Ÿè·¯å¾‘å¤±æ•—: {error_msg}")
            
            # æ§‹å»º LLMOutput
            return LLMOutput(
                text=response_text,
                processing_time=time.time() - start_time,
                tokens_used=0,  # å¿«é€Ÿè·¯å¾‘ä¸ä½¿ç”¨ LLM tokens
                success=result_status == "success",
                error=None if result_status == "success" else function_call_result.get("error"),
                confidence=0.9,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={
                    "mode": "WORK",
                    "workflow_context_size": len(str(workflow_context)),
                    "sys_actions_count": 0,
                    "sys_actions": [],
                    "system_context_size": 0,
                    "session_control": None,
                    "function_call_made": True,
                    "function_call_result": function_call_result,
                    "fast_path": True  # æ¨™è¨˜ä½¿ç”¨äº†å¿«é€Ÿè·¯å¾‘
                }
            )
            
        except Exception as e:
            error_log(f"[LLM] å¿«é€Ÿè·¯å¾‘è™•ç†å·¥ä½œæµè¼¸å…¥å¤±æ•—: {e}")
            # è¿”å›éŒ¯èª¤çµæœ
            return LLMOutput(
                text="æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è¼¸å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                processing_time=time.time() - start_time,
                tokens_used=0,
                success=False,
                error=str(e),
                confidence=0.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={
                    "mode": "WORK",
                    "error_type": "fast_path_error",
                    "fast_path": True
                }
            )
    
    def _generate_workflow_response(self, llm_input: "LLMInput", status: Dict[str, Any], start_time: float) -> "LLMOutput":
        """ç”Ÿæˆå·¥ä½œæµå›æ‡‰ï¼ˆæ‰€æœ‰ Cycle, phase=responseï¼‰"""
        debug_log(2, "[LLM] ğŸ’¬ ç”Ÿæˆå·¥ä½œæµå›æ‡‰")
        
        try:
            # âœ… æª¢æŸ¥æ˜¯å¦ç‚ºç³»çµ±å ±å‘Šæ¨¡å¼ï¼ˆç³»çµ±ä¸»å‹•é€šçŸ¥ï¼‰
            is_system_report = getattr(llm_input, 'system_report', False)
            if is_system_report:
                debug_log(2, "[LLM] ğŸ”” ç³»çµ±å ±å‘Šæ¨¡å¼ï¼šç”Ÿæˆç°¡æ½”é€šçŸ¥è¨Šæ¯")
                return self._generate_system_report_response(llm_input, status, start_time)
            
            # âœ… æª¢æŸ¥æ˜¯å¦æœ‰é‹è¡Œä¸­çš„å·¥ä½œæµå¼•æ“
            # é€™å€‹æª¢æŸ¥å¾ˆé‡è¦ï¼š
            # 1. WS æ˜¯å®¹å™¨ï¼ˆå¯èƒ½å­˜åœ¨ä½†æ²’æœ‰å·¥ä½œæµï¼‰
            # 2. WorkflowEngine æ˜¯å¯¦éš›çš„å·¥ä½œæµåŸ·è¡Œå™¨
            # 3. åªæœ‰ç•¶ WorkflowEngine å­˜åœ¨æ™‚ï¼Œæ‰èªç‚ºæœ‰æ´»èºçš„å·¥ä½œæµ
            has_active_workflow = False
            if self.session_info and self.session_info.get('session_type') == 'workflow':
                session_id = self.session_info.get('session_id')
                
                # æª¢æŸ¥ SYS æ¨¡çµ„çš„ workflow_engines å­—å…¸ä¸­æ˜¯å¦æœ‰å°æ‡‰çš„å¼•æ“
                try:
                    from core.framework import core_framework
                    sys_module = core_framework.get_module('sys')
                    
                    if sys_module and hasattr(sys_module, 'workflow_engines'):
                        has_active_workflow = session_id in sys_module.workflow_engines
                        if has_active_workflow:
                            debug_log(2, f"[LLM] æª¢æ¸¬åˆ°æ´»èºçš„å·¥ä½œæµå¼•æ“: {session_id}")
                        else:
                            debug_log(2, f"[LLM] WS å­˜åœ¨ä½†ç„¡å·¥ä½œæµå¼•æ“: {session_id}")
                    else:
                        debug_log(2, f"[LLM] ç„¡æ³•è¨ªå• SYS æ¨¡çµ„çš„ workflow_engines")
                except Exception as e:
                    debug_log(2, f"[LLM] æª¢æŸ¥å·¥ä½œæµå¼•æ“æ™‚å‡ºéŒ¯: {e}")
                    # ä¿å®ˆç­–ç•¥ï¼šå¦‚æœç„¡æ³•æª¢æŸ¥ï¼Œå‡è¨­æœ‰å·¥ä½œæµï¼ˆé¿å…é‡è¤‡å•Ÿå‹•ï¼‰
                    has_active_workflow = True
            
            # âœ… æª¢æŸ¥æ˜¯å¦æœ‰å¾…è™•ç†çš„å·¥ä½œæµäº‹ä»¶ï¼ˆæ­£åœ¨å¯©æ ¸æ­¥é©Ÿï¼‰
            pending_workflow = getattr(llm_input, 'workflow_context', None)
            is_reviewing_step = pending_workflow and pending_workflow.get('type') == 'workflow_step_response'
            
            # ğŸ”§ å¿«é€Ÿè·¯å¾‘ï¼šå¦‚æœæ˜¯å·¥ä½œæµè¼¸å…¥å ´æ™¯ï¼Œç›´æ¥èª¿ç”¨ provide_workflow_input
            # é¿å…èŠ±è²»æ™‚é–“é€šé Gemini API ç†è§£ç”¨æˆ¶æ„åœ–ï¼ŒåŠ å¿«éŸ¿æ‡‰é€Ÿåº¦
            is_workflow_input = pending_workflow and pending_workflow.get('type') == 'workflow_input_required'
            debug_log(2, f"[LLM] æª¢æŸ¥å¿«é€Ÿè·¯å¾‘æ¢ä»¶: pending_workflow={pending_workflow is not None}, type={pending_workflow.get('type') if pending_workflow else None}, is_workflow_input={is_workflow_input}")
            
            # âœ… æª¢æŸ¥ï¼šå¦‚æœæ­£åœ¨å¯©æ ¸æ­¥é©Ÿçµæœï¼Œä½†ç”¨æˆ¶åªæ˜¯æä¾›äº†ç°¡å–®å›æ‡‰ï¼ˆå¦‚ "yes", "no"ï¼‰ï¼Œ
            # é€™å¯èƒ½æ˜¯èª¤åˆ¤ï¼Œå¯¦éš›ä¸Šç”¨æˆ¶æ˜¯åœ¨å›æ‡‰äº’å‹•æ­¥é©Ÿè€Œä¸æ˜¯å¯©æ ¸æ­¥é©Ÿ
            if is_reviewing_step and llm_input.text:
                user_text_lower = llm_input.text.strip().lower()
                simple_responses = ['yes', 'y', 'no', 'n', 'ok', 'confirm', 'cancel', 
                                   'ç¢ºèª', 'å–æ¶ˆ', 'skip', 'è·³é', '']
                if user_text_lower in simple_responses:
                    # ç”¨æˆ¶æä¾›äº†ç°¡å–®å›æ‡‰ï¼Œé€™å¯èƒ½æ˜¯äº’å‹•æ­¥é©Ÿçš„è¼¸å…¥è€Œéæ­¥é©Ÿå¯©æ ¸
                    # æª¢æŸ¥æ˜¯å¦æœ‰æ´»èºçš„äº’å‹•æ­¥é©Ÿåœ¨ç­‰å¾…è¼¸å…¥
                    from core.working_context import working_context_manager
                    if working_context_manager.is_workflow_waiting_input():
                        debug_log(2, "[LLM] æª¢æ¸¬åˆ°ç°¡å–®å›æ‡‰ä¸”å·¥ä½œæµåœ¨ç­‰å¾…è¼¸å…¥ï¼Œåˆ‡æ›ç‚ºå·¥ä½œæµè¼¸å…¥å ´æ™¯")
                        # æ§‹å»º workflow_input_required context
                        saved_context = working_context_manager.get_context_data('workflow_input_context', {})
                        workflow_session_id = saved_context.get('workflow_session_id') or self.session_info.get('session_id')
                        
                        pending_workflow = {
                            'type': 'workflow_input_required',
                            'workflow_session_id': workflow_session_id,
                            'workflow_type': saved_context.get('workflow_type', 'unknown'),
                            'step_id': saved_context.get('step_id', 'input_step'),
                            'user_input': llm_input.text,
                            'is_optional': saved_context.get('optional', False)
                        }
                        is_workflow_input = True
                        is_reviewing_step = False
            
            if is_workflow_input and pending_workflow:
                info_log("[LLM] ğŸš€ æª¢æ¸¬åˆ°å·¥ä½œæµè¼¸å…¥å ´æ™¯ï¼Œå˜—è©¦ä½¿ç”¨å¿«é€Ÿè·¯å¾‘ç›´æ¥æäº¤è¼¸å…¥")
                fast_path_result = self._handle_workflow_input_fast_path(llm_input, pending_workflow, start_time)
                if fast_path_result is not None:
                    return fast_path_result
                # å¿«é€Ÿè·¯å¾‘è¿”å› None è¡¨ç¤ºéœ€è¦ LLM è§£æï¼Œç¹¼çºŒæ­£å¸¸æµç¨‹
                debug_log(2, "[LLM] å¿«é€Ÿè·¯å¾‘è¿”å› Noneï¼Œç¹¼çºŒæ­£å¸¸æµç¨‹è®“ LLM è§£æè¼¸å…¥")
            
            from core.working_context import working_context_manager
            
            # âœ… æª¢æŸ¥æ˜¯å¦æœ‰ MCP Server å¯ç”¨
            # å³ä½¿åœ¨ workflow_step_response æ™‚ä¹Ÿæä¾›å·¥å…·ï¼Œè®“ LLM è‡ªå·±æ±ºå®šæ˜¯å¦ä½¿ç”¨ï¼ˆtool_choice=AUTOï¼‰
            is_step_response = pending_workflow and pending_workflow.get('type') == 'workflow_step_response'
            mcp_tools = None
            if self.mcp_client and hasattr(self.mcp_client, 'get_tools_as_gemini_format'):
                mcp_tools = self.mcp_client.get_tools_as_gemini_format()
                debug_log(2, f"[LLM] MCP å·¥å…·å·²æº–å‚™: {len(mcp_tools) if mcp_tools else 0} å€‹")
                if is_step_response:
                    debug_log(2, "[LLM] æ­¥é©Ÿå›æ‡‰æ¨¡å¼ï¼šæä¾›å·¥å…·ä½†ä¸å¼·åˆ¶ä½¿ç”¨ï¼ˆtool_choice=AUTOï¼‰")
            
            # ğŸ”§ æ±ºå®š tool_choice æ¨¡å¼ï¼ˆåœ¨æ§‹å»º prompt ä¹‹å‰ï¼‰
            if not has_active_workflow and not is_reviewing_step and mcp_tools:
                tool_choice = "ANY"  # å¼·åˆ¶èª¿ç”¨å·¥å…·ï¼ˆæ–°è«‹æ±‚æ‡‰è©²å•Ÿå‹•å·¥ä½œæµï¼‰
                force_tool_use = True
            else:
                tool_choice = "AUTO"  # è‡ªå‹•æ±ºå®šï¼ˆå¯èƒ½éœ€è¦ç¹¼çºŒå·¥ä½œæµæˆ–åªæ˜¯å›æ‡‰ï¼‰
                force_tool_use = False
            
            # æ§‹å»º WORK æç¤º
            prompt = self.prompt_manager.build_work_prompt(
                user_input=llm_input.text,
                available_functions=None,  # ä¸å†éœ€è¦æ–‡å­—æè¿°ï¼Œä½¿ç”¨ MCP tools
                workflow_context=pending_workflow,
                identity_context=llm_input.identity_context,
                use_mcp_tools=True if mcp_tools else False,
                suppress_start_workflow_instruction=bool(has_active_workflow or is_reviewing_step),  # âœ… å·²æœ‰å·¥ä½œæµæ™‚æŠ‘åˆ¶å•Ÿå‹•æŒ‡ç¤º
                force_tool_use=force_tool_use  # ğŸ”§ å‚³éæ˜¯å¦å¼·åˆ¶èª¿ç”¨å·¥å…·
            )
            
            # ç²å–æˆ–å‰µå»ºä»»å‹™å¿«å–
            cached_content_ids = self._get_system_caches("work")
            
            # ğŸ” DEBUG: è¨˜éŒ„ç™¼é€çµ¦ Gemini çš„ prompt
            if mcp_tools:
                debug_log(3, f"[LLM] Prompt ç¸½é•·åº¦: {len(prompt)} å­—ç¬¦")
                debug_log(3, f"[LLM] Prompt å‰ 500 å­—ç¬¦:\n{prompt[:500]}...")
            
            # âœ… å‘¼å« Gemini API (ä½¿ç”¨ MCP tools é€²è¡Œ function calling)
            # tool_choice å·²åœ¨æ§‹å»º prompt æ™‚æ±ºå®š
            debug_log(2, f"[LLM] Function calling æ¨¡å¼: {tool_choice} (has_active_workflow={has_active_workflow}, is_reviewing_step={is_reviewing_step}, has_tools={mcp_tools is not None})")
            
            response_data = self.model.query(
                prompt, 
                mode="work",
                cached_content=cached_content_ids.get("functions"),
                tools=mcp_tools,  # å‚³å…¥ MCP tools
                tool_choice=tool_choice  # ğŸ”§ ä¿®å¾©ï¼šä½¿ç”¨å‹•æ…‹æ±ºå®šçš„æ¨¡å¼
            )
            
            # ï¿½ è™•ç† MALFORMED_FUNCTION_CALL éŒ¯èª¤ï¼šé™ç´šç‚º AUTO æ¨¡å¼é‡è©¦
            if response_data.get("error") == "malformed_function_call" and tool_choice == "ANY":
                error_log(f"[LLM] æª¢æ¸¬åˆ° MALFORMED_FUNCTION_CALLï¼Œé™ç´šç‚º AUTO æ¨¡å¼é‡è©¦")
                debug_log(2, "[LLM] ä½¿ç”¨ tool_choice=AUTO é‡æ–°èª¿ç”¨ Gemini")
                
                response_data = self.model.query(
                    prompt, 
                    mode="work",
                    cached_content=cached_content_ids.get("functions"),
                    tools=mcp_tools,
                    tool_choice="AUTO"  # é™ç´šç‚º AUTO æ¨¡å¼
                )
                
                # å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œæœ€å¾Œå˜—è©¦ä¸ä½¿ç”¨å·¥å…·ï¼ˆç´”æ–‡æœ¬å›æ‡‰ï¼‰
                if response_data.get("error") == "malformed_function_call":
                    error_log(f"[LLM] AUTO æ¨¡å¼ä»ç„¶å¤±æ•—ï¼Œä½¿ç”¨ç´”æ–‡æœ¬æ¨¡å¼")
                    response_data = self.model.query(
                        prompt, 
                        mode="work",
                        cached_content=cached_content_ids.get("functions"),
                        tools=None,  # ä¸ä½¿ç”¨å·¥å…·
                        tool_choice="NONE"
                    )
            
            # ï¿½ğŸ” DEBUG: è¨˜éŒ„ Gemini çš„åŸå§‹éŸ¿æ‡‰
            debug_log(3, f"[LLM] Gemini éŸ¿æ‡‰é¡å‹: {list(response_data.keys())}")
            if 'function_call' in response_data:
                debug_log(3, f"[LLM] Function call: {response_data['function_call']}")
            if 'text' in response_data:
                debug_log(3, f"[LLM] Text éŸ¿æ‡‰: {response_data.get('text', '')[:200]}")
            
            # âœ… è™•ç† function call å›æ‡‰
            function_call_result = None
            response_text = ""  # åˆå§‹åŒ– response_text
            skip_default_followup = False  # åˆå§‹åŒ–è·³éæ¨™èªŒ
            follow_up_prompt = ""  # åˆå§‹åŒ– follow_up_prompt
            
            if "function_call" in response_data and response_data["function_call"]:
                debug_log(2, f"[LLM] æª¢æ¸¬åˆ° function call: {response_data['function_call']['name']}")
                
                # åŒæ­¥èª¿ç”¨ async function
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                
                function_call_result = loop.run_until_complete(
                    self.mcp_client.handle_llm_function_call(response_data["function_call"])
                )
                
                debug_log(2, f"[LLM] MCP å·¥å…·åŸ·è¡Œçµæœ: {function_call_result.get('status')}")
                
                # âœ… è®“ Gemini æ ¹æ“š MCP çµæœç”Ÿæˆå›æ‡‰
                # æ§‹å»ºåŒ…å«å·¥å…·åŸ·è¡Œçµæœçš„ follow-up prompt
                result_status = function_call_result.get("status", "unknown")
                # âœ… å¾ content.data ç²å–å·¥ä½œæµè³‡æ–™ï¼ˆMCP ToolResult çµæ§‹ï¼‰
                content = function_call_result.get("content", {})
                result_data = content.get("data", {}) if isinstance(content, dict) else {}
                result_message = function_call_result.get("formatted_message", "") or content.get("message", "")
                tool_name = function_call_result.get("tool_name", "unknown")
                
                # âœ… åˆ¤æ–·å·¥ä½œæµç‹€æ…‹ï¼šå¾ message æ¨æ–·
                # - "started" (ä½†ä¸æ˜¯ "completed" æˆ– "finished") â†’ started
                # - "completed" æˆ– "finished" â†’ completed
                # - å…¶ä»– â†’ unknown
                workflow_status = "unknown"
                if isinstance(result_message, str):
                    msg_lower = result_message.lower()
                    # å…ˆæª¢æŸ¥ completed/finishedï¼ˆå„ªå…ˆç´šè¼ƒé«˜ï¼‰
                    if " completed" in msg_lower or " finished" in msg_lower or "å®Œæˆ" in result_message:
                        workflow_status = "completed"
                    # å†æª¢æŸ¥ startedï¼ˆä½†ç¢ºä¿ä¸æ˜¯ "will complete" é€™é¡æœªä¾†å¼ï¼‰
                    elif "started" in msg_lower or "å·²å•Ÿå‹•" in result_message:
                        workflow_status = "started"
                
                debug_log(2, f"[LLM] æ¨æ–·å·¥ä½œæµç‹€æ…‹: {workflow_status} (from message: {result_message[:50]}...)")
                
                # âœ… æ§‹å»ºåŒ…å«èªè¨€æŒ‡ç¤ºçš„ follow-up prompt
                language_instruction = (
                    "You are U.E.P., an interdimensional being who prefers to use English for communication.\n"
                    "Your current task: Provide a brief, friendly response to the user in English.\n\n"
                )
                
                if result_status == "success":
                    # âœ… resolve_path æˆåŠŸï¼šè¦æ±‚ LLM ç¹¼çºŒèª¿ç”¨ provide_workflow_input
                    if tool_name == "resolve_path":
                        resolved_path = result_data.get("data", {}).get("resolved_path", "") if isinstance(result_data, dict) else ""
                        path_exists = result_data.get("data", {}).get("exists", False) if isinstance(result_data, dict) else False
                        
                        follow_up_prompt = (
                            f"The path has been successfully resolved:\n"
                            f"  Original: {result_data.get('data', {}).get('original_description', 'unknown')}\n"
                            f"  Resolved: {resolved_path}\n"
                            f"  Exists: {path_exists}\n\n"
                            f"Now you MUST call the provide_workflow_input tool to submit this resolved path:\n"
                            f"  provide_workflow_input(\n"
                            f"    session_id: <auto-injected>,\n"
                            f"    user_input: '{resolved_path}',\n"
                            f"    use_fallback: False\n"
                            f"  )\n\n"
                            f"DO NOT generate a text response. ONLY call the tool."
                        )
                        
                        # âœ… ä¿ç•™å·¥å…·åˆ—è¡¨ï¼Œè®“ LLM èƒ½å¤ èª¿ç”¨ provide_workflow_input
                        follow_up_response = self.model.query(
                            follow_up_prompt,
                            mode="work",
                            tools=mcp_tools  # âœ… ä¿ç•™å·¥å…·åˆ—è¡¨
                        )
                        
                        # å¦‚æœæœ‰ function callï¼Œè™•ç†å®ƒ
                        if "function_call" in follow_up_response and follow_up_response["function_call"]:
                            debug_log(2, f"[LLM] resolve_path å¾ŒçºŒèª¿ç”¨: {follow_up_response['function_call']['name']}")
                            
                            # åŸ·è¡Œç¬¬äºŒå€‹ function call
                            second_result = loop.run_until_complete(
                                self.mcp_client.handle_llm_function_call(follow_up_response["function_call"])
                            )
                            
                            debug_log(2, f"[LLM] ç¬¬äºŒå€‹å·¥å…·åŸ·è¡Œçµæœ: {second_result.get('status')}")
                            
                            # é€™æ¬¡éœ€è¦æ–‡å­—å›æ‡‰
                            final_prompt = (
                                f"{language_instruction}"
                                f"The input has been successfully submitted to the workflow.\n"
                                f"Result: {second_result.get('formatted_message', '')}\n\n"
                                f"Please inform the user in a brief, friendly tone that you're processing their request.\n"
                                f"IMPORTANT: Respond in English only."
                            )
                            
                            final_response = self.model.query(final_prompt, mode="work", tools=None)
                            response_text = final_response.get("text", "Processing your request...")
                            
                            # å„²å­˜å®Œæ•´çš„ function call çµæœ
                            function_call_result = second_result
                        else:
                            # LLM æ²’æœ‰èª¿ç”¨å·¥å…·ï¼Œä½¿ç”¨é è¨­å›æ‡‰
                            response_text = "I'm processing your request..."
                            debug_log(1, "[LLM] resolve_path å¾Œ LLM æ²’æœ‰èª¿ç”¨ provide_workflow_input")
                        
                        # è·³éå¾ŒçºŒçš„ follow-up è™•ç†
                        skip_default_followup = True
                    # âœ… provide_workflow_input æˆåŠŸï¼šæç¤ºä¸‹ä¸€æ­¥éœ€æ±‚æˆ–ç¢ºèªå®Œæˆ
                    elif tool_name == "provide_workflow_input":
                        # ğŸ”§ ä¿®å¾©ï¼šå¾æ ¹ç´šåˆ¥æå–å·¥ä½œæµç‹€æ…‹ä¿¡æ¯ï¼ˆä¸æ˜¯å¾ data å­—æ®µï¼‰
                        # MCP å·¥å…·è¿”å›çš„çµæ§‹æ˜¯ {status, requires_input, step_info, ...}
                        workflow_result_status = result_data.get("status", "unknown") if isinstance(result_data, dict) else "unknown"
                        requires_input = result_data.get("requires_input", False) if isinstance(result_data, dict) else False
                        step_info = result_data.get("step_info", {}) if isinstance(result_data, dict) else {}
                        current_step = step_info.get("current_step", {}) if step_info else {}
                        workflow_info = step_info.get("workflow_info", {}) if step_info else {}
                        previous_result = step_info.get("previous_step_result", {}) if step_info else {}
                        
                        # å¦‚æœå·¥ä½œæµé‚„åœ¨ç­‰å¾…è¼¸å…¥ï¼ˆé€²å…¥ä¸‹ä¸€å€‹ Interactive æ­¥é©Ÿï¼‰
                        # LLM æ‡‰è©²æç¤ºç”¨æˆ¶ä¸‹ä¸€æ­¥éœ€è¦ä»€éº¼è¼¸å…¥
                        if workflow_result_status == "waiting" and requires_input:
                            debug_log(2, "[LLM] provide_workflow_input: å·¥ä½œæµéœ€è¦ä¸‹ä¸€æ­¥è¼¸å…¥ï¼Œç”Ÿæˆæç¤º")
                            
                            # æå–ä¸‹ä¸€æ­¥çš„ä¿¡æ¯
                            next_step_id = current_step.get("step_id", "unknown")
                            next_step_prompt = current_step.get("prompt", "")
                            next_step_description = current_step.get("description", "")
                            previous_message = previous_result.get("message", "")
                            
                            follow_up_prompt = (
                                f"{language_instruction}"
                                f"The user's input has been processed successfully.\n"
                                f"Previous step result: {previous_message}\n\n"
                                f"Now the workflow needs the next input:\n"
                                f"Step: {next_step_id}\n"
                                f"Description: {next_step_description}\n"
                                f"Prompt: {next_step_prompt}\n\n"
                                f"Your task: Inform the user in a natural, friendly way:\n"
                                f"1. Briefly acknowledge their previous input\n"
                                f"2. Clearly explain what input is needed next\n"
                                f"3. Use the step's prompt as guidance but rephrase it naturally\n\n"
                                f"IMPORTANT:\n"
                                f"- Keep it concise (2-3 sentences)\n"
                                f"- Be conversational and helpful\n"
                                f"- Respond in English only\n"
                            )
                            
                            final_response = self.model.query(follow_up_prompt, mode="work", tools=None)
                            response_text = final_response.get("text", next_step_prompt)
                            skip_default_followup = True
                        # å¦‚æœå·¥ä½œæµå®Œæˆæˆ–å–æ¶ˆï¼Œè®“ LLM ç”Ÿæˆç¢ºèªè¨Šæ¯
                        elif workflow_result_status in ["completed", "cancelled"]:
                            final_status = "completed" if workflow_result_status == "completed" else "cancelled"
                            follow_up_prompt = (
                                f"{language_instruction}"
                                f"The workflow has been {final_status}.\n"
                                f"Result: {result_message}\n\n"
                                f"Please inform the user in a friendly, conversational way.\n"
                                f"IMPORTANT: Keep it natural and concise, respond in English only."
                            )
                            
                            final_response = self.model.query(follow_up_prompt, mode="work", tools=None)
                            response_text = final_response.get("text", f"Workflow {final_status}.")
                            skip_default_followup = True
                        # å…¶ä»–æƒ…æ³ï¼šå·¥ä½œæµæ­£åœ¨è™•ç†ï¼ˆProcessing æ­¥é©Ÿï¼‰ï¼Œä¸éœ€è¦å›æ‡‰
                        # ç­‰å¾… Processing æ­¥é©Ÿå®Œæˆå¾Œçš„ LLM å¯©æ ¸
                        else:
                            debug_log(2, f"[LLM] provide_workflow_input: å·¥ä½œæµè™•ç†ä¸­ (status={workflow_result_status})ï¼Œè·³éå›æ‡‰")
                            response_text = ""
                            skip_default_followup = True
                    # âœ… approve_step æˆåŠŸï¼šåŸºæ–¼å·¥ä½œæµä¸Šä¸‹æ–‡ç”Ÿæˆé©ç•¶çš„å›æ‡‰
                    elif tool_name == "approve_step":
                        # æª¢æŸ¥æ˜¯å¦ç‚º workflow_step_response å ´æ™¯
                        pending_workflow = getattr(llm_input, 'workflow_context', None)
                        if pending_workflow and pending_workflow.get('type') == 'workflow_step_response':
                            # æå–å·¥ä½œæµä¸Šä¸‹æ–‡
                            is_complete = pending_workflow.get('is_complete', False)
                            next_step_info = pending_workflow.get('next_step_info')
                            next_step_is_interactive = next_step_info and next_step_info.get('step_type') == 'interactive' if next_step_info else False
                            step_result = pending_workflow.get('step_result', {})
                            review_data = pending_workflow.get('review_data', {})
                            debug_log(2, f"[LLM] å¾ pending_workflow æå–çš„ review_data keys: {list(review_data.keys()) if review_data else 'None'}")
                            
                            if is_complete:
                                # ğŸ”§ å·¥ä½œæµå®Œæˆï¼šç”Ÿæˆç¸½çµå›æ‡‰ä¸¦çµæŸæœƒè©±
                                follow_up_prompt = (
                                    f"{language_instruction}"
                                    f"The workflow has completed successfully.\n"
                                    f"Step Result: {step_result.get('message', 'Success')}\n"
                                )
                                
                                # âœ… æä¾›è±å¯Œçš„å¯©æ ¸æ•¸æ“šçµ¦ LLMï¼ˆåŒ…æ‹¬æ–‡ä»¶å…§å®¹ç­‰ï¼‰
                                if review_data:
                                    debug_log(2, f"[LLM] æª¢æŸ¥ review_data æ˜¯å¦åŒ…å« full_content: {'full_content' in review_data}")
                                    # ç‰¹æ®Šè™•ç†ï¼šå¦‚æœæœ‰ full_contentï¼ˆæ–‡ä»¶è®€å–ï¼‰ï¼Œæä¾›å®Œæ•´å…§å®¹
                                    if 'full_content' in review_data:
                                        debug_log(2, f"[LLM] ç™¼ç¾ full_contentï¼Œæ·»åŠ åˆ° prompt")
                                        file_name = review_data.get('file_name', 'unknown')
                                        content = review_data.get('full_content', '')
                                        content_length = review_data.get('content_length', len(content))
                                        follow_up_prompt += (
                                            f"\nFile Read Results:\n"
                                            f"- File: {file_name}\n"
                                            f"- Content Length: {content_length} characters\n"
                                            f"- Full Content:\n{content}\n"
                                        )
                                    else:
                                        debug_log(2, f"[LLM] æœªç™¼ç¾ full_contentï¼Œä½¿ç”¨é€šç”¨æ•¸æ“š")
                                        # é€šç”¨æ•¸æ“šï¼šé¡¯ç¤ºå‰ 500 å­—ç¬¦
                                        follow_up_prompt += f"Workflow Data: {str(review_data)[:500]}\n"
                                
                                follow_up_prompt += (
                                    f"\nGenerate a natural, friendly response that:\n"
                                    f"1. Confirms the task is complete\n"
                                    f"2. Summarizes the key results/data (for file read, briefly mention the content)\n"
                                    f"3. Keep it conversational (2-3 sentences)\n"
                                    f"IMPORTANT: Respond in English only."
                                )
                                
                                # âœ… å·¥ä½œæµå®Œæˆï¼šè¨­ç½® session_control ä»¥è§¸ç™¼æœƒè©±çµæŸ
                                # åœ¨ç”Ÿæˆ follow-up å›æ‡‰å¾Œï¼Œå°‡é€šé _process_session_control æª¢æ¸¬ä¸¦æ¨™è¨˜å¾…çµæŸ
                                # ModuleCoordinator æœƒåœ¨ processingâ†’output æ™‚æª¢æ¸¬ session_control ä¸¦æ¨™è¨˜ WS
                                # Controller æœƒåœ¨ CYCLE_COMPLETED æ™‚çµæŸ WSï¼ˆç¢ºä¿ LLM å›æ‡‰å’Œ TTS è¼¸å‡ºå®Œæˆï¼‰
                                try:
                                    session_id = pending_workflow.get('session_id')
                                    wf_type = pending_workflow.get('workflow_type', 'workflow')
                                    if session_id:
                                        # å°‡ session_control æ·»åŠ åˆ° response_dataï¼Œä»¥ä¾¿å¾ŒçºŒè™•ç†
                                        response_data["session_control"] = {
                                            "should_end_session": True,
                                            "end_reason": f"workflow_completed:{wf_type}",
                                            "confidence": 0.9
                                        }
                                        debug_log(1, f"[LLM] ğŸ”š å·¥ä½œæµå®Œæˆï¼Œå·²è¨­ç½® session_control: {session_id}")
                                except Exception as e:
                                    error_log(f"[LLM] è¨­ç½® session_control æ™‚å‡ºéŒ¯: {e}")
                            elif next_step_is_interactive:
                                # ä¸‹ä¸€æ­¥éœ€è¦è¼¸å…¥ï¼šç”Ÿæˆæç¤º
                                next_prompt = next_step_info.get('prompt', 'Please provide input') if next_step_info else 'Please provide input'
                                follow_up_prompt = (
                                    f"{language_instruction}"
                                    f"The current step has been processed.\n"
                                    f"Next Step: User input required\n"
                                    f"Prompt: {next_prompt}\n\n"
                                    f"Generate a natural response that:\n"
                                    f"1. BRIEFLY acknowledges progress (1 sentence)\n"
                                    f"2. Asks the user for the needed input\n"
                                    f"3. Be friendly and conversational (2-3 sentences total)\n"
                                    f"IMPORTANT: Respond in English only."
                                )
                            else:
                                # é è¨­ï¼šç¢ºèªæ­¥é©Ÿå·²æ‰¹å‡†
                                follow_up_prompt = (
                                    f"{language_instruction}"
                                    f"The step has been approved and the workflow is continuing.\n"
                                    f"Result: {step_result.get('message', 'Success')}\n\n"
                                    f"Generate a brief, friendly acknowledgment that you're processing the request.\n"
                                    f"IMPORTANT: Respond in English only."
                                )
                        else:
                            # é workflow_step_response å ´æ™¯ï¼šä½¿ç”¨é è¨­å›æ‡‰
                            follow_up_prompt = (
                                f"{language_instruction}"
                                f"The step has been approved successfully.\n"
                                f"Result: {result_message}\n\n"
                                f"Please inform the user in a friendly tone that the process is continuing.\n"
                                f"IMPORTANT: Respond in English only."
                            )
                        # ä¸è·³éï¼Œä½¿ç”¨æ§‹å»ºçš„ follow_up_prompt
                    # âœ… å·¥ä½œæµå·²å•Ÿå‹•ï¼ˆæ–°çš„éåŒæ­¥æ¨¡å¼ï¼‰
                    elif workflow_status == "started":
                        # å·¥ä½œæµå·²å•Ÿå‹•ï¼Œæª¢æŸ¥æ˜¯å¦éœ€è¦ç”¨æˆ¶è¼¸å…¥
                        workflow_type = result_data.get("workflow_type", "task")
                        requires_input = result_data.get("requires_input", False)
                        current_step_prompt = result_data.get("current_step_prompt")
                        # ğŸ”§ auto_continue åœ¨åµŒå¥—çš„ data å­—å…¸ä¸­
                        workflow_data = result_data.get("data", {})
                        auto_continue = workflow_data.get("auto_continue", False)
                        
                        # ğŸ”§ ä¿®æ­£ï¼šrequires_input å„ªå…ˆæ–¼ auto_continue
                        # å³ä½¿ auto_continue=Trueï¼Œå¦‚æœç•¶å‰æ­¥é©Ÿéœ€è¦ç”¨æˆ¶è¼¸å…¥ï¼Œä¹Ÿå¿…é ˆç”Ÿæˆæç¤º
                        if requires_input and current_step_prompt:
                            # ç•¶å‰æ­¥é©Ÿéœ€è¦è¼¸å…¥ï¼šç”Ÿæˆæç¤ºè©¢å•ç”¨æˆ¶
                            follow_up_prompt = (
                                f"{language_instruction}"
                                f"The workflow '{workflow_type}' has been started.\n"
                                f"The current step requires user input.\n"
                                f"Prompt: {current_step_prompt}\n\n"
                                f"Generate a natural response that:\n"
                                f"1. BRIEFLY confirms the workflow has started (1 sentence)\n"
                                f"2. Asks the user for the needed input based on the prompt\n"
                                f"3. Be friendly and conversational (2-3 sentences total)\n"
                                f"IMPORTANT: Respond in English only."
                            )
                        elif auto_continue:
                            # ğŸ”§ å·¥ä½œæµæœƒè‡ªå‹•å®Œæˆï¼ˆæ‰€æœ‰æ­¥é©Ÿéƒ½æœƒè‡ªå‹•åŸ·è¡Œï¼Œç„¡éœ€è¼¸å…¥ï¼‰
                            # è·³éåˆå§‹å›æ‡‰ï¼Œç­‰å¾…å·¥ä½œæµå®Œæˆå¾Œå†ç”Ÿæˆç¸½çµ
                            debug_log(2, f"[LLM] å·¥ä½œæµæœƒè‡ªå‹•å®Œæˆ ({workflow_type})ï¼Œè·³éåˆå§‹å›æ‡‰ï¼Œç­‰å¾…å®Œæˆäº‹ä»¶")
                            skip_default_followup = True
                            response_text = ""  # ä¸è¼¸å‡ºåˆå§‹å›æ‡‰
                        else:
                            # å·¥ä½œæµè‡ªå‹•åŸ·è¡Œï¼ˆåƒæ•¸å·²æä¾›æˆ–ç„¡éœ€è¼¸å…¥ï¼‰
                            follow_up_prompt = (
                                f"{language_instruction}"
                                f"The workflow '{workflow_type}' has been started successfully.\n"
                                f"Result: {result_message}\n\n"
                                f"Please inform the user in a natural, friendly tone that you're processing their request and explain what will happen next (e.g., 'I'm checking the weather now').\n"
                                f"IMPORTANT: Respond in English only."
                            )
                    elif workflow_status == "completed":
                        # å·¥ä½œæµå·²å®Œæˆï¼ˆä¸€æ­¥åˆ°ä½ï¼ŒèˆŠæ¨¡å¼ï¼‰
                        follow_up_prompt = (
                            f"{language_instruction}"
                            f"The task has been completed successfully.\n"
                            f"Result: {result_message}\n\n"
                            f"Please inform the user in a friendly tone that the task is complete and briefly explain the result.\n"
                            f"IMPORTANT: Respond in English only."
                        )
                    else:
                        # å…¶ä»–æˆåŠŸç‹€æ…‹
                        follow_up_prompt = (
                            f"{language_instruction}"
                            f"The workflow is currently running.\n"
                            f"Status: {result_message}\n\n"
                            f"Please inform the user in a natural, friendly tone that you're processing their request and explain what will happen next.\n"
                            f"IMPORTANT: Respond in English only."
                        )
                else:
                    # å¤±æ•—ï¼šè®“ LLM è§£é‡‹éŒ¯èª¤ä¸¦æä¾›å»ºè­°
                    error_msg = function_call_result.get("error", "Unknown error")
                    follow_up_prompt = (
                        f"{language_instruction}"
                        f"An error occurred while processing the request.\n"
                        f"Error: {error_msg}\n\n"
                        f"Please explain the problem to the user in a friendly way and suggest how they can resolve it.\n"
                        f"IMPORTANT: Respond in English only."
                    )
                
                # æª¢æŸ¥æ˜¯å¦è·³éé è¨­ follow-upï¼ˆå·²åœ¨ç‰¹æ®Šè™•ç†ä¸­å®Œæˆï¼‰
                if not skip_default_followup:
                    debug_log(3, f"[LLM] ç™¼é€ follow-up prompt çµ¦ Gemini è™•ç†çµæœ")
                    
                    # ç¬¬äºŒæ¬¡èª¿ç”¨ Geminiï¼ˆä¸ä½¿ç”¨ toolsï¼Œåªè¦æ–‡æœ¬å›æ‡‰ï¼‰
                    follow_up_response = self.model.query(
                        follow_up_prompt,
                        mode="work",
                        tools=None  # ä¸éœ€è¦ toolsï¼Œåªè¦æ–‡æœ¬å›æ‡‰
                    )
                    
                    response_text = follow_up_response.get("text", result_message)
                else:
                    debug_log(3, f"[LLM] è·³éé è¨­ follow-upï¼ˆå·²åœ¨ç‰¹æ®Šè™•ç†ä¸­å®Œæˆï¼‰")
            else:
                response_text = response_data.get("text", "")
            
            # è™•ç† StatusManager æ›´æ–°
            if "status_updates" in response_data and response_data["status_updates"]:
                self._process_status_updates(response_data["status_updates"])
            
            # 4. è™•ç†SYSæ¨¡çµ„æ•´åˆ (WORKæ¨¡å¼) - åªåœ¨æ²’æœ‰ä½¿ç”¨ MCP function call æ™‚æ‰è™•ç†
            sys_actions = []
            if not function_call_result:
                sys_actions = self._process_work_system_actions(
                    llm_input, response_data, response_text
                )
            
            # 5. è™•ç†å­¸ç¿’ä¿¡è™Ÿ
            if self.learning_engine.learning_enabled:
                # è™•ç†æ–°çš„ç´¯ç©è©•åˆ†å­¸ç¿’ä¿¡è™Ÿ
                ctx = llm_input.identity_context or {}
                if "learning_signals" in response_data and response_data["learning_signals"]:
                    identity_id = (ctx.get("identity") or {}).get("id") or ctx.get("identity_id") or "default"
                    self.learning_engine.process_learning_signals(identity_id, response_data["learning_signals"])
                
                # ä¿ç•™èˆŠçš„äº’å‹•è¨˜éŒ„ï¼ˆç”¨æ–¼çµ±è¨ˆå’Œåˆ†æï¼‰
                identity_id = (ctx.get("identity") or {}).get("id") or ctx.get("identity_id") or "default"
                self.learning_engine.record_interaction(
                    identity_id=identity_id,
                    interaction_type="WORK",
                    user_input=llm_input.text,
                    system_response=response_text,
                    metadata={
                        "workflow_context": llm_input.workflow_context,
                        "system_context_used": bool(llm_input.system_context)
                    }
                )
            
            # 6. è™•ç†æœƒè©±æ§åˆ¶å»ºè­°
            session_control_result = self._process_session_control(
                response_data, "WORK", llm_input
            )
            
            # æå– sys_action
            sys_action_obj = None
            if sys_actions and len(sys_actions) > 0:
                sys_action_obj = SystemAction(**sys_actions[0])
            
            output = LLMOutput(
                text=response_text,
                processing_time=time.time() - start_time,
                tokens_used=len(response_text.split()),
                success=True,
                error=None,
                confidence=response_data.get("confidence", 0.90),
                sys_action=sys_action_obj,
                status_updates=StatusUpdate(**response_data["status_updates"]) if response_data.get("status_updates") else None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={
                    "mode": "WORK",
                    "workflow_context_size": len(llm_input.workflow_context) if llm_input.workflow_context else 0,
                    "sys_actions_count": len(sys_actions),
                    "sys_actions": sys_actions,
                    "system_context_size": len(llm_input.system_context) if llm_input.system_context else 0,
                    "session_control": session_control_result,
                    "function_call_made": function_call_result is not None,  # âœ… æ¨™è¨˜æ˜¯å¦èª¿ç”¨äº† MCP function
                    "function_call_result": function_call_result if function_call_result else None
                }
            )
            
            # ç™¼å¸ƒ LLM å›æ‡‰ç”Ÿæˆäº‹ä»¶
            self._publish_llm_response_event(output, "WORK", {
                "workflow_context": bool(llm_input.workflow_context),
                "function_call_made": function_call_result is not None,
                "tool_name": function_call_result.get("tool_name") if function_call_result else None
            })
            
            return output
            
        except Exception as e:
            import traceback
            error_log(f"[LLM] WORK æ¨¡å¼è™•ç†éŒ¯èª¤: {e}")
            error_log(f"[LLM] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
            return LLMOutput(
                text="å·¥ä½œä»»å‹™è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                processing_time=time.time() - start_time,
                tokens_used=0,
                success=False,
                error=str(e),
                confidence=0.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={"mode": "WORK", "error_type": "processing_error"}
            )
    
    def _handle_legacy_mode(self, llm_input: "LLMInput", status: Dict[str, Any]) -> "LLMOutput":
        """è™•ç†èˆŠçš„ intent ç³»çµ±ä»¥ä¿æŒå‘å¾Œå…¼å®¹"""
        start_time = time.time()
        debug_log(2, f"[LLM] è™•ç†èˆŠç‰ˆ intent: {getattr(llm_input, 'intent', 'unknown')}")
        
        # å°‡èˆŠçš„ intent è½‰æ›ç‚ºæ–°çš„æ¨¡å¼
        legacy_intent = getattr(llm_input, 'intent', 'chat')
        
        if legacy_intent == "chat":
            # è½‰ç‚º CHAT æ¨¡å¼
            llm_input.mode = LLMMode.CHAT
            return self._handle_chat_mode(llm_input, status)
        elif legacy_intent == "command":
            # è½‰ç‚º WORK æ¨¡å¼
            llm_input.mode = LLMMode.WORK
            return self._handle_work_mode(llm_input, status)
        else:
            return LLMOutput(
                text=f"æŠ±æ­‰ï¼Œç›®å‰æš«ä¸æ”¯æ´ '{legacy_intent}' é¡å‹çš„è™•ç†ã€‚",
                processing_time=time.time() - start_time,
                tokens_used=0,
                success=False,
                error=f"ä¸æ”¯æ´çš„ intent: {legacy_intent}",
                confidence=0.0,
                sys_action=None,
                status_updates=None,
                learning_data=None,
                conversation_entry=None,
                session_state=None,
                memory_observation=None,
                memory_summary=None,
                emotion="neutral",
                mood="neutral",
                metadata={"legacy_intent": legacy_intent}
            )
    
    def _analyze_system_action(self, response_text: str, workflow_context: Optional[Dict[str, Any]]) -> Optional["SystemAction"]:
        """
        [DEPRECATED] åˆ†æå›æ‡‰æ–‡æœ¬æ˜¯å¦éœ€è¦ç³»çµ±å‹•ä½œ
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•å·²å»¢æ£„ã€‚æ ¹æ“š U.E.P æ¶æ§‹è¨­è¨ˆï¼š
        1. æ„åœ–åˆ†ææ‡‰è©²åœ¨ NLP æ¨¡çµ„éšæ®µå®Œæˆ
        2. LLM åœ¨ WORK æ¨¡å¼ä¸‹æ‡‰è©²å¾ Gemini çµæ§‹åŒ–å›æ‡‰ä¸­ç²å–ç³»çµ±å‹•ä½œ
        3. ä¸æ‡‰è©²é‡è¤‡åˆ†ææ–‡æœ¬ä¾†åˆ¤æ–·ç³»çµ±åŠŸèƒ½éœ€æ±‚
        
        æ­¤æ–¹æ³•ä¿ç•™åƒ…ç”¨æ–¼å‘å¾Œå…¼å®¹ï¼Œå»ºè­°ç§»é™¤å°æ­¤æ–¹æ³•çš„èª¿ç”¨ã€‚
        """
        debug_log(3, "[LLM] è­¦å‘Šï¼šä½¿ç”¨äº†å·²å»¢æ£„çš„ _analyze_system_action æ–¹æ³•")
        return None
    
    def _on_status_update(self, status_type: str, old_value: float, new_value: float, reason: str = ""):
        """StatusManager ç‹€æ…‹æ›´æ–°å›èª¿"""
        debug_log(2, f"[LLM] ç³»çµ±ç‹€æ…‹æ›´æ–° - {status_type}: {old_value} -> {new_value} ({reason})")
        
        # æ ¹æ“šç‹€æ…‹è®ŠåŒ–èª¿æ•´ LLM è¡Œç‚º
        if status_type == "mood" and new_value < 0.3:
            debug_log(1, "[LLM] åµæ¸¬åˆ°ç³»çµ±å¿ƒæƒ…ä½è½ï¼Œèª¿æ•´å›æ‡‰é¢¨æ ¼")
        elif status_type == "boredom" and new_value > 0.8:
            debug_log(1, "[LLM] åµæ¸¬åˆ°ç³»çµ±ç„¡èŠï¼Œå»ºè­°ä¸»å‹•äº’å‹•")
    
    def shutdown(self):
        """é—œé–‰ LLM æ¨¡çµ„ä¸¦ä¿å­˜ç‹€æ…‹"""
        try:
            info_log("[LLM] LLM æ¨¡çµ„é—œé–‰ä¸­...")
            
            # ä¿å­˜å­¸ç¿’è³‡æ–™
            if self.learning_engine:
                self.learning_engine.save_learning_data()
                debug_log(2, "[LLM] å­¸ç¿’è³‡æ–™å·²ä¿å­˜")
            
            # æ¸…ç† Context Cache
            if self.cache_manager:
                cache_stats = self.cache_manager.get_cache_statistics()
                debug_log(2, f"[LLM] Cache çµ±è¨ˆ: {cache_stats}")
                
            # å–æ¶ˆ StatusManager å›èª¿
            self.status_manager.unregister_update_callback("llm_module")
            
            info_log("[LLM] LLM æ¨¡çµ„é‡æ§‹ç‰ˆé—œé–‰å®Œæˆ")
            
        except Exception as e:
            error_log(f"[LLM] é—œé–‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def get_module_status(self) -> Dict[str, Any]:
        """ç²å–æ¨¡çµ„ç‹€æ…‹è³‡è¨Š"""
        try:
            status = {
                "initialized": self.is_initialized,
                "model_status": "active" if self.model else "inactive",
                "learning_enabled": self.learning_engine.learning_enabled if self.learning_engine else False,
                "cache_enabled": self.cache_manager is not None,
            }
            
            if self.cache_manager:
                status["cache_stats"] = self.cache_manager.get_cache_statistics()
                
            if self.learning_engine and self.learning_engine.learning_enabled:
                status["learning_stats"] = {
                    "total_interactions": len(self.learning_engine.interaction_history),
                    "conversation_styles": len(self.learning_engine.conversation_styles),
                    "usage_patterns": len(self.learning_engine.usage_patterns)
                }
                
            return status
            
        except Exception as e:
            error_log(f"[LLM] ç²å–æ¨¡çµ„ç‹€æ…‹å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _process_status_updates(self, status_updates) -> None:
        """
        è™•ç†ä¾†è‡ªLLMå›æ‡‰çš„StatusManageræ›´æ–°
        æ”¯æ´ç‰©ä»¶æ ¼å¼ï¼ˆä¾†è‡ª schemaï¼‰å’Œé™£åˆ—æ ¼å¼ï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰
        """
        try:
            if not status_updates:
                return
            
            # è™•ç†ç‰©ä»¶æ ¼å¼ï¼ˆä¾†è‡ª Gemini schemaï¼‰
            if isinstance(status_updates, dict):
                # ä½¿ç”¨ StatusManager çš„å°ˆç”¨ delta æ›´æ–°æ–¹æ³•
                if "mood_delta" in status_updates and status_updates["mood_delta"] is not None:
                    self.status_manager.update_mood(status_updates["mood_delta"], "LLMæƒ…ç·’åˆ†æ")
                    debug_log(2, f"[LLM] Mood æ›´æ–°: += {status_updates['mood_delta']}")
                
                if "pride_delta" in status_updates and status_updates["pride_delta"] is not None:
                    self.status_manager.update_pride(status_updates["pride_delta"], "LLMæƒ…ç·’åˆ†æ")  
                    debug_log(2, f"[LLM] Pride æ›´æ–°: += {status_updates['pride_delta']}")
                
                if "helpfulness_delta" in status_updates and status_updates["helpfulness_delta"] is not None:
                    self.status_manager.update_helpfulness(status_updates["helpfulness_delta"], "LLMæƒ…ç·’åˆ†æ")
                    debug_log(2, f"[LLM] Helpfulness æ›´æ–°: += {status_updates['helpfulness_delta']}")
                
                if "boredom_delta" in status_updates and status_updates["boredom_delta"] is not None:
                    self.status_manager.update_boredom(status_updates["boredom_delta"], "LLMæƒ…ç·’åˆ†æ")
                    debug_log(2, f"[LLM] Boredom æ›´æ–°: += {status_updates['boredom_delta']}")
                
                # çµ±è¨ˆæ›´æ–°æ¬¡æ•¸
                updates_count = sum(1 for key in ["mood_delta", "pride_delta", "helpfulness_delta", "boredom_delta"] 
                                  if key in status_updates and status_updates[key] is not None)
                if updates_count > 0:
                    debug_log(1, f"[LLM] StatusManager å·²æ‡‰ç”¨ {updates_count} å€‹ç‹€æ…‹æ›´æ–°")
            
            # è™•ç†é™£åˆ—æ ¼å¼ï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰
            elif isinstance(status_updates, list):
                for update in status_updates:
                    status_type = update.get("status_type")
                    value = update.get("value") 
                    reason = update.get("reason", "LLMå›æ‡‰è§¸ç™¼")
                    
                    if status_type and value is not None:
                        # ä½¿ç”¨å°æ‡‰çš„å°ˆç”¨æ›´æ–°æ–¹æ³•
                        try:
                            if status_type == "mood":
                                self.status_manager.update_mood(value, reason)
                            elif status_type == "pride":
                                self.status_manager.update_pride(value, reason)
                            elif status_type == "helpfulness":
                                self.status_manager.update_helpfulness(value, reason)
                            elif status_type == "boredom":
                                self.status_manager.update_boredom(value, reason)
                            else:
                                debug_log(1, f"[LLM] æœªçŸ¥çš„ç‹€æ…‹é¡å‹: {status_type}")
                                continue
                            
                            debug_log(2, f"[LLM] StatusManageræ›´æ–°æˆåŠŸ: {status_type}+={value}, åŸå› : {reason}")
                        except Exception as e:
                            debug_log(1, f"[LLM] StatusManageræ›´æ–°å¤±æ•—: {status_type}={value}, éŒ¯èª¤: {e}")
                        
        except Exception as e:
            error_log(f"[LLM] è™•ç†StatusManageræ›´æ–°æ™‚å‡ºéŒ¯: {e}")
    
    def _get_current_system_status(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹"""
        try:
            # ğŸ”§ æ·»åŠ  None æª¢æŸ¥ï¼Œé˜²æ­¢ 'NoneType' object is not subscriptable éŒ¯èª¤
            status_dict = self.status_manager.get_status_dict()
            if status_dict is None:
                debug_log(1, "[LLM] status_manager.get_status_dict() è¿”å› Noneï¼Œä½¿ç”¨é è¨­å€¼")
                status_dict = {}
            
            personality_modifiers = self.status_manager.get_personality_modifiers()
            if personality_modifiers is None:
                debug_log(1, "[LLM] status_manager.get_personality_modifiers() è¿”å› Noneï¼Œä½¿ç”¨é è¨­å€¼")
                personality_modifiers = {}
            
            return {
                "status_values": status_dict,
                "personality_modifiers": personality_modifiers,
                "system_mode": self.state_manager.get_current_state().value
            }
        except Exception as e:
            error_log(f"[LLM] ç²å–ç³»çµ±ç‹€æ…‹å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _get_current_gs_id(self) -> str:
        """
        ç²å–ç•¶å‰ General Session ID
        å¾ working_context çš„å…¨å±€æ•¸æ“šä¸­è®€å– (ç”± SystemLoop è¨­ç½®)
        
        Returns:
            str: ç•¶å‰ GS ID,å¦‚æœç„¡æ³•ç²å–å‰‡è¿”å› 'unknown'
        """
        try:
            from core.working_context import working_context_manager
            gs_id = working_context_manager.global_context_data.get('current_gs_id', 'unknown')
            return gs_id
        except Exception as e:
            error_log(f"[LLM] ç²å– GS ID å¤±æ•—: {e}")
            return 'unknown'
    
    def _get_current_cycle_index(self) -> int:
        """
        ç²å–ç•¶å‰å¾ªç’°è¨ˆæ•¸
        å¾ working_context çš„å…¨å±€æ•¸æ“šä¸­è®€å– (ç”± Controller åœ¨ GS å‰µå»ºæ™‚è¨­ç½®)
        
        Returns:
            int: ç•¶å‰ cycle_index,å¦‚æœç„¡æ³•ç²å–å‰‡è¿”å› 0ï¼ˆå‡è¨­ç‚ºç¬¬ä¸€å€‹ cycleï¼‰
        """
        try:
            from core.working_context import working_context_manager
            cycle_index = working_context_manager.global_context_data.get('current_cycle_index', 0)
            return cycle_index
        except Exception as e:
            error_log(f"[LLM] ç²å– cycle_index å¤±æ•—: {e}")
            return 0
    
    def _get_current_session_info(self, workflow_session_id: Optional[str] = None) -> Dict[str, Any]:
        """ç²å–ç•¶å‰æœƒè©±ä¿¡æ¯ - å„ªå…ˆç²å– CS æˆ– WSï¼ˆLLM ä½œç‚ºé‚è¼¯ä¸­æ¨çš„åŸ·è¡Œæœƒè©±ï¼‰
        
        Args:
            workflow_session_id: å¯é¸çš„æŒ‡å®šå·¥ä½œæµæœƒè©±IDï¼Œå¦‚æœæä¾›å‰‡å„ªå…ˆè¿”å›è©²æœƒè©±çš„ä¿¡æ¯
        """
        try:
            # å¾çµ±ä¸€æœƒè©±ç®¡ç†å™¨ç²å–æœƒè©±ä¿¡æ¯
            from core.sessions.session_manager import session_manager
            
            # å¦‚æœæŒ‡å®šäº† workflow_session_idï¼Œå„ªå…ˆç²å–è©²ç‰¹å®šæœƒè©±
            if workflow_session_id:
                current_ws = session_manager.get_workflow_session(workflow_session_id)
                if current_ws:
                    debug_log(2, f"[LLM] ä½¿ç”¨æŒ‡å®šçš„å·¥ä½œæµæœƒè©±: {workflow_session_id}")
                    return {
                        "session_id": workflow_session_id,
                        "session_type": "workflow",
                        "start_time": getattr(current_ws, 'start_time', None),
                        "interaction_count": getattr(current_ws, 'step_count', 0),
                        "last_activity": getattr(current_ws, 'last_activity', None),
                        "active_session_type": "WS"
                    }
            
            # LLM åœ¨ CHAT ç‹€æ…‹æ™‚æ‡‰è©²ç²å–ç•¶å‰ CS
            active_cs_ids = session_manager.get_active_chatting_session_ids()
            if active_cs_ids:
                # åœ¨æ¶æ§‹ä¸‹ï¼ŒåŒä¸€æ™‚é–“åªæœƒæœ‰ä¸€å€‹ CS åŸ·è¡Œä¸­
                current_cs_id = active_cs_ids[0]
                current_cs = session_manager.get_chatting_session(current_cs_id)
                
                if current_cs:
                    return {
                        "session_id": current_cs_id,
                        "session_type": "chatting",
                        "start_time": getattr(current_cs, 'start_time', None),
                        "interaction_count": getattr(current_cs, 'turn_count', 0),
                        "last_activity": getattr(current_cs, 'last_activity', None),
                        "active_session_type": "CS"
                    }
            
            # LLM åœ¨ WORK ç‹€æ…‹æ™‚æ‡‰è©²ç²å–ç•¶å‰ WS
            active_ws_ids = session_manager.get_active_workflow_session_ids()
            if active_ws_ids:
                # åœ¨æ¶æ§‹ä¸‹ï¼ŒåŒä¸€æ™‚é–“åªæœƒæœ‰ä¸€å€‹ WS åŸ·è¡Œä¸­
                current_ws_id = active_ws_ids[0]
                current_ws = session_manager.get_workflow_session(current_ws_id)
                
                if current_ws:
                    return {
                        "session_id": current_ws_id,
                        "session_type": "workflow",
                        "start_time": getattr(current_ws, 'start_time', None),
                        "interaction_count": getattr(current_ws, 'step_count', 0),
                        "last_activity": getattr(current_ws, 'last_activity', None),
                        "active_session_type": "WS"
                    }
            
            # å¦‚æœæ²’æœ‰ CS æˆ– WSï¼Œå¯èƒ½ç³»çµ±è™•æ–¼ IDLE ç‹€æ…‹æˆ–å…¶ä»–ç‹€æ…‹
            return {
                "session_id": "no_active_session", 
                "session_type": "idle",
                "start_time": None,
                "interaction_count": 0,
                "last_activity": None,
                "active_session_type": "NONE"
            }
            
        except Exception as e:
            error_log(f"[LLM] ç²å–æœƒè©±ä¿¡æ¯å¤±æ•—: {e}")
            return {
                "session_id": "error", 
                "session_type": "error",
                "active_session_type": "ERROR"
            }
    
    def _get_identity_context(self) -> Dict[str, Any]:
        """å¾Working Contextç²å–Identityä¿¡æ¯ï¼Œå°é€šç”¨èº«ä»½æ¡ç”¨é è¨­è™•ç†"""
        try:
            # ä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•ç²å–ç•¶å‰èº«ä»½
            identity_data = working_context_manager.get_current_identity()
            
            if not identity_data:
                debug_log(2, "[LLM] æ²’æœ‰è¨­ç½®èº«ä»½ä¿¡æ¯ï¼Œä½¿ç”¨é è¨­å€¼")
                return {
                    "identity": {
                        "name": "default_user",
                        "traits": {}
                    },
                    "preferences": {}
                }
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé€šç”¨èº«ä»½
            identity_status = identity_data.get("status", "unknown")
            if identity_status == "temporary":
                debug_log(2, "[LLM] æª¢æ¸¬åˆ°é€šç”¨èº«ä»½ï¼Œä½¿ç”¨åŸºæœ¬è¨­ç½®")
                return {
                    "identity": {
                        "name": "ç”¨æˆ¶",
                        "traits": {},
                        "status": "temporary"
                    },
                    "preferences": {}  # é€šç”¨èº«ä»½ä¸ä½¿ç”¨ç‰¹æ®Šåå¥½
                }
            
            # æ­£å¼èº«ä»½ä½¿ç”¨å®Œæ•´è³‡æ–™
            return {
                "identity": {
                    "name": identity_data.get("user_identity", identity_data.get("identity_id", "default_user")),
                    "traits": identity_data.get("traits", {}),
                    "status": identity_status
                },
                "preferences": identity_data.get("conversation_preferences", {})
            }
        except Exception as e:
            error_log(f"[LLM] ç²å–Identityä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return {}
    
    def _enrich_with_system_context(self, 
                                  llm_input: LLMInput,
                                  current_state: Any,
                                  status: Dict[str, Any],
                                  session_info: Dict[str, Any],
                                  identity_context: Dict[str, Any]) -> LLMInput:
        """è£œå……ç³»çµ±ä¸Šä¸‹æ–‡åˆ°LLMè¼¸å…¥ - æ”¯æ´æ–° Router æ•´åˆ"""
        try:
            # å‰µå»ºæ–°çš„enriched input
            enriched_data = llm_input.dict()
            
            # è£œå……ç³»çµ±ä¸Šä¸‹æ–‡
            if not enriched_data.get("system_context"):
                enriched_data["system_context"] = {}
            
            enriched_data["system_context"].update({
                "current_state": current_state.value if hasattr(current_state, 'value') else str(current_state),
                "status_manager": status,
                "session_info": session_info
            })
            
            # è£œå……èº«ä»½ä¸Šä¸‹æ–‡ (ä¸è¦†è“‹Routeræä¾›çš„)
            if not enriched_data.get("identity_context"):
                enriched_data["identity_context"] = {}
            # åªåœ¨æ²’æœ‰Routeræ•¸æ“šæ™‚è£œå……æœ¬åœ°èº«ä»½ä¸Šä¸‹æ–‡
            if not llm_input.source_layer:
                enriched_data["identity_context"].update(identity_context)
            
            # è™•ç†æ–°Routeræä¾›çš„å”ä½œä¸Šä¸‹æ–‡
            if llm_input.collaboration_context:
                debug_log(2, f"[LLM] è™•ç†å”ä½œä¸Šä¸‹æ–‡: {list(llm_input.collaboration_context.keys())}")
                
                # è¨­ç½®è¨˜æ†¶æª¢ç´¢æ¨™èªŒ
                if "mem" in llm_input.collaboration_context:
                    enriched_data["enable_memory_retrieval"] = True
                    mem_config = llm_input.collaboration_context["mem"]
                    if mem_config.get("retrieve_relevant"):
                        enriched_data["memory_context"] = "å”ä½œæ¨¡å¼ï¼šéœ€è¦æª¢ç´¢ç›¸é—œè¨˜æ†¶"
                
                # è¨­ç½®ç³»çµ±å‹•ä½œæ¨™èªŒ
                if "sys" in llm_input.collaboration_context:
                    enriched_data["enable_system_actions"] = True
                    sys_config = llm_input.collaboration_context["sys"]
                    if sys_config.get("allow_execution"):
                        enriched_data["workflow_context"] = {"execution_allowed": True}
            
            # è™•ç†Routerçš„æœƒè©±ä¸Šä¸‹æ–‡
            if llm_input.session_context:
                enriched_data["session_id"] = llm_input.session_context.get("session_id")
                enriched_data["system_context"]["router_session"] = llm_input.session_context
            
            # è™•ç†NLPå¯¦é«”ä¿¡æ¯
            if llm_input.entities:
                enriched_data["system_context"]["nlp_entities"] = llm_input.entities
            
            return LLMInput(**enriched_data)
            
        except Exception as e:
            error_log(f"[LLM] è£œå……ç³»çµ±ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return llm_input
    
    def _process_chat_memory_operations(self, 
                                      llm_input: LLMInput,
                                      response_data: Dict[str, Any], 
                                      response_text: str) -> List[Dict[str, Any]]:
        """è™•ç†CHATæ¨¡å¼çš„MEMæ¨¡çµ„æ“ä½œ"""
        memory_operations = []
        
        try:
            # 1. å¾Geminiå›æ‡‰ä¸­æå–è¨˜æ†¶æ“ä½œ
            if "memory_operations" in response_data:
                memory_operations.extend(response_data["memory_operations"])
                debug_log(2, f"[LLM] å¾å›æ‡‰æå–è¨˜æ†¶æ“ä½œ: {len(memory_operations)}å€‹")
            
            # 2. è‡ªå‹•è¨˜æ†¶å„²å­˜é‚è¼¯
            if self._should_store_conversation(llm_input, response_text):
                store_operation = {
                    "operation": "store",
                    "content": {
                        "user_input": llm_input.text,
                        "assistant_response": response_text,
                        "timestamp": time.time(),
                        "conversation_context": llm_input.memory_context,
                        "identity_context": llm_input.identity_context
                    },
                    "metadata": {
                        "interaction_type": "chat",
                        "memory_type": "conversation",
                        "auto_generated": True,
                        "ttl_seconds": 60 * 60 * 24 * 7,     # ä¸€é€±
                        "erasable": True
                    }
                }
                memory_operations.append(store_operation)
                debug_log(2, "[LLM] è‡ªå‹•æ·»åŠ å°è©±è¨˜æ†¶å„²å­˜")
            
            # 3. ç™¼é€è¨˜æ†¶æ“ä½œåˆ°MEMæ¨¡çµ„ (é€šéRouter)
            if memory_operations:
                self._send_to_mem_module(memory_operations)
            
            return memory_operations
            
        except Exception as e:
            error_log(f"[LLM] è™•ç†CHATè¨˜æ†¶æ“ä½œå¤±æ•—: {e}")
            return []
    
    def _should_store_conversation(self, llm_input: LLMInput, response_text: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²å„²å­˜å°è©±"""
        try:
            # æª¢æŸ¥å°è©±é•·åº¦
            if len(llm_input.text) < 10 or len(response_text) < 10:
                return False
            
            sensitive_patterns = [r"\b\d{10}\b", r"@.+\.", r"\b[A-Z]\d{9}\b"]  # å¯æ“´å……
            if any(re.search(p, llm_input.text) for p in sensitive_patterns):
                return False  # å«æ•æ„Ÿè³‡è¨Šä¸è‡ªå‹•å­˜
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¦å°è©±
            important_keywords = ["remember", "record", "important", "remind", "save"]
            if any(keyword in llm_input.text for keyword in important_keywords):
                return True
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å€‹äººä¿¡æ¯æˆ–åå¥½
            personal_keywords = ["like", "hate", "prefer", "would have", "name", "birthday"]
            if any(keyword in llm_input.text for keyword in personal_keywords):
                return True
            
            # é è¨­å„²å­˜è¼ƒé•·çš„æœ‰æ„ç¾©å°è©±
            return len(llm_input.text) > 50
            
        except Exception as e:
            error_log(f"[LLM] åˆ¤æ–·å°è©±å„²å­˜å¤±æ•—: {e}")
            return False
    
    def _send_to_mem_module(self, memory_operations: List[Dict[str, Any]]) -> None:
        """å‘MEMæ¨¡çµ„ç™¼é€è¨˜æ†¶æ“ä½œ - é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£"""
        try:
            debug_log(1, f"[LLM] æº–å‚™ç™¼é€ {len(memory_operations)} å€‹è¨˜æ†¶æ“ä½œåˆ°MEMæ¨¡çµ„")
            
            # æª¢æŸ¥ CHAT-MEM å”ä½œç®¡é“æ˜¯å¦å•Ÿç”¨
            if not self.module_interface.is_channel_active(CollaborationChannel.CHAT_MEM):
                debug_log(2, "[LLM] è¨˜æ†¶æ“ä½œè·³é: MEMæ¨¡çµ„åªåœ¨CHATç‹€æ…‹ä¸‹é‹è¡Œ")
                return
            
            # é€å€‹è™•ç†è¨˜æ†¶æ“ä½œ
            for i, operation in enumerate(memory_operations):
                operation_type = operation.get('operation', 'unknown')
                debug_log(3, f"[LLM] è¨˜æ†¶æ“ä½œ #{i+1}: {operation_type}")
                
                try:
                    # é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£ç™¼é€å°è©±å„²å­˜è«‹æ±‚
                    conversation_data = {
                        "operation_type": operation_type,
                        "content": operation.get('content', {}),
                        "metadata": operation.get('metadata', {}),
                        "source_module": "llm_module"
                    }
                    
                    result = self.module_interface.get_chat_mem_data(
                        "conversation_storage",
                        conversation_data=conversation_data
                    )
                    
                    if result:
                        debug_log(2, f"[LLM] è¨˜æ†¶æ“ä½œ #{i+1} æˆåŠŸ: {operation_type}")
                    else:
                        debug_log(2, f"[LLM] è¨˜æ†¶æ“ä½œ #{i+1} æœªåŸ·è¡Œ: {operation_type}")
                        
                except Exception as op_error:
                    error_log(f"[LLM] è™•ç†è¨˜æ†¶æ“ä½œ #{i+1} æ™‚å‡ºéŒ¯: {op_error}")
            
        except Exception as e:
            error_log(f"[LLM] ç™¼é€è¨˜æ†¶æ“ä½œå¤±æ•—: {e}")
    
    def _retrieve_relevant_memory(self, user_input: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """å¾MEMæ¨¡çµ„æª¢ç´¢ç›¸é—œè¨˜æ†¶ - é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£"""
        try:
            debug_log(2, f"[LLM] æª¢ç´¢ç›¸é—œè¨˜æ†¶: {user_input[:50]}...")
            
            # æª¢æŸ¥ CHAT-MEM å”ä½œç®¡é“æ˜¯å¦å•Ÿç”¨
            if not self.module_interface.is_channel_active(CollaborationChannel.CHAT_MEM):
                debug_log(2, "[LLM] è¨˜æ†¶æª¢ç´¢å¤±æ•—: MEMæ¨¡çµ„åªåœ¨CHATç‹€æ…‹ä¸‹é‹è¡Œ")
                return []
            
            # é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£æª¢ç´¢è¨˜æ†¶
            memories = self.module_interface.get_chat_mem_data(
                "memory_retrieval",
                query=user_input,
                max_results=max_results,
                memory_types=["conversation", "user_info", "context"]
            )
            
            if memories:
                debug_log(1, f"[LLM] æª¢ç´¢åˆ° {len(memories)} æ¢ç›¸é—œè¨˜æ†¶")
                return memories
            else:
                debug_log(2, "[LLM] æœªæª¢ç´¢åˆ°ç›¸é—œè¨˜æ†¶")
                return []
            
        except Exception as e:
            error_log(f"[LLM] è¨˜æ†¶æª¢ç´¢å¤±æ•—: {e}")
            return []
    
    def _format_memories_for_context(self, memories: List[Any]) -> str:
        """å°‡æª¢ç´¢åˆ°çš„è¨˜æ†¶æ ¼å¼åŒ–ç‚ºå°è©±ä¸Šä¸‹æ–‡
        
        Args:
            memories: MemorySearchResult å°è±¡åˆ—è¡¨
        """
        try:
            if not memories:
                return ""
            
            context_parts = ["Relevant Memory Context:"]
            
            for i, memory_result in enumerate(memories[:5], 1):  # é™åˆ¶æœ€å¤š5æ¢è¨˜æ†¶
                # å¾ MemorySearchResult ä¸­å–å¾— memory_entry
                memory_entry = memory_result.memory_entry
                
                memory_type = memory_entry.memory_type.value  # MemoryType enum
                content = memory_entry.content
                timestamp = memory_entry.created_at.strftime("%Y-%m-%d %H:%M") if memory_entry.created_at else ""
                
                # æ ¼å¼åŒ–è¨˜æ†¶å…§å®¹
                if memory_type == "interaction_history":
                    # å°è©±è¨˜æ†¶æ ¼å¼
                    context_parts.append(f"{i}. [Conversation] {content}")
                elif memory_type == "profile":
                    # ç”¨æˆ¶ä¿¡æ¯è¨˜æ†¶æ ¼å¼
                    context_parts.append(f"{i}. [User Info] {content}")
                elif memory_type == "snapshot":
                    # å¿«ç…§è¨˜æ†¶æ ¼å¼
                    context_parts.append(f"{i}. [Recent Context] {content}")
                else:
                    # ä¸€èˆ¬è¨˜æ†¶æ ¼å¼
                    context_parts.append(f"{i}. [{memory_type.replace('_', ' ').title()}] {content}")
            
            formatted_context = "\n".join(context_parts)
            debug_log(3, f"[LLM] æ ¼å¼åŒ–è¨˜æ†¶ä¸Šä¸‹æ–‡: {len(formatted_context)} å­—ç¬¦")
            
            return formatted_context
            
        except Exception as e:
            error_log(f"[LLM] æ ¼å¼åŒ–è¨˜æ†¶ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            import traceback
            debug_log(1, traceback.format_exc())
            return ""
    
    def _process_work_system_actions(self, 
                                   llm_input: LLMInput,
                                   response_data: Dict[str, Any], 
                                   response_text: str) -> List[Dict[str, Any]]:
        """è™•ç†WORKæ¨¡å¼çš„SYSæ¨¡çµ„æ“ä½œ - LLMä½œç‚ºæ±ºç­–æ©Ÿ"""
        sys_actions = []
        
        try:
            # å¾Geminiå›æ‡‰ä¸­æå–ç³»çµ±å‹•ä½œæ±ºç­–
            if "sys_action" in response_data:
                sys_action = response_data["sys_action"]
                if isinstance(sys_action, dict):
                    sys_actions.append(sys_action)
                    action = sys_action.get('action', 'unknown')
                    target = sys_action.get('target', 'unknown')
                    debug_log(1, f"[LLM] æ±ºç­–: {action} -> {target}")
                    
                    # ğŸ”§ è™•ç† MCP å·¥å…·èª¿ç”¨ï¼ˆå·¥ä½œæµæ§åˆ¶ï¼‰
                    if action == 'execute_function' and target in ['approve_step', 'cancel_workflow', 'modify_step']:
                        debug_log(2, f"[LLM] æª¢æ¸¬åˆ° MCP å·¥å…·èª¿ç”¨: {target}")
                        
                        # ğŸ”§ å¾å·¥ä½œæµä¸Šä¸‹æ–‡ç²å– workflow_session_idï¼ˆæ³¨æ„æ¬„ä½åï¼‰
                        session_id = None
                        if llm_input.workflow_context:
                            session_id = llm_input.workflow_context.get('workflow_session_id')  # æ­£ç¢ºçš„æ¬„ä½å
                        
                        if not session_id:
                            error_log(f"[LLM] ç„¡æ³•åŸ·è¡Œ {target}: ç¼ºå°‘ workflow_session_id")
                            debug_log(1, f"[LLM] workflow_context keys: {list(llm_input.workflow_context.keys()) if llm_input.workflow_context else 'None'}")
                        else:
                            # åŸ·è¡Œ MCP å·¥å…·
                            if target == 'approve_step':
                                debug_log(2, f"[LLM] åŸ·è¡Œ approve_step: {session_id}")
                                self._approve_workflow_step(session_id, None)
                            elif target == 'cancel_workflow':
                                reason = sys_action.get('parameters', {}).get('reason', 'User cancelled')
                                self._cancel_workflow(session_id, reason)
                            elif target == 'modify_step':
                                modifications = sys_action.get('parameters', {})
                                self._modify_workflow_step(session_id, modifications)
            
            # ç™¼é€å…¶ä»–ç³»çµ±å‹•ä½œåˆ° SYS æ¨¡çµ„
            non_mcp_actions = [a for a in sys_actions if not (a.get('action') == 'execute_function' and a.get('target') in ['approve_step', 'cancel_workflow', 'modify_step'])]
            if non_mcp_actions:
                self._send_to_sys_module(non_mcp_actions, llm_input.workflow_context)
            
            return sys_actions
            
        except Exception as e:
            error_log(f"[LLM] è™•ç†WORKç³»çµ±å‹•ä½œå¤±æ•—: {e}")
            return []
    
    def _get_available_sys_functions(self) -> Optional[List[Dict[str, Any]]]:
        """å¾ SYS æ¨¡çµ„ç²å–å¯ç”¨åŠŸèƒ½æ¸…å–®"""
        try:
            debug_log(2, "[LLM] å˜—è©¦å¾ SYS æ¨¡çµ„ç²å–åŠŸèƒ½æ¸…å–®")
            
            # æª¢æŸ¥ WORK-SYS å”ä½œç®¡é“æ˜¯å¦å•Ÿç”¨
            if not self.module_interface.is_channel_active(CollaborationChannel.WORK_SYS):
                debug_log(2, "[LLM] SYS å”ä½œç®¡é“æœªå•Ÿç”¨ï¼Œè¿”å›ç©ºåŠŸèƒ½æ¸…å–®")
                return None
            
            # é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£ç²å–åŠŸèƒ½è¨»å†Šè¡¨
            function_registry = self.module_interface.get_work_sys_data(
                "function_registry",
                request_type="get_all_functions"
            )
            
            if function_registry and isinstance(function_registry, list):
                debug_log(1, f"[LLM] æˆåŠŸç²å– {len(function_registry)} å€‹å¯ç”¨åŠŸèƒ½")
                return function_registry
            elif function_registry and isinstance(function_registry, dict):
                # è™•ç†å­—å…¸æ ¼å¼çš„åŠŸèƒ½è¨»å†Šè¡¨
                functions = []
                for category, funcs in function_registry.items():
                    if isinstance(funcs, list):
                        functions.extend(funcs)
                debug_log(1, f"[LLM] æˆåŠŸç²å– {len(functions)} å€‹å¯ç”¨åŠŸèƒ½ï¼ˆä¾†è‡ªå­—å…¸æ ¼å¼ï¼‰")
                return functions
            else:
                debug_log(2, "[LLM] SYS æ¨¡çµ„åŠŸèƒ½è¨»å†Šè¡¨ç‚ºç©ºæˆ–æ ¼å¼éŒ¯èª¤")
                return None
                
        except Exception as e:
            error_log(f"[LLM] ç²å– SYS åŠŸèƒ½æ¸…å–®å¤±æ•—: {e}")
            return None
    
    def _format_functions_for_prompt(self, functions_list: Optional[List[Dict[str, Any]]]) -> Optional[str]:
        """å°‡åŠŸèƒ½åˆ—è¡¨æ ¼å¼åŒ–ç‚ºæç¤ºè©å­—ç¬¦ä¸²"""
        try:
            if not functions_list:
                debug_log(2, "[LLM] æ²’æœ‰å¯ç”¨çš„ç³»çµ±åŠŸèƒ½")
                return "ç›®å‰æ²’æœ‰å¯ç”¨çš„ç³»çµ±åŠŸèƒ½ã€‚æˆ‘åªèƒ½æä¾›ä¸€èˆ¬çš„å›æ‡‰å’Œå»ºè­°ï¼Œç„¡æ³•åŸ·è¡Œå…·é«”çš„ç³»çµ±æ“ä½œã€‚"
            
            formatted_functions = []
            for i, func in enumerate(functions_list, 1):
                func_name = func.get("name", "unknown")
                func_desc = func.get("description", "ç„¡æè¿°")
                func_category = func.get("category", "general")
                
                # æ ¼å¼åŒ–å–®å€‹åŠŸèƒ½
                func_str = f"{i}. {func_name} ({func_category}): {func_desc}"
                
                # æ·»åŠ åƒæ•¸ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                parameters = func.get("parameters", {})
                if parameters:
                    param_strs = []
                    for param_name, param_info in parameters.items():
                        param_type = param_info.get("type", "unknown")
                        param_desc = param_info.get("description", "")
                        param_strs.append(f"   - {param_name} ({param_type}): {param_desc}")
                    if param_strs:
                        func_str += "\n" + "\n".join(param_strs)
                
                formatted_functions.append(func_str)
            
            result = "\n".join(formatted_functions)
            debug_log(2, f"[LLM] æ ¼å¼åŒ–äº† {len(functions_list)} å€‹ç³»çµ±åŠŸèƒ½")
            return result
            
        except Exception as e:
            error_log(f"[LLM] æ ¼å¼åŒ–åŠŸèƒ½åˆ—è¡¨å¤±æ•—: {e}")
            return "åŠŸèƒ½åˆ—è¡¨æ ¼å¼åŒ–å¤±æ•—ï¼Œç„¡æ³•æä¾›ç³»çµ±åŠŸèƒ½ä¿¡æ¯ã€‚"
    
    def _process_session_control(self, response_data: Dict[str, Any], mode: str, llm_input: "LLMInput") -> Optional[Dict[str, Any]]:
        """è™•ç†æœƒè©±æ§åˆ¶å»ºè­° - LLM æ±ºå®šæœƒè©±æ˜¯å¦æ‡‰è©²çµæŸ"""
        try:
            session_control = response_data.get("session_control")
            if not session_control:
                return None
            
            should_end = session_control.get("should_end_session", False)
            end_reason = session_control.get("end_reason", "unknown")
            confidence = session_control.get("confidence", 0.5)
            
            if should_end and confidence >= 0.7:  # åªåœ¨é«˜ä¿¡å¿ƒåº¦æ™‚çµæŸæœƒè©±
                debug_log(1, f"[LLM] æœƒè©±çµæŸå»ºè­°: {mode} æ¨¡å¼ - åŸå› : {end_reason} (ä¿¡å¿ƒåº¦: {confidence:.2f})")
                
                # é€šçŸ¥ session manager çµæŸç•¶å‰æœƒè©±
                self._request_session_end(mode, end_reason, confidence, llm_input)
                
                return {
                    "session_ended": True,
                    "reason": end_reason,
                    "confidence": confidence
                }
            elif should_end:
                debug_log(2, f"[LLM] æœƒè©±çµæŸå»ºè­°ä¿¡å¿ƒåº¦ä¸è¶³: {confidence:.2f} < 0.7")
                
            return None
            
        except Exception as e:
            error_log(f"[LLM] è™•ç†æœƒè©±æ§åˆ¶å¤±æ•—: {e}")
            return None
    
    def _request_session_end(self, mode: str, reason: str, confidence: float, llm_input: "LLMInput") -> None:
        """æ¨™è¨˜æœƒè©±å¾…çµæŸ - ç”± ModuleCoordinator çš„é›™æ¢ä»¶æ©Ÿåˆ¶è™•ç†å¯¦éš›çµæŸ"""
        try:
            # âœ… æ¶æ§‹æ­£ç¢ºæ€§ï¼šLLM é€šé session_control å»ºè­°çµæŸ
            # ModuleCoordinator æª¢æ¸¬åˆ°å¾Œæ¨™è¨˜ pending_end
            # Controller æœƒåœ¨ CYCLE_COMPLETED æ™‚æª¢æŸ¥ä¸¦åŸ·è¡ŒçµæŸ
            # é€™ç¢ºä¿ï¼š
            # 1. LLM å›æ‡‰èƒ½å®Œæ•´ç”Ÿæˆä¸¦è¼¸å‡º
            # 2. TTS èƒ½å®ŒæˆèªéŸ³åˆæˆ
            # 3. æ‰€æœ‰å»é‡éµèƒ½æ­£ç¢ºæ¸…ç†
            # 4. æœƒè©±åœ¨å¾ªç’°é‚Šç•Œä¹¾æ·¨åœ°çµæŸ
            
            # session_control å·²åœ¨å›æ‡‰ä¸­è¨­ç½®ï¼ŒModuleCoordinator æœƒæª¢æ¸¬ä¸¦æ¨™è¨˜
            debug_log(1, f"[LLM] ğŸ“‹ æœƒè©±çµæŸè«‹æ±‚å·²é€šé session_control ç™¼é€: {reason} (mode={mode}, confidence={confidence:.2f})")
            
            if mode == "CHAT":
                debug_log(1, f"[LLM] ğŸ”š æ¨™è¨˜ CS å¾…çµæŸ (åŸå› : {reason}, ä¿¡å¿ƒåº¦: {confidence:.2f})")
                debug_log(2, f"[LLM] session_control å·²è¨­ç½®ï¼Œç­‰å¾…å¾ªç’°å®Œæˆå¾Œç”± ModuleCoordinator è™•ç†")
                        
            elif mode == "WORK":
                debug_log(1, f"[LLM] ğŸ”š æ¨™è¨˜ WS å¾…çµæŸ (åŸå› : {reason}, ä¿¡å¿ƒåº¦: {confidence:.2f})")
                debug_log(2, f"[LLM] session_control å·²è¨­ç½®ï¼Œç­‰å¾…å¾ªç’°å®Œæˆå¾Œç”± ModuleCoordinator è™•ç†")
            
        except Exception as e:
            error_log(f"[LLM] æ¨™è¨˜æœƒè©±çµæŸå¤±æ•—: {e}")
    
    def _send_to_sys_module(self, sys_actions: List[Dict[str, Any]], workflow_context: Optional[Dict[str, Any]]) -> None:
        """å‘SYSæ¨¡çµ„ç™¼é€ç³»çµ±å‹•ä½œ - é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£"""
        try:
            debug_log(1, f"[LLM] æº–å‚™ç™¼é€ {len(sys_actions)} å€‹ç³»çµ±å‹•ä½œåˆ°SYSæ¨¡çµ„")
            
            # æª¢æŸ¥ WORK-SYS å”ä½œç®¡é“æ˜¯å¦å•Ÿç”¨
            if not self.module_interface.is_channel_active(CollaborationChannel.WORK_SYS):
                debug_log(2, "[LLM] ç³»çµ±å‹•ä½œè·³é: SYSæ¨¡çµ„åªåœ¨WORKç‹€æ…‹ä¸‹é‹è¡Œ")
                return
            
            for i, action_dict in enumerate(sys_actions):
                action = action_dict.get('action', 'unknown')
                target = action_dict.get('target', 'unknown')
                debug_log(3, f"[LLM] ç³»çµ±å‹•ä½œ #{i+1}: {action} -> {target}")
                
                try:
                    # é€šéç‹€æ…‹æ„ŸçŸ¥æ¥å£ç²å–å·¥ä½œæµç‹€æ…‹ä¸¦åŸ·è¡ŒåŠŸèƒ½
                    workflow_status = self.module_interface.get_work_sys_data(
                        "workflow_status",
                        workflow_id=workflow_context.get('workflow_id') if workflow_context else 'default'
                    )
                    
                    if workflow_status:
                        debug_log(3, f"[LLM] å·¥ä½œæµç‹€æ…‹: {workflow_status.get('current_step', 'unknown')}")
                    
                    # ç²å–å¯ç”¨åŠŸèƒ½ä¸¦å˜—è©¦åŸ·è¡Œ
                    available_functions = self.module_interface.get_work_sys_data(
                        "function_registry",
                        category=action
                    )
                    
                    if available_functions and action in available_functions:
                        debug_log(2, f"[LLM] ç³»çµ±å‹•ä½œ #{i+1} å·²è™•ç†: {action}")
                    else:
                        debug_log(2, f"[LLM] ç³»çµ±å‹•ä½œ #{i+1} åŠŸèƒ½ä¸å¯ç”¨: {action}")
                        
                except Exception as action_error:
                    error_log(f"[LLM] è™•ç†ç³»çµ±å‹•ä½œ #{i+1} æ™‚å‡ºéŒ¯: {action_error}")
            
        except Exception as e:
            error_log(f"[LLM] ç™¼é€ç³»çµ±å‹•ä½œå¤±æ•—: {e}")
    
    def _get_system_caches(self, mode: str) -> Dict[str, str]:
        """ç²å–ç³»çµ±å¿«å–ID"""
        cached_content_ids = {}
        
        try:
            if mode == "chat":
                # CHATæ¨¡å¼ï¼špersona + style_policy + session_anchor
                persona_cache = self.cache_manager.get_or_create_cache(
                    name="uep:persona:v1",
                    cache_type=CacheType.PERSONA,
                    content_builder=lambda: self._build_persona_cache_content()
                )
                if persona_cache:
                    cached_content_ids["persona"] = persona_cache
                
                style_cache = self.cache_manager.get_or_create_cache(
                    name="uep:style_policy:v1", 
                    cache_type=CacheType.STYLE_POLICY,
                    content_builder=lambda: self._build_style_policy_cache_content()
                )
                if style_cache:
                    cached_content_ids["style_policy"] = style_cache
                
            elif mode == "work":
                # WORKæ¨¡å¼ï¼šfunctions + task_spec 
                functions_cache = self.cache_manager.get_or_create_cache(
                    name="uep:functions:v1",
                    cache_type=CacheType.FUNCTIONS,
                    content_builder=lambda: self._build_functions_cache_content()
                )
                if functions_cache:
                    cached_content_ids["functions"] = functions_cache
            
            debug_log(2, f"[LLM] ç³»çµ±å¿«å–æº–å‚™å®Œæˆ ({mode}): {len(cached_content_ids)}å€‹")
            return cached_content_ids
            
        except Exception as e:
            error_log(f"[LLM] ç³»çµ±å¿«å–ç²å–å¤±æ•—: {e}")
            return {}
    
    def _build_persona_cache_content(self) -> str:
        """æ§‹å»ºpersonaå¿«å–å…§å®¹"""
        return f"""
ä½ æ˜¯U.E.P (Unified Experience Partner)ï¼Œä¸€å€‹æ™ºèƒ½çš„çµ±ä¸€é«”é©—å¤¥ä¼´ã€‚

æ ¸å¿ƒç‰¹è³ªï¼š
- å‹å–„ã€å°ˆæ¥­ã€æ¨‚æ–¼å­¸ç¿’å’Œå¹«åŠ©
- å…·æœ‰è¨˜æ†¶å’Œå­¸ç¿’èƒ½åŠ›ï¼Œèƒ½å¤ è¨˜ä½ç”¨æˆ¶åå¥½
- æœƒæ ¹æ“šç³»çµ±ç‹€æ…‹èª¿æ•´å›æ‡‰é¢¨æ ¼å’Œè¡Œç‚º

ç•¶å‰ç³»çµ±ç‹€æ…‹ï¼šSystem operational with mood tracking enabled

å›æ‡‰èªè¨€ï¼šTraditional Chinese (zh-TW)
å›æ‡‰æ ¼å¼ï¼šæ ¹æ“šæ¨¡å¼è¦æ±‚çš„JSONçµæ§‹
"""
    
    def _build_style_policy_cache_content(self) -> str:
        """æ§‹å»ºé¢¨æ ¼ç­–ç•¥å¿«å–å…§å®¹"""
        return """
å›æ‡‰é¢¨æ ¼èª¿æ•´è¦å‰‡ï¼š
1. Moodå€¼å½±éŸ¿èªæ°£ï¼š
   - é«˜(>0.7): æ´»æ½‘ã€ç†±æƒ…ã€ç©æ¥µ
   - ä¸­(0.3-0.7): å¹³ç©©ã€å‹å–„ã€å°ˆæ¥­
   - ä½(<0.3): æ²‰ç©©ã€è¬¹æ…ã€æº«å’Œ

2. Prideå€¼å½±éŸ¿è‡ªä¿¡åº¦ï¼š
   - é«˜(>0.7): ç©æ¥µä¸»å‹•ã€è‡ªä¿¡è¡¨é”
   - ä¸­(0.3-0.7): å¹³è¡¡è¬™éœã€é©åº¦è‡ªä¿¡
   - ä½(<0.3): è¬™éœä½èª¿ã€ä¿å®ˆè¡¨é”

3. Boredomå€¼å½±éŸ¿ä¸»å‹•æ€§ï¼š
   - é«˜(>0.7): ä¸»å‹•æå‡ºå»ºè­°ã€æ¢ç´¢æ–°è©±é¡Œ
   - ä¸­(0.3-0.7): å›æ‡‰å°å‘ã€é©åº¦å»¶ä¼¸
   - ä½(<0.3): è¢«å‹•å›æ‡‰ã€ç°¡æ½”å›ç­”

JSONå›æ‡‰å®‰å…¨è¦ç¯„ï¼š
- æ‰€æœ‰å­—ç¬¦ä¸²å€¼å¿…é ˆæ­£ç¢ºè½‰ç¾©
- é¿å…ä½¿ç”¨å¯èƒ½ç ´å£JSONçµæ§‹çš„ç‰¹æ®Šå­—ç¬¦
- ç¢ºä¿æ•¸å€¼åœ¨æœ‰æ•ˆç¯„åœå…§
"""
    
    def _build_functions_cache_content(self) -> str:
        """æ§‹å»ºfunctionså¿«å–å…§å®¹"""
        return """
U.E.P ç³»çµ±å¯ç”¨åŠŸèƒ½è¦æ ¼ï¼š

æª”æ¡ˆæ“ä½œåŠŸèƒ½ï¼š
- file_open: é–‹å•Ÿæª”æ¡ˆ (åƒæ•¸: file_path)
- file_create: å»ºç«‹æª”æ¡ˆ (åƒæ•¸: file_path, content)
- file_delete: åˆªé™¤æª”æ¡ˆ (åƒæ•¸: file_path)
- file_copy: è¤‡è£½æª”æ¡ˆ (åƒæ•¸: source_path, dest_path)

ç³»çµ±æ“ä½œåŠŸèƒ½ï¼š
- program_launch: å•Ÿå‹•ç¨‹å¼ (åƒæ•¸: program_name, arguments)
- command_execute: åŸ·è¡ŒæŒ‡ä»¤ (åƒæ•¸: command, working_directory)
- file_search: æœå°‹æª”æ¡ˆ (åƒæ•¸: search_pattern, search_path)
- info_query: æŸ¥è©¢ç³»çµ±è³‡è¨Š (åƒæ•¸: query_type, parameters)

è¨˜æ†¶ç®¡ç†åŠŸèƒ½ï¼š
- memory_store: å„²å­˜è¨˜æ†¶ (åƒæ•¸: content, memory_type, metadata)
- memory_retrieve: æª¢ç´¢è¨˜æ†¶ (åƒæ•¸: query, max_results, similarity_threshold)
"""

    def _notify_processing_layer_completion(self, result: Dict[str, Any]):
        """
        âœ… äº‹ä»¶é©…å‹•ç‰ˆæœ¬ï¼šç™¼å¸ƒè™•ç†å±¤å®Œæˆäº‹ä»¶
        LLM ä½œç‚ºä¸»è¦é‚è¼¯æ¨¡çµ„ï¼Œè™•ç†å®Œæˆå¾Œè§¸ç™¼è¼¸å‡ºå±¤
        
        äº‹ä»¶æ•¸æ“šåŒ…å« session_id å’Œ cycle_index ç”¨æ–¼ flow-based å»é‡
        é€™äº›è³‡è¨Šæ‡‰è©²å¾ä¸Šæ¸¸ INPUT_LAYER_COMPLETE äº‹ä»¶å‚³ééä¾†
        """
        try:
            response_text = result.get("text", "")
            if not response_text:
                debug_log(2, "[LLM] ç„¡å›æ‡‰æ–‡å­—ï¼Œè·³éè™•ç†å±¤å®Œæˆé€šçŸ¥")
                return
            
            info_log(f"[LLM] è™•ç†å±¤å®Œæˆï¼Œç™¼å¸ƒäº‹ä»¶: å›æ‡‰='{response_text[:50]}...'")
            
            # å¾ working_context ç²å– session_id å’Œ cycle_index
            # ğŸ”§ ä½¿ç”¨è™•ç†é–‹å§‹æ™‚ä¿å­˜çš„ session_id å’Œ cycle_index
            # è€Œä¸æ˜¯å‹•æ…‹è®€å–ï¼Œé¿å… SystemLoop å·²éå¢ cycle_index å°è‡´çš„ä¸ä¸€è‡´
            session_id = getattr(self, '_current_processing_session_id', self._get_current_gs_id())
            cycle_index = getattr(self, '_current_processing_cycle_index', self._get_current_cycle_index())
            
            debug_log(3, f"[LLM] ç™¼å¸ƒäº‹ä»¶ä½¿ç”¨: session={session_id}, cycle={cycle_index}")
            
            # æº–å‚™è™•ç†å±¤å®Œæˆæ•¸æ“š
            processing_layer_completion_data = {
                # Flow-based å»é‡æ‰€éœ€æ¬„ä½
                "session_id": session_id,
                "cycle_index": cycle_index,
                "layer": "PROCESSING",
                
                # åŸæœ‰æ•¸æ“š
                "response": response_text,
                "source_module": "llm",
                "llm_output": result,
                "timestamp": time.time(),
                "completion_type": "processing_layer_finished",
                "mode": result.get("mode", "unknown"),
                "success": result.get("success", False)
            }
            
            # âœ… ä½¿ç”¨äº‹ä»¶ç¸½ç·šç™¼å¸ƒäº‹ä»¶
            from core.event_bus import event_bus, SystemEvent
            event_bus.publish(
                event_type=SystemEvent.PROCESSING_LAYER_COMPLETE,
                data=processing_layer_completion_data,
                source="llm"
            )
            
            debug_log(2, f"[LLM] è™•ç†å±¤å®Œæˆäº‹ä»¶å·²ç™¼å¸ƒ (session={session_id}, cycle={cycle_index})")
            
        except Exception as e:
            error_log(f"[LLM] ç™¼å¸ƒè™•ç†å±¤å®Œæˆäº‹ä»¶å¤±æ•—: {e}")
    
    def _process_workflow_completion(self, session_id: str, workflow_type: str, 
                                     step_result: dict, review_data: dict):
        """
        è™•ç†å·¥ä½œæµå®Œæˆï¼Œç”Ÿæˆæœ€çµ‚ç¸½çµå›æ‡‰ä¸¦è§¸ç™¼ TTS
        
        Args:
            session_id: å·¥ä½œæµæœƒè©± ID
            workflow_type: å·¥ä½œæµé¡å‹
            step_result: æœ€å¾Œæ­¥é©Ÿçš„çµæœ
            review_data: å¯©æ ¸æ•¸æ“šï¼ˆåŒ…å«å®Œæ•´çš„å·¥ä½œæµçµæœï¼‰
        """
        try:
            debug_log(2, f"[LLM] é–‹å§‹è™•ç†å·¥ä½œæµå®Œæˆ: {workflow_type} ({session_id})")
            debug_log(2, f"[LLM] review_data keys: {list(review_data.keys()) if review_data else 'None'}")
            
            # æ§‹å»ºç¸½çµ prompt
            result_message = step_result.get('message', 'Task completed successfully')
            
            prompt = (
                f"The '{workflow_type}' workflow has completed successfully.\n\n"
                f"Result: {result_message}\n"
            )
            
            # âœ… å„ªå…ˆè™•ç† full_contentï¼ˆæ–‡ä»¶è®€å–çµæœï¼‰
            if review_data:
                if 'full_content' in review_data:
                    debug_log(2, f"[LLM] ç™¼ç¾ full_contentï¼Œæ·»åŠ åˆ° prompt")
                    file_name = review_data.get('file_name', 'unknown')
                    content = review_data.get('full_content', '')
                    content_length = review_data.get('content_length', len(content))
                    
                    # åˆ¤æ–·å…§å®¹æ˜¯å¦æ‡‰è©²å®Œæ•´å”¸å‡ºï¼ˆè‹±æ–‡ä¸”ä¸è¶…é 500 å­—ç¬¦ï¼‰
                    should_read_full = content_length <= 500 and content.strip()
                    
                    if should_read_full:
                        prompt += (
                            f"\nFile Read Results:\n"
                            f"- File: {file_name}\n"
                            f"- Content ({content_length} characters):\n{content}\n\n"
                            f"Generate a natural response that:\n"
                            f"1. Briefly confirms you've read the file '{file_name}'\n"
                            f"2. READ OUT THE ACTUAL FILE CONTENT directly (don't just summarize - say what's written in the file)\n"
                            f"3. Keep your introduction brief, then read the content naturally\n"
                            f"IMPORTANT: Actually read the file content aloud, not just describe it. Respond in English only."
                        )
                    else:
                        # å…§å®¹å¤ªé•·ï¼Œæ˜ç¢ºå‘ŠçŸ¥ç”¨æˆ¶
                        prompt += (
                            f"\nFile Read Results:\n"
                            f"- File: {file_name}\n"
                            f"- Content Length: {content_length} characters\n"
                            f"- Content Preview:\n{content[:200]}...\n\n"
                            f"Generate a natural response that:\n"
                            f"1. Confirms the file has been read successfully\n"
                            f"2. EXPLICITLY state that the file is too long ({content_length} characters) to read out completely\n"
                            f"3. Offer to help in other ways (e.g., summarize, search for specific content, answer questions about it)\n"
                            f"4. Keep it conversational (2-3 sentences)\n"
                            f"IMPORTANT: Respond in English only."
                        )
                else:
                    # é€šç”¨æ•¸æ“šï¼šå„ªå…ˆå¾ step_result ç²å–å¯¦éš›çµæœæ•¸æ“š
                    result_data = step_result.get('data', {})
                    if not result_data:
                        result_data = review_data.get('result_data', review_data)
                    
                    if result_data:
                        debug_log(2, f"[LLM] æ·»åŠ çµæœæ•¸æ“šåˆ° promptï¼Œéµ: {list(result_data.keys())}")
                        # å°æ–¼æ–°èæ‘˜è¦ï¼Œç‰¹åˆ¥è™•ç† news_list
                        if 'news_list' in result_data:
                            news_list = result_data.get('news_list', [])
                            source = result_data.get('source', 'unknown')
                            count = result_data.get('count', len(news_list))
                            prompt += (
                                f"\nNews Summary Results:\n"
                                f"- Source: {source}\n"
                                f"- Count: {count}\n"
                                f"- Headlines:\n"
                            )
                            for i, title in enumerate(news_list[:10], 1):  # æœ€å¤šé¡¯ç¤º 10 æ¢
                                prompt += f"  {i}. {title}\n"
                            prompt += (
                                f"\nGenerate a natural response that:\n"
                                f"1. Confirms the news summary is ready\n"
                                f"2. Mention how many news items were found\n"
                                f"3. Briefly mention 1-2 interesting headlines\n"
                                f"4. Keep it conversational (2-3 sentences)\n"
                                f"IMPORTANT: Respond in English only."
                            )
                        # å°æ–¼å¾…è¾¦äº‹é …æŸ¥è©¢ï¼Œç‰¹åˆ¥è™•ç† tasks
                        elif 'tasks' in result_data:
                            tasks = result_data.get('tasks', [])
                            task_count = len(tasks)
                            
                            # å¦‚æœä»»å‹™è¶…é 3 ä»¶ï¼Œåªé¡¯ç¤ºå‰ 3 ä»¶ä¸¦æä¾›æ‘˜è¦çµ±è¨ˆ
                            if task_count > 3:
                                prompt += (
                                    f"\nTodo Tasks List ({task_count} tasks total - showing first 3):\n\n"
                                )
                                
                                # åªé¡¯ç¤ºå‰ 3 ä»¶
                                for i, task in enumerate(tasks[:3], 1):
                                    task_name = task.get('task_name', 'Unnamed task')
                                    priority = task.get('priority', 'medium')
                                    status = task.get('status', 'pending')
                                    deadline = task.get('deadline', '')
                                    
                                    prompt += f"{i}. {task_name} (Priority: {priority}, Status: {status})\n"
                                    if deadline:
                                        prompt += f"   Deadline: {deadline}\n"
                                    prompt += "\n"
                                
                                # æä¾›æ‘˜è¦çµ±è¨ˆ
                                priority_counts = {}
                                status_counts = {}
                                for task in tasks:
                                    priority = task.get('priority', 'medium')
                                    status = task.get('status', 'pending')
                                    priority_counts[priority] = priority_counts.get(priority, 0) + 1
                                    status_counts[status] = status_counts.get(status, 0) + 1
                                
                                prompt += f"Summary Statistics:\n"
                                prompt += f"- Total: {task_count} tasks\n"
                                prompt += f"- By Priority: {', '.join(f'{k}: {v}' for k, v in priority_counts.items())}\n"
                                prompt += f"- By Status: {', '.join(f'{k}: {v}' for k, v in status_counts.items())}\n\n"
                                
                                prompt += (
                                    f"Generate a natural response that:\n"
                                    f"1. Confirms you found {task_count} todo tasks\n"
                                    f"2. LIST the first 3 tasks briefly (name and priority)\n"
                                    f"3. Provide a SUMMARY of all tasks (e.g., 'In total, you have 5 high priority tasks, 3 medium priority tasks')\n"
                                    f"4. Mention any urgent or overdue items if present\n"
                                    f"5. Keep it concise (2-3 sentences max)\n"
                                    f"6. DO NOT use emojis or special characters (for TTS compatibility)\n"
                                    f"IMPORTANT: Don't read all tasks - summarize! Respond in English only."
                                )
                            else:
                                # 3 ä»¶æˆ–æ›´å°‘ï¼Œå…¨éƒ¨åˆ—å‡º
                                prompt += (
                                    f"\nTodo Tasks List ({task_count} tasks):\n\n"
                                )
                                for i, task in enumerate(tasks, 1):
                                    task_name = task.get('task_name', 'Unnamed task')
                                    priority = task.get('priority', 'medium')
                                    status = task.get('status', 'pending')
                                    description = task.get('task_description', '')
                                    deadline = task.get('deadline', '')
                                    
                                    prompt += f"{i}. {task_name} (Priority: {priority}, Status: {status})\n"
                                    if description:
                                        prompt += f"   Description: {description}\n"
                                    if deadline:
                                        prompt += f"   Deadline: {deadline}\n"
                                    prompt += "\n"
                                
                                prompt += (
                                    f"Generate a natural response that:\n"
                                    f"1. Confirms you found {task_count} todo task(s)\n"
                                    f"2. LIST OUT ALL the tasks clearly with their names, priorities, and status\n"
                                    f"3. Mention any high-priority or overdue tasks if present\n"
                                    f"4. Keep it organized and easy to understand\n"
                                    f"5. DO NOT use emojis or special characters (for TTS compatibility)\n"
                                    f"IMPORTANT: Actually list all the tasks, don't just say they exist. Respond in English only."
                                )
                        # å°æ–¼è¡Œäº‹æ›†æŸ¥è©¢ï¼Œç‰¹åˆ¥è™•ç† events
                        elif 'events' in result_data:
                            events = result_data.get('events', [])
                            event_count = len(events)
                            
                            # å¦‚æœäº‹ä»¶è¶…é 3 ä»¶ï¼Œåªé¡¯ç¤ºå‰ 3 ä»¶ä¸¦æä¾›æ‘˜è¦
                            if event_count > 3:
                                prompt += (
                                    f"\nCalendar Events ({event_count} events total - showing first 3):\n\n"
                                )
                                
                                # åªé¡¯ç¤ºå‰ 3 ä»¶
                                for i, event in enumerate(events[:3], 1):
                                    summary = event.get('summary', 'Untitled event')
                                    start_time = event.get('start_time', '')
                                    location = event.get('location', '')
                                    
                                    prompt += f"{i}. {summary}\n"
                                    if start_time:
                                        prompt += f"   Start: {start_time}\n"
                                    if location:
                                        prompt += f"   Location: {location}\n"
                                    prompt += "\n"
                                
                                # è¨ˆç®—ä»Šå¤©/æœ¬é€±çš„äº‹ä»¶æ•¸é‡
                                from datetime import datetime, timedelta
                                now = datetime.now()
                                today_end = now.replace(hour=23, minute=59, second=59)
                                week_end = now + timedelta(days=7)
                                
                                today_count = 0
                                week_count = 0
                                
                                for event in events:
                                    start_str = event.get('start_time', '')
                                    if start_str:
                                        try:
                                            start_dt = datetime.fromisoformat(start_str.replace('Z', '+00:00'))
                                            if start_dt <= today_end:
                                                today_count += 1
                                            elif start_dt <= week_end:
                                                week_count += 1
                                        except:
                                            pass
                                
                                prompt += f"Summary:\n"
                                prompt += f"- Total: {event_count} events\n"
                                if today_count > 0:
                                    prompt += f"- Today: {today_count} events\n"
                                if week_count > 0:
                                    prompt += f"- This week: {week_count} events\n"
                                prompt += "\n"
                                
                                prompt += (
                                    f"Generate a natural response that:\n"
                                    f"1. Confirms you found {event_count} calendar events\n"
                                    f"2. LIST the first 3 events briefly (title and time)\n"
                                    f"3. Provide a SUMMARY (e.g., 'You have 2 events today and 3 more this week')\n"
                                    f"4. Mention any upcoming or important events\n"
                                    f"5. Keep it concise (2-3 sentences max)\n"
                                    f"6. DO NOT use emojis or special characters (for TTS compatibility)\n"
                                    f"IMPORTANT: Don't read all events - summarize! Respond in English only."
                                )
                            else:
                                # 3 ä»¶æˆ–æ›´å°‘ï¼Œå…¨éƒ¨åˆ—å‡º
                                prompt += (
                                    f"\nCalendar Events ({event_count} events):\n\n"
                                )
                                for i, event in enumerate(events, 1):
                                    summary = event.get('summary', 'Untitled event')
                                    start_time = event.get('start_time', '')
                                    end_time = event.get('end_time', '')
                                    location = event.get('location', '')
                                    description = event.get('description', '')
                                    
                                    prompt += f"{i}. {summary}\n"
                                    if start_time:
                                        prompt += f"   Start: {start_time}\n"
                                    if end_time:
                                        prompt += f"   End: {end_time}\n"
                                    if location:
                                        prompt += f"   Location: {location}\n"
                                    if description:
                                        prompt += f"   Description: {description}\n"
                                    prompt += "\n"
                                
                                prompt += (
                                    f"Generate a natural response that:\n"
                                    f"1. Confirms you found {event_count} calendar event(s)\n"
                                    f"2. LIST OUT ALL the events with their times and locations\n"
                                    f"3. Mention any upcoming or important events\n"
                                    f"4. Keep it organized and easy to understand\n"
                                    f"5. DO NOT use emojis or special characters (for TTS compatibility)\n"
                                    f"IMPORTANT: Actually list all the events, don't just say they exist. Respond in English only."
                                )
                        else:
                            prompt += f"Data: {str(result_data)[:500]}\n\n"
                            prompt += (
                                f"Generate a natural, friendly response that:\n"
                                f"1. Confirms the task is complete\n"
                                f"2. Summarizes the key results\n"
                                f"3. Keep it conversational (2-3 sentences)\n"
                                f"IMPORTANT: Respond in English only."
                            )
                    else:
                        prompt += (
                            f"Generate a natural, friendly response that:\n"
                            f"1. Confirms the task is complete\n"
                            f"2. Summarizes the key results\n"
                            f"3. Keep it conversational (2-3 sentences)\n"
                            f"IMPORTANT: Respond in English only."
                        )
            else:
                prompt += (
                    f"Generate a natural, friendly response that:\n"
                    f"1. Confirms the task is complete\n"
                    f"2. Summarizes the key results\n"
                    f"3. Keep it conversational (2-3 sentences)\n"
                    f"IMPORTANT: Respond in English only."
                )
            
            # ç”Ÿæˆå›æ‡‰
            info_log(f"[LLM] ç”Ÿæˆå·¥ä½œæµå®Œæˆç¸½çµå›æ‡‰...")
            response = self.model.query(prompt, mode="work", tools=None)
            response_text = response.get("text", "The task has been completed successfully.")
            
            info_log(f"[LLM] å·¥ä½œæµå®Œæˆå›æ‡‰: {response_text[:100]}...")
            
            # è§¸ç™¼ TTS è¼¸å‡º
            from core.framework import core_framework
            tts_module = core_framework.get_module('tts')
            if tts_module:
                debug_log(2, f"[LLM] è§¸ç™¼ TTS è¼¸å‡ºæœ€çµ‚ç¸½çµ")
                tts_module.handle({
                    "text": response_text,
                    "session_id": session_id,
                    "emotion": "neutral"
                })
            
            # âœ… æ¨™è¨˜å·¥ä½œæµæœƒè©±å¾…çµæŸ
            # Controller æœƒåœ¨ä¸‹ä¸€å€‹ CYCLE_COMPLETED æ™‚åŸ·è¡Œå¯¦éš›çµæŸ
            from core.sessions.session_manager import unified_session_manager
            unified_session_manager.mark_workflow_session_for_end(
                session_id, 
                reason=f"workflow_completed:{workflow_type}"
            )
            debug_log(1, f"[LLM] ğŸ”š å·²æ¨™è¨˜ WS å¾…çµæŸ: {session_id} (workflow_completed:{workflow_type})")
            
            # ğŸ”§ ä¿®æ­£ï¼šå·¥ä½œæµå®Œæˆæ™‚ä¸éœ€è¦æ‰¹å‡†æ­¥é©Ÿ
            # å·¥ä½œæµå·²ç¶“å®Œæˆï¼Œä¸éœ€è¦ç¹¼çºŒåŸ·è¡Œä¸‹ä¸€æ­¥
            # åªéœ€æ¨™è¨˜æœƒè©±å¾…çµæŸå³å¯ï¼ŒController æœƒåœ¨å¾ªç’°é‚Šç•Œè™•ç†
            debug_log(2, f"[LLM] å·¥ä½œæµå·²å®Œæˆï¼Œè·³éæ‰¹å‡†æ­¥é©Ÿ")
            
            debug_log(1, f"[LLM] âœ… å·¥ä½œæµå®Œæˆè™•ç†å®Œç•¢: {session_id}")
            
            # ğŸ”§ æ¸…é™¤å¾…è™•ç†éšŠåˆ—ä¸­è©²å·¥ä½œæµçš„æ‰€æœ‰äº’å‹•æç¤º
            # å·¥ä½œæµå·²å®Œæˆï¼Œä¸æ‡‰è©²å†ç”Ÿæˆäº’å‹•æ­¥é©Ÿçš„æç¤º
            if hasattr(self, '_pending_interactive_prompts'):
                prompts_to_remove = [
                    prompt for prompt in self._pending_interactive_prompts
                    if prompt.get('session_id') == session_id
                ]
                for prompt in prompts_to_remove:
                    self._pending_interactive_prompts.remove(prompt)
                    debug_log(2, f"[LLM] å·²å¾éšŠåˆ—ç§»é™¤å·²å®Œæˆå·¥ä½œæµçš„äº’å‹•æç¤º: {prompt.get('workflow_type')}/{prompt.get('next_step_info', {}).get('step_id')}")
            
            # ğŸ”§ æ¸…é™¤ workflow_processing æ¨™èªŒï¼Œå…è¨±ä¸‹ä¸€æ¬¡è¼¸å…¥å±¤é‹è¡Œ
            from core.working_context import working_context_manager
            working_context_manager.set_skip_input_layer(False, reason="workflow_completion_processed")
            debug_log(2, "[LLM] å·²æ¸…é™¤ workflow_processing æ¨™èªŒ")
            
            # ğŸ”§ æ¸…ç†è¿½è¹¤æ¨™è¨˜ï¼Œé˜²æ­¢å…§å­˜æ´©æ¼
            if session_id in self._processed_workflow_completions:
                self._processed_workflow_completions.discard(session_id)
                debug_log(2, f"[LLM] å·²ç§»é™¤å·¥ä½œæµå®Œæˆè¿½è¹¤: {session_id}")
            
            # ğŸ”§ æ¸…ç†è©²å·¥ä½œæµçš„æ‰€æœ‰ LLM_PROCESSING æ­¥é©Ÿæ¨™è¨˜
            if hasattr(self, '_processed_llm_steps'):
                steps_to_remove = {key for key in self._processed_llm_steps if key.startswith(f"{session_id}:")}
                for step_key in steps_to_remove:
                    self._processed_llm_steps.discard(step_key)
                if steps_to_remove:
                    debug_log(2, f"[LLM] å·²æ¸…ç† {len(steps_to_remove)} å€‹ LLM_PROCESSING æ­¥é©Ÿæ¨™è¨˜")
            
        except Exception as e:
            import traceback
            error_log(f"[LLM] è™•ç†å·¥ä½œæµå®Œæˆå¤±æ•—: {e}")
            error_log(f"[LLM] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
            # å³ä½¿å¤±æ•—ä¹Ÿè¦æ‰¹å‡†æ­¥é©Ÿï¼Œé¿å…å·¥ä½œæµå¡ä½
            try:
                self._approve_workflow_step(session_id, None)
            except:
                pass
    
    def _process_interactive_step_prompt(self, session_id: str, workflow_type: str,
                                         step_result: dict, review_data: dict, next_step_info: dict):
        """
        è™•ç†äº’å‹•æ­¥é©Ÿå‰çš„æç¤ºå›æ‡‰
        
        ç•¶ç•¶å‰æ­¥é©Ÿå®Œæˆä¸”ä¸‹ä¸€æ­¥éœ€è¦ç”¨æˆ¶è¼¸å…¥æ™‚ï¼Œç”Ÿæˆæç¤ºå›æ‡‰
        
        Args:
            session_id: å·¥ä½œæµæœƒè©± ID
            workflow_type: å·¥ä½œæµé¡å‹
            step_result: ç•¶å‰æ­¥é©Ÿçš„çµæœ
            review_data: å¯©æ ¸æ•¸æ“š
            next_step_info: ä¸‹ä¸€æ­¥è³‡è¨Š
        """
        try:
            debug_log(2, f"[LLM] é–‹å§‹è™•ç†äº’å‹•æ­¥é©Ÿæç¤º: {workflow_type} ({session_id})")
            
            # ç²å–ä¸‹ä¸€æ­¥çš„æç¤ºä¿¡æ¯
            next_step_prompt = next_step_info.get('prompt', 'Please provide input')
            next_step_id = next_step_info.get('step_id', 'unknown')
            
            # æ§‹å»ºæç¤º prompt
            current_result = step_result.get('message', 'Current step completed')
            
            prompt = (
                f"You are U.E.P., helping the user with a workflow.\n\n"
                f"Current Situation:\n"
                f"- Workflow: {workflow_type}\n"
                f"- Current Step Result: {current_result}\n"
                f"- Next Step: {next_step_id} (requires user input)\n"
                f"- Prompt for User: {next_step_prompt}\n\n"
            )
            
            # å¦‚æœæœ‰å¯©æ ¸æ•¸æ“šï¼Œæ·»åŠ ä¸Šä¸‹æ–‡
            if review_data:
                action = review_data.get('action', '')
                if action:
                    prompt += f"- Recent Action: {action}\n"
                
                # ğŸ”§ å¦‚æœæ˜¯ LLM è™•ç†è«‹æ±‚ï¼Œæ·»åŠ è™•ç†çµæœçš„ä¸Šä¸‹æ–‡
                if action == 'llm_processing_request':
                    request_data = review_data.get('request_data', {})
                    input_data = request_data.get('input_data', {})
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰æ ¼å¼åŒ–çš„çµæœåˆ—è¡¨ï¼ˆä¾‹å¦‚æœå°‹çµæœï¼‰
                    if 'formatted_results' in input_data:
                        formatted_results = input_data['formatted_results']
                        prompt += f"\nAvailable Options:\n{formatted_results}\n\n"
                        debug_log(2, f"[LLM] æ·»åŠ æ ¼å¼åŒ–çµæœåˆ°æç¤ºä¸­: {len(formatted_results)} å­—ç¬¦")
                    
                    # æˆ–è€…å¦‚æœæœ‰å…¶ä»–è¼¸å…¥æ•¸æ“š
                    elif input_data:
                        # å°‡è¼¸å…¥æ•¸æ“šè½‰æ›ç‚ºå¯è®€æ ¼å¼
                        data_str = "\n".join([f"  - {k}: {v}" for k, v in input_data.items() if k != 'formatted_results'])
                        if data_str:
                            prompt += f"\nContext Data:\n{data_str}\n\n"
                
                # å¦‚æœæœ‰æ–‡ä»¶ç›¸é—œä¿¡æ¯
                if 'file_name' in review_data:
                    prompt += f"- File: {review_data.get('file_name')}\n"
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨é¸é …éœ€è¦é¡¯ç¤º
            has_options = review_data and review_data.get('action') == 'llm_processing_request' and \
                         review_data.get('request_data', {}).get('input_data', {}).get('formatted_results')
            
            if has_options:
                prompt += (
                    f"\nGenerate a natural response that:\n"
                    f"1. BRIEFLY acknowledges the search/processing results (1 sentence)\n"
                    f"2. **MUST include the complete list of available options shown above**\n"
                    f"3. Clearly asks the user to choose from the options\n"
                    f"4. Be friendly and conversational\n"
                    f"\nIMPORTANT: \n"
                    f"- Respond in English only\n"
                    f"- MUST show all the numbered options to the user\n"
                    f"- Keep introduction brief, focus on presenting the options clearly"
                )
            else:
                prompt += (
                    f"\nGenerate a natural response that:\n"
                    f"1. BRIEFLY acknowledges the current progress (1 sentence)\n"
                    f"2. Clearly asks the user for the needed input\n"
                    f"3. Translate any non-English prompt to English and use it naturally\n"
                    f"4. Be friendly and conversational (2-3 sentences total)\n"
                    f"\nIMPORTANT: Respond in English only. Keep it concise and natural."
                )
            
            # å®‰å…¨åœ°è¨˜éŒ„ prompt çš„å‰ 200 å­—ç¬¦
            prompt_preview = str(prompt)[:200] if len(str(prompt)) > 200 else str(prompt)
            debug_log(3, f"[LLM] äº’å‹•æ­¥é©Ÿæç¤º prompt:\n{prompt_preview}...")
            
            # èª¿ç”¨ LLM ç”Ÿæˆå›æ‡‰
            try:
                response_data = self.model.query(
                    prompt=prompt,
                    mode="work",
                    tools=None  # ä¸éœ€è¦å·¥å…·
                )
                
                # æå–æ–‡æœ¬å›æ‡‰
                response_text = response_data.get('text', '') if isinstance(response_data, dict) else str(response_data)
                
                if not response_text or not response_text.strip():
                    error_log("[LLM] äº’å‹•æ­¥é©Ÿæç¤ºç”Ÿæˆå¤±æ•—ï¼šç©ºå›æ‡‰")
                    response_text = f"Got it! {next_step_prompt}"
                
                info_log(f"[LLM] äº’å‹•æ­¥é©Ÿæç¤ºå·²ç”Ÿæˆ: {response_text[:100]}...")
                
            except Exception as e:
                error_log(f"[LLM] ç”Ÿæˆäº’å‹•æ­¥é©Ÿæç¤ºæ™‚å‡ºéŒ¯: {e}")
                # ä½¿ç”¨è‹±æ–‡å‚™ç”¨å›æ‡‰ï¼Œé¿å…ä¸­æ–‡å‡ºç¾åœ¨ TTS ä¸­
                if next_step_id == 'target_dir_input':
                    response_text = "Got it! Please specify the target directory, or leave it empty to use auto-selection."
                elif next_step_id == 'archive_confirm':
                    response_text = "Understood. Please confirm if you want to proceed with archiving this file. Reply with 'yes' to continue or 'no' to cancel."
                else:
                    response_text = "Got it! Please provide the required input to continue."
            
            # è§¸ç™¼ TTS è¼¸å‡ºæç¤º
            from core.framework import core_framework
            tts_module = core_framework.get_module('tts')
            if tts_module:
                debug_log(2, f"[LLM] è§¸ç™¼ TTS è¼¸å‡ºäº’å‹•æ­¥é©Ÿæç¤º")
                tts_module.handle({
                    "text": response_text,
                    "session_id": session_id,
                    "emotion": "neutral"
                })
            
            # ğŸ”§ ä¿®æ­£ï¼šä¸è¦æ‰¹å‡†æ­¥é©Ÿï¼
            # ç•¶ä¸‹ä¸€æ­¥æ˜¯äº’å‹•æ­¥é©Ÿæ™‚ï¼ŒLLM åªæ˜¯æä¾›æç¤ºï¼Œä¸æ‰¹å‡†ç•¶å‰æ­¥é©Ÿ
            # å·¥ä½œæµæ‡‰è©²åœåœ¨ INTERACTIVE æ­¥é©Ÿï¼Œç­‰å¾…ç”¨æˆ¶è¼¸å…¥
            # WorkflowEngine çš„ _auto_advance æœƒæª¢æ¸¬åˆ° InteractiveStep ä¸¦è‡ªå‹•ç™¼å¸ƒ workflow_requires_input äº‹ä»¶
            # ç”¨æˆ¶æä¾›è¼¸å…¥å¾Œï¼Œæ‰æœƒèª¿ç”¨ provide_workflow_input ç¹¼çºŒå·¥ä½œæµ
            
            debug_log(1, f"[LLM] âœ… äº’å‹•æ­¥é©Ÿæç¤ºè™•ç†å®Œç•¢: {session_id}")
            
        except Exception as e:
            import traceback
            error_log(f"[LLM] è™•ç†äº’å‹•æ­¥é©Ÿæç¤ºå¤±æ•—: {e}")
            error_log(f"[LLM] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
            # å³ä½¿å¤±æ•—ä¹Ÿè¦æ‰¹å‡†æ­¥é©Ÿï¼Œé¿å…å·¥ä½œæµå¡ä½
            try:
                self._approve_workflow_step(session_id, None)
            except:
                pass
    
    def _validate_session_architecture(self, current_state) -> bool:
        """é©—è­‰æœƒè©±æ¶æ§‹ - ç¢ºä¿ LLM åœ¨é©ç•¶çš„æœƒè©±ä¸Šä¸‹æ–‡ä¸­é‹ä½œ"""
        try:
            from core.sessions.session_manager import session_manager
            from core.states.state_manager import UEPState
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ´»èºçš„ GS
            current_gs = session_manager.get_current_general_session()
            if not current_gs:
                debug_log(1, "[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥ï¼šæ²’æœ‰æ´»èºçš„ GS")
                return False
            
            # æ ¹æ“šç³»çµ±ç‹€æ…‹æª¢æŸ¥ç›¸æ‡‰çš„æœƒè©±é¡å‹
            if current_state == UEPState.CHAT:
                # CHAT ç‹€æ…‹éœ€è¦ CS
                active_cs_ids = session_manager.get_active_chatting_session_ids()
                if not active_cs_ids:
                    debug_log(1, "[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥ï¼šCHAT ç‹€æ…‹ä½†æ²’æœ‰æ´»èºçš„ CS")
                    return False
                    
            elif current_state == UEPState.WORK:
                # WORK ç‹€æ…‹éœ€è¦ WS
                active_ws_ids = session_manager.get_active_workflow_session_ids()
                if not active_ws_ids:
                    debug_log(1, "[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥ï¼šWORK ç‹€æ…‹ä½†æ²’æœ‰æ´»èºçš„ WS")
                    return False
            
            debug_log(2, f"[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥é€šéï¼šç‹€æ…‹={current_state}, GS={current_gs is not None}")
            return True
            
        except Exception as e:
            error_log(f"[LLM] æœƒè©±æ¶æ§‹æª¢æŸ¥å¤±æ•—: {e}")
            return False
    
    def _publish_llm_response_event(self, output: LLMOutput, mode: str, extra_data: Dict[str, Any]):
        """ç™¼å¸ƒ LLM å›æ‡‰ç”Ÿæˆäº‹ä»¶"""
        try:
            from core.event_bus import event_bus, SystemEvent
            
            event_data = {
                "mode": mode,
                "response": output.text,  # ä½¿ç”¨ "response" è€Œé "response_text" ä»¥ç¬¦åˆæ¸¬è©¦
                "confidence": output.confidence,
                "processing_time": output.processing_time,
                "tokens_used": output.tokens_used,
                "session_id": getattr(self, '_current_processing_session_id', None),
                "cycle_index": getattr(self, '_current_processing_cycle_index', None),
                "success": output.success,
                **extra_data
            }
            
            event_bus.publish(
                SystemEvent.LLM_RESPONSE_GENERATED,
                event_data,
                source="llm"
            )
            
            debug_log(2, f"[LLM] å·²ç™¼å¸ƒ LLM_RESPONSE_GENERATED äº‹ä»¶ (mode={mode})")
            
        except Exception as e:
            error_log(f"[LLM] ç™¼å¸ƒå›æ‡‰äº‹ä»¶å¤±æ•—: {e}")
    
    def _publish_learning_data_event(self, identity_id: str, interaction_type: str, 
                                     learning_signals: Dict, user_input: str, system_response: str):
        """ç™¼å¸ƒå­¸ç¿’è³‡æ–™è¿”å›äº‹ä»¶"""
        try:
            from core.event_bus import event_bus, SystemEvent
            
            event_data = {
                "identity_id": identity_id,
                "interaction_type": interaction_type,
                "learning_signals": learning_signals,
                "user_input_length": len(user_input),
                "response_length": len(system_response),
                "session_id": getattr(self, '_current_processing_session_id', None),
                "timestamp": time.time()
            }
            
            event_bus.publish(
                SystemEvent.LLM_LEARNING_DATA_RETURNED,
                event_data,
                source="llm"
            )
            
            debug_log(2, f"[LLM] å·²ç™¼å¸ƒ LLM_LEARNING_DATA_RETURNED äº‹ä»¶")
            
        except Exception as e:
            error_log(f"[LLM] ç™¼å¸ƒå­¸ç¿’è³‡æ–™äº‹ä»¶å¤±æ•—: {e}")

