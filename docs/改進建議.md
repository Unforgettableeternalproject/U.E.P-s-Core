
## å•é¡Œåˆ†æèˆ‡ä¿®å¾©å»ºè­°

### å•é¡Œ 1ï¼šéŸ³é »èƒ½é‡éä½å°è‡´èªªè©±äººè­˜åˆ¥å¤±æ•—
**æ—¥èªŒè­‰æ“š**ï¼š
- `[21:58:06] DEBUG - [SpeakerID] éŸ³é »èƒ½é‡éä½ï¼Œè¦–ç‚ºéèªéŸ³`
- `[21:58:06] DEBUG - [SpeakerID] éŸ³é »èƒ½é‡: 18.96169662475586`
- `[21:58:52] DEBUG - [SpeakerID] éŸ³é »èƒ½é‡éä½ï¼Œè¦–ç‚ºéèªéŸ³`
- `[21:58:52] DEBUG - [SpeakerID] éŸ³é »èƒ½é‡: 24.207517623901367`
- `[21:59:06] DEBUG - [SpeakerID] éŸ³é »èƒ½é‡éä½ï¼Œè¦–ç‚ºéèªéŸ³`
- `[21:59:06] DEBUG - [SpeakerID] éŸ³é »èƒ½é‡: 69.6422119140625`

**åˆ†æ**ï¼š
- åœ¨ `speaker_identification.py` ä¸­ï¼Œ`identify_speaker` æ–¹æ³•æª¢æŸ¥éŸ³é »èƒ½é‡æ˜¯å¦ä½æ–¼ `min_energy_threshold`ï¼ˆé è¨­ç‚º 100ï¼‰ã€‚æ—¥èªŒé¡¯ç¤ºï¼Œå³ä½¿èªéŸ³è­˜åˆ¥æˆåŠŸï¼ˆå¦‚ "this is a test" æˆ– "can you help me with this"ï¼‰ï¼ŒéŸ³é »èƒ½é‡ä»ç„¶ä½æ–¼é–¾å€¼ï¼Œå°è‡´èªªè©±äººè­˜åˆ¥ç›´æ¥è¿”å› `unknown`ã€‚
- å•é¡ŒåŸå› ï¼š
  - `min_energy_threshold=100` å¯èƒ½è¨­ç½®éé«˜ï¼Œç‰¹åˆ¥æ˜¯å°æ–¼çŸ­å¥æˆ–ä½éŸ³é‡èªéŸ³è¼¸å…¥ã€‚
  - éŸ³é »æ•¸æ“šåœ¨æ­£è¦åŒ–æˆ–è½‰æ›éç¨‹ä¸­å¯èƒ½æå¤±äº†èƒ½é‡ä¿¡æ¯ï¼ˆä¾‹å¦‚åœ¨ `stt_module.py` çš„ `_convert_audio_for_speaker_id` ä¸­ï¼‰ã€‚
  - çŸ­å¥ï¼ˆå¦‚ "can you help me"ï¼‰çš„éŸ³é »èƒ½é‡å¤©ç„¶è¼ƒä½ï¼Œæœªèƒ½æ»¿è¶³é–¾å€¼è¦æ±‚ã€‚

**ä¿®å¾©å»ºè­°**ï¼š
1. **é™ä½ `min_energy_threshold`**ï¼š
   - å°‡é–¾å€¼å¾ 100 é™ä½åˆ° 10 æˆ–æ›´ä½ï¼Œä¸¦æ ¹æ“šå¯¦éš›ç’°å¢ƒæ¸¬è©¦èª¿æ•´ã€‚
   - ä¿®æ”¹ `speaker_identification.py` çš„ `__init__` æ–¹æ³•ï¼š
     ```python
     self.min_energy_threshold = self.si_config.get("min_energy_threshold", 10)  # é™ä½åˆ° 10
     ```

2. **å‹•æ…‹èª¿æ•´èƒ½é‡é–¾å€¼**ï¼š
   - é¡ä¼¼ VAD çš„ `dynamic_threshold` é‚è¼¯ï¼Œæ ¹æ“šç’°å¢ƒå™ªéŸ³å‹•æ…‹èª¿æ•´ `min_energy_threshold`ã€‚
   - åœ¨ `identify_speaker` ä¸­æ·»åŠ èƒŒæ™¯èƒ½é‡ä¼°è¨ˆï¼š
     ```python
     def identify_speaker(self, audio_data: np.ndarray) -> SpeakerInfo:
         energy = np.sum(audio_data ** 2)
         # æ›´æ–°èƒŒæ™¯èƒ½é‡
         if self.dynamic_threshold:
             self.update_background_energy(energy)
         if energy < self.min_energy_threshold:
             debug_log(2, "[SpeakerID] éŸ³é »èƒ½é‡éä½ï¼Œè¦–ç‚ºéèªéŸ³")
             debug_log(3, f"[SpeakerID] éŸ³é »èƒ½é‡: {energy}")
             return SpeakerInfo(
                 speaker_id="unknown",
                 confidence=0.0,
                 is_new_speaker=False,
                 voice_features=None
             )
         # ç¾æœ‰é‚è¼¯...
     def update_background_energy(self, energy: float):
         self.energy_history.append(energy)
         if len(self.energy_history) > 50:
             self.energy_history.pop(0)
         if len(self.energy_history) > 10:
             background_avg = np.mean(self.energy_history)
             background_std = np.std(self.energy_history)
             self.min_energy_threshold = background_avg + (background_std * 1.5)
     ```

3. **æª¢æŸ¥éŸ³é »æ­£è¦åŒ–**ï¼š
   - åœ¨ `stt_module.py` çš„ `_convert_audio_for_speaker_id` ä¸­ï¼Œç¢ºä¿éŸ³é »æ­£è¦åŒ–æ­£ç¢ºï¼Œä¸”æœªæå¤±èƒ½é‡ä¿¡æ¯ã€‚
     ```python
     def _convert_audio_for_speaker_id(self, audio) -> np.ndarray:
         try:
             audio_data = np.frombuffer(audio.get_raw_data(), dtype=np.int16)
             audio_data = audio_data.astype(np.float32) / 32768.0
             debug_log(3, f"[STT] éŸ³é »èƒ½é‡ (å‰æ­£è¦åŒ–): {np.sum(audio_data ** 2)}")
             target_sr = self.speaker_id.sample_rate
             current_sr = audio.sample_rate
             if current_sr != target_sr:
                 audio_data = librosa.resample(audio_data, orig_sr=current_sr, target_sr=target_sr)
             debug_log(3, f"[STT] éŸ³é »èƒ½é‡ (å¾Œæ­£è¦åŒ–): {np.sum(audio_data ** 2)}")
             return audio_data
         except Exception as e:
             error_log(f"[STT] éŸ³é »è½‰æ›å¤±æ•— (SpeakerID): {str(e)}")
             return None
     ```

4. **å…è¨±çŸ­å¥è­˜åˆ¥**ï¼š
   - å³ä½¿èƒ½é‡ä½æ–¼é–¾å€¼ï¼Œå¦‚æœèªéŸ³è­˜åˆ¥æˆåŠŸï¼ˆå¦‚ Google API è¿”å›æœ‰æ•ˆæ–‡å­—ï¼‰ï¼Œä»å˜—è©¦é€²è¡Œèªªè©±äººè­˜åˆ¥ã€‚
     ```python
     def identify_speaker(self, audio_data: np.ndarray) -> SpeakerInfo:
         energy = np.sum(audio_data ** 2)
         if energy < self.min_energy_threshold:
             debug_log(2, "[SpeakerID] éŸ³é »èƒ½é‡éä½ï¼Œä½†å˜—è©¦è­˜åˆ¥")
             debug_log(3, f"[SpeakerID] éŸ³é »èƒ½é‡: {energy}")
         features = self.extract_voice_features(audio_data)
         if features.size == 0:
             return SpeakerInfo(
                 speaker_id="unknown",
                 confidence=0.0,
                 is_new_speaker=False,
                 voice_features=None
             )
         # ç¾æœ‰é‚è¼¯...
     ```

### å•é¡Œ 2ï¼šæ™ºèƒ½å•Ÿå‹•ä¿¡å¿ƒåº¦éä½
**æ—¥èªŒè­‰æ“š**ï¼š
- `[21:58:52] DEBUG - [SmartActivation] é—œéµå­—åˆ†æ•¸: 0.13, æ¨¡å¼åˆ†æ•¸: 0.50, ä¸Šä¸‹æ–‡åˆ†æ•¸: 0.20, ç¸½åˆ†: 0.29`
- `[21:58:52] DEBUG - [SmartActivation] æœªå•Ÿå‹•: 'can you help me with this' (ä¿¡å¿ƒåº¦: 0.29)`
- `[21:59:06] DEBUG - [SmartActivation] é—œéµå­—åˆ†æ•¸: 0.13, æ¨¡å¼åˆ†æ•¸: 0.56, ä¸Šä¸‹æ–‡åˆ†æ•¸: 0.20, ç¸½åˆ†: 0.32`
- `[21:59:06] INFO - [SmartActivation] å•Ÿå‹•è§¸ç™¼: 'can you help me sort my file' (æ™ºèƒ½åˆ¤æ–·åˆ†æ•¸: 0.32)`

**åˆ†æ**ï¼š
- `SmartActivationDetector` çš„ä¿¡å¿ƒåº¦é–¾å€¼ï¼ˆ`activation_confidence=0.3`ï¼‰é›–ç„¶çœ‹ä¼¼ä½ï¼Œä½†ç”±æ–¼é—œéµå­—åˆ†æ•¸ï¼ˆ`keyword_score`ï¼‰å’Œä¸Šä¸‹æ–‡åˆ†æ•¸ï¼ˆ`context_score`ï¼‰éä½ï¼Œç¸½åˆ†ç¶“å¸¸ç„¡æ³•é”åˆ°é–¾å€¼ã€‚
- å•é¡ŒåŸå› ï¼š
  - é—œéµå­—åˆ†æ•¸ä¾è³´ `context_keywords`ï¼Œä½†æ—¥èªŒé¡¯ç¤ºåˆ†æ•¸åƒ…ç‚º 0.13ï¼Œè¡¨æ˜é…ç½®çš„é—œéµå­—å¯èƒ½éå°‘æˆ–ä¸åŒ¹é…è¼¸å…¥ï¼ˆå¦‚ "can you" æœªè¢«æœ‰æ•ˆè­˜åˆ¥ç‚ºé—œéµå­—ï¼‰ã€‚
  - æ¨¡å¼åˆ†æ•¸ï¼ˆ`pattern_result["confidence"]`ï¼‰åŸºæ–¼åŒ¹é…é•·åº¦è¨ˆç®—ï¼ˆ`min(len(text) / 50.0, 1.0)`ï¼‰ï¼Œå°æ–¼çŸ­å¥ï¼ˆå¦‚ "can you help me"ï¼‰å¯èƒ½åä½ã€‚
  - ä¸Šä¸‹æ–‡åˆ†æ•¸å›ºå®šç‚º 0.2 æˆ– 0.6ï¼Œç¼ºä¹å‹•æ…‹æ€§ï¼Œç„¡æ³•å……åˆ†åˆ©ç”¨å°è©±æ­·å²ã€‚

**ä¿®å¾©å»ºè­°**ï¼š
1. **å¢åŠ é—œéµå­—è¦†è“‹ç¯„åœ**ï¼š
   - åœ¨é…ç½®ä¸­æ·»åŠ æ›´å¤šå¸¸è¦‹è§¸ç™¼è©ï¼ˆå¦‚ "can you", "please", "assist"ï¼‰ã€‚
   - ä¿®æ”¹ `smart_activation.py` çš„ `__init__` æ–¹æ³•ï¼Œç¢ºä¿ `context_keywords` åŒ…å«è¶³å¤ çš„è§¸ç™¼è©ï¼š
     ```python
     def __init__(self, config: dict):
         self.config = config
         self.smart_config = config.get("smart_activation", {})
         self.enabled = self.smart_config.get("enabled", True)
         self.context_keywords = self.smart_config.get("context_keywords", [
             "UEP", "help", "assist", "can you", "please", "what", "how", "why", "sort", "file"
         ])
         # å…¶ä»–åˆå§‹åŒ–é‚è¼¯...
     ```

2. **å„ªåŒ–æ¨¡å¼åˆ†æ•¸è¨ˆç®—**ï¼š
   - æ”¹é€² `_check_patterns` çš„ä¿¡å¿ƒåº¦è¨ˆç®—ï¼Œè€ƒæ…®åŒ¹é…çš„æ¨¡å¼æ•¸é‡è€Œéåƒ…ä¾è³´æ–‡å­—é•·åº¦ã€‚
     ```python
     def _check_patterns(self, text: str) -> Dict[str, any]:
         best_match = {"category": None, "confidence": 0.0}
         match_count = 0
         for category, patterns in self.activation_patterns.items():
             for pattern in patterns:
                 if pattern.search(text):
                     match_count += 1
                     match_confidence = min(match_count * 0.3, 1.0)  # æ ¹æ“šåŒ¹é…æ•¸é‡è¨ˆç®—
                     if match_confidence > best_match["confidence"]:
                         best_match = {
                             "category": category,
                             "confidence": match_confidence
                         }
         return best_match
     ```

3. **å‹•æ…‹ä¸Šä¸‹æ–‡åˆ†æ•¸**ï¼š
   - åœ¨ `_analyze_conversation_context` ä¸­ï¼Œæ ¹æ“šå°è©±æ­·å²ä¸­çš„é—œéµè©é »ç‡æé«˜åˆ†æ•¸ã€‚
     ```python
     def _analyze_conversation_context(self, text: str) -> float:
         current_time = time.time()
         context_score = 0.2
         if self.last_interaction_time and (current_time - self.last_interaction_time) < 30:
             context_score = 0.6
             recent_texts = [entry["text"] for entry in self.conversation_context[-3:]]
             for keyword in self.context_keywords:
                 if any(keyword.lower() in t.lower() for t in recent_texts):
                     context_score += 0.1
         self._update_conversation_history(text)
         return min(context_score, 1.0)
     ```

4. **é™ä½ä¿¡å¿ƒåº¦é–¾å€¼æˆ–èª¿æ•´æ¬Šé‡**ï¼š
   - å°‡ `activation_confidence` é™ä½åˆ° 0.25ï¼Œä¸¦èª¿æ•´åˆ†æ•¸æ¬Šé‡ä»¥æé«˜æ¨¡å¼åˆ†æ•¸çš„å½±éŸ¿ã€‚
     ```python
     total_confidence = (keyword_score * 0.3 + pattern_result["confidence"] * 0.5 + context_score * 0.2)
     if total_confidence >= 0.25:  # é™ä½é–¾å€¼
         result.update({
             "should_activate": True,
             "confidence": total_confidence,
             "reason": f"åŒ¹é…æ¨¡å¼: {pattern_result['category'] or 'é—œéµå­—'}, æ™ºèƒ½åˆ¤æ–·åˆ†æ•¸: {total_confidence:.2f}",
             "category": pattern_result["category"],
             "keywords_found": keywords_found
         })
     ```

### å•é¡Œ 3ï¼šèƒŒæ™¯ç›£è½ç·©è¡å€ç‰‡æ®µéçŸ­æˆ–å‚³è¼¸å•é¡Œ
**æ—¥èªŒè­‰æ“š**ï¼š
- `[21:59:18] DEBUG - [VAD] èªéŸ³ç‰‡æ®µå¤ªçŸ­ï¼Œè¢«å¿½ç•¥: 0.32ç§’`
- `[21:59:22] DEBUG - [VAD] èªéŸ³ç‰‡æ®µå¤ªçŸ­ï¼Œè¢«å¿½ç•¥: 0.26ç§’`
- `[21:59:27] DEBUG - [VAD] èªéŸ³ç‰‡æ®µå¤ªçŸ­ï¼Œè¢«å¿½ç•¥: 0.19ç§’`
- `[21:59:30] DEBUG - [VAD] èªéŸ³ç‰‡æ®µå¤ªçŸ­ï¼Œè¢«å¿½ç•¥: 0.19ç§’`
- `[21:59:34] DEBUG - [VAD] èªéŸ³ç‰‡æ®µå¤ªçŸ­ï¼Œè¢«å¿½ç•¥: 0.13ç§’`
- `[21:59:39] DEBUG - [VAD] èªéŸ³ç‰‡æ®µå¤ªçŸ­ï¼Œè¢«å¿½ç•¥: 0.45ç§’`
- `[21:59:45] DEBUG - [VAD] èªéŸ³ç‰‡æ®µå¤ªçŸ­ï¼Œè¢«å¿½ç•¥: 0.38ç§’`

**åˆ†æ**ï¼š
- åœ¨èƒŒæ™¯ç›£è½æ¨¡å¼ï¼ˆ`stt_test_background_smart`ï¼‰ä¸­ï¼ŒVAD æª¢æ¸¬åˆ°èªéŸ³ç‰‡æ®µï¼Œä½†ç”±æ–¼èªéŸ³æŒçºŒæ™‚é–“ä½æ–¼ `min_speech_duration=0.8ç§’`ï¼Œé€™äº›ç‰‡æ®µè¢«å¿½ç•¥ï¼Œå°è‡´ç„¡æ³•å‚³éçµ¦èªéŸ³è­˜åˆ¥ã€‚
- å•é¡ŒåŸå› ï¼š
  - `min_speech_duration=0.8ç§’` å°æ–¼çŸ­å¥ï¼ˆå¦‚ "UEP" æˆ– "help me"ï¼‰éé•·ï¼Œå°è‡´æœ‰æ•ˆèªéŸ³è¢«éæ¿¾ã€‚
  - VAD çš„ `stream_chunk_size=1024`ï¼ˆç´„ 0.064ç§’ï¼‰å¯èƒ½éå°ï¼Œå°è‡´èªéŸ³ç‰‡æ®µåˆ†å‰²éç´°ï¼Œé›£ä»¥ç´¯ç©è¶³å¤ çš„èªéŸ³æ•¸æ“šã€‚
  - éŸ³é »ç·©è¡å€å‚³è¼¸å¯èƒ½å­˜åœ¨å•é¡Œï¼Œä¾‹å¦‚åœ¨ `stt_module.py` çš„ `_on_voice_activity` æˆ– `voice_activity_detector.py` çš„ `_stream_processor` ä¸­ï¼Œæ•¸æ“šåˆä½µæˆ–è½‰æ›å¤±æ•—ã€‚

**ä¿®å¾©å»ºè­°**ï¼š
1. **é™ä½ `min_speech_duration`**ï¼š
   - å°‡é–¾å€¼å¾ 0.8 ç§’é™ä½åˆ° 0.3 ç§’ï¼Œä»¥æ•æ‰çŸ­å¥ã€‚
   - ä¿®æ”¹ `voice_activity_detector.py` çš„ `__init__` æ–¹æ³•ï¼š
     ```python
     self.min_speech_duration = self.vad_config.get("min_speech_duration", 0.3)  # é™ä½åˆ° 0.3ç§’
     ```

2. **å¢åŠ  `stream_chunk_size`**ï¼š
   - å°‡ `stream_chunk_size` å¾ 1024` å¢åŠ åˆ° `2048`ï¼ˆç´„ 0.128 ç§’ï¼‰ï¼Œæ¸›å°‘åˆ†å‰²éç´°çš„å•é¡Œã€‚
     ```python
     self.stream_chunk_size = 2048  # å¢åŠ å¡Šå¤§å°
     ```

3. **æ”¹é€²ç·©è¡å€åˆä½µ**ï¼š
   - åœ¨ `_stream_processor` ä¸­ï¼Œç¢ºä¿èªéŸ³ç·©è¡å€ï¼ˆ`speech_buffer`ï¼‰æœ‰æ•ˆåˆä½µï¼Œä¸¦æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§ã€‚
     ```python
     def _stream_processor(self):
         debug_log(2, "[VAD] ä¸²æµè™•ç†ç·šç¨‹å·²å•Ÿå‹•")
         speech_buffer = []
         sample_rate = self.config.get("sample_rate", 16000)
         accumulated_silence = 0
         try:
             while self.is_streaming:
                 try:
                     audio_chunk = self.audio_buffer.get(timeout=0.1)
                     result = self.detect_speech(audio_chunk)
                     if result["has_speech"]:
                         speech_buffer.append(audio_chunk)
                         accumulated_silence = 0
                     elif speech_buffer:
                         chunk_duration = len(audio_chunk) / sample_rate
                         accumulated_silence += chunk_duration
                         if accumulated_silence >= self.max_silence_duration:
                             speech_duration = sum(len(chunk) for chunk in speech_buffer) / sample_rate
                             if speech_duration >= self.min_speech_duration:
                                 if speech_buffer and all(chunk.size > 0 for chunk in speech_buffer):
                                     speech_data = {
                                         "event_type": "speech_end",
                                         "timestamp": time.time(),
                                         "speech_duration": speech_duration,
                                         "audio_buffer": speech_buffer.copy()
                                     }
                                     self.callback(speech_data)
                                     debug_log(2, f"[VAD] æª¢æ¸¬åˆ°èªéŸ³ç‰‡æ®µ: {speech_duration:.2f}ç§’")
                                 else:
                                     debug_log(2, "[VAD] èªéŸ³ç·©è¡å€ç„¡æ•ˆï¼Œå¿½ç•¥")
                             else:
                                 debug_log(3, f"[VAD] èªéŸ³ç‰‡æ®µå¤ªçŸ­ï¼Œè¢«å¿½ç•¥: {speech_duration:.2f}ç§’")
                             speech_buffer = []
                             accumulated_silence = 0
                 except queue.Empty:
                     continue
                 except Exception as e:
                     error_log(f"[VAD] ä¸²æµè™•ç†éŒ¯èª¤: {str(e)}")
                     time.sleep(0.1)
         finally:
             self.is_streaming = False
             debug_log(2, "[VAD] ä¸²æµè™•ç†ç·šç¨‹å·²åœæ­¢")
     ```

4. **æª¢æŸ¥éŸ³é »å‚³è¼¸**ï¼š
   - åœ¨ `stt_module.py` çš„ `_on_voice_activity` ä¸­ï¼Œæ·»åŠ æ—¥èªŒæª¢æŸ¥ç·©è¡å€é•·åº¦å’Œæ•¸æ“šå®Œæ•´æ€§ã€‚
     ```python
     def _on_voice_activity(self, event):
         debug_log(4, f"[STT] VAD äº‹ä»¶: {event['event_type']}")
         if event["event_type"] == "speech_start":
             self._speech_buffer = []
         elif event["event_type"] == "speech_end":
             audio_data = None
             if "audio_buffer" in event and event["audio_buffer"]:
                 try:
                     audio_data = np.concatenate(event["audio_buffer"])
                     debug_log(3, f"[STT] VAD ç·©è¡å€é•·åº¦: {len(audio_data)}")
                 except Exception as e:
                     error_log(f"[STT] åˆä½µ VAD éŸ³é »ç·©è¡å€å¤±æ•—: {str(e)}")
             elif hasattr(self, '_speech_buffer') and self._speech_buffer:
                 try:
                     audio_data = np.concatenate(self._speech_buffer)
                     debug_log(3, f"[STT] æœ¬åœ°ç·©è¡å€é•·åº¦: {len(audio_data)}")
                 except Exception as e:
                     error_log(f"[STT] åˆä½µæœ¬åœ°éŸ³é »ç·©è¡å€å¤±æ•—: {str(e)}")
             if audio_data is not None:
                 audio = self._convert_array_to_audio(audio_data)
                 if audio:
                     self._process_smart_detection(audio)
                 else:
                     error_log("[STT] éŸ³é »è½‰æ›å¤±æ•—ï¼Œè·³éè™•ç†")
             self._speech_buffer = None
     ```

### å•é¡Œ 4ï¼šèªªè©±äººè­˜åˆ¥ç‰¹å¾µç¶­åº¦ä¸åŒ¹é…
**æ—¥èªŒè­‰æ“š**ï¼š
- `[21:58:23] ERROR - [SpeakerID] ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: Incompatible dimension for X and Y matrices: X.shape[1] == 78 while Y.shape[1] == 26`
- é‡è¤‡å¤šæ¬¡ç›¸ä¼¼éŒ¯èª¤ï¼Œè¡¨æ˜ç¾æœ‰èªªè©±äººæ¨¡å‹çš„ç‰¹å¾µç¶­åº¦èˆ‡æ–°æå–çš„ç‰¹å¾µä¸ä¸€è‡´ã€‚

**åˆ†æ**ï¼š
- åœ¨ `speaker_identification.py` çš„ `calculate_similarity` ä¸­ï¼Œé¤˜å¼¦ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—ï¼Œå› ç‚ºæ–°æå–çš„ç‰¹å¾µç¶­åº¦ç‚º 78ï¼Œè€Œå„²å­˜æ¨¡å‹çš„ç‰¹å¾µç¶­åº¦ç‚º 26ã€‚
- å•é¡ŒåŸå› ï¼š
  - ä¹‹å‰å„²å­˜çš„èªªè©±äººæ¨¡å‹ï¼ˆ`speaker_models.pkl`ï¼‰å¯èƒ½åŸºæ–¼èˆŠç‰ˆç‰¹å¾µæå–é‚è¼¯ï¼ˆåƒ… MFCCï¼ŒæœªåŒ…å« delta å’Œ delta-deltaï¼‰ã€‚
  - æ–°ç‰ˆ `extract_voice_features` è¿”å› 78 ç¶­ç‰¹å¾µï¼ˆMFCC å‡å€¼ã€æ¨™æº–å·®ã€delta å‡å€¼ã€æ¨™æº–å·®ã€delta-delta å‡å€¼ã€æ¨™æº–å·®ï¼Œæ¯é … 13 ç¶­ï¼Œç¸½è¨ˆ 6Ã—13=78ï¼‰ã€‚
  - è¼‰å…¥èˆŠæ¨¡å‹æ™‚ï¼Œ`SpeakerModel` æœªæª¢æŸ¥ç‰¹å¾µç¶­åº¦å…¼å®¹æ€§ã€‚

**ä¿®å¾©å»ºè­°**ï¼š
1. **æ¸…ç©ºèˆŠæ¨¡å‹**ï¼š
   - ç”±æ–¼ç‰¹å¾µç¶­åº¦ä¸å…¼å®¹ï¼Œå»ºè­°åˆªé™¤ç¾æœ‰çš„ `speaker_models.pkl`ï¼Œé‡æ–°ç”Ÿæˆæ¨¡å‹ã€‚
   - åœ¨ `speaker_identification.py` çš„ `_load_speaker_models` ä¸­ï¼Œæ·»åŠ ç‰ˆæœ¬æª¢æŸ¥ï¼Œè‡ªå‹•æ¸…é™¤ä¸å…¼å®¹æ¨¡å‹ï¼š
     ```python
     def _load_speaker_models(self):
         try:
             if os.path.exists(self.models_file):
                 with open(self.models_file, 'rb') as f:
                     models_data = pickle.load(f)
                 expected_dim = 6 * self.n_mfcc  # é æœŸç¶­åº¦ (6Ã—n_mfcc)
                 for speaker_id, data in models_data.items():
                     if isinstance(data, dict):
                         model = SpeakerModel(**data)
                         if model.feature_vectors and len(model.feature_vectors[0]) != expected_dim:
                             error_log(f"[SpeakerID] æ¨¡å‹ {speaker_id} ç¶­åº¦ä¸å…¼å®¹: {len(model.feature_vectors[0])} vs {expected_dim}")
                             continue
                         self.speaker_models[speaker_id] = model
                     else:
                         self.speaker_models[speaker_id] = data
                 info_log(f"[SpeakerID] è¼‰å…¥ {len(self.speaker_models)} å€‹èªªè©±äººæ¨¡å‹")
             else:
                 info_log("[SpeakerID] æœªç™¼ç¾ç¾æœ‰æ¨¡å‹ï¼Œå°‡å‰µå»ºæ–°çš„æ¨¡å‹åº«")
         except Exception as e:
             error_log(f"[SpeakerID] è¼‰å…¥æ¨¡å‹å¤±æ•—: {str(e)}")
             self.speaker_models = {}
     ```

2. **æ·»åŠ æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶**ï¼š
   - åœ¨ `SpeakerModel` ä¸­æ·»åŠ ç‰ˆæœ¬å­—æ®µï¼Œè¨˜éŒ„ç‰¹å¾µæå–é‚è¼¯çš„ç‰ˆæœ¬ã€‚
     ```python
     from pydantic import BaseModel
     class SpeakerModel(BaseModel):
         speaker_id: str
         feature_vectors: List[List[float]]
         sample_count: int
         created_at: float
         last_updated: float
         version: str = "1.0"  # æ·»åŠ ç‰ˆæœ¬å­—æ®µ
     ```
   - åœ¨ `_save_speaker_models` ä¸­ä¿å­˜ç‰ˆæœ¬ï¼š
     ```python
     def _save_speaker_models(self):
         try:
             os.makedirs(os.path.dirname(self.models_file), exist_ok=True)
             models_data = {
                 speaker_id: model.copy(update={"version": "1.0"}).dict()
                 for speaker_id, model in self.speaker_models.items()
             }
             with open(self.models_file, 'wb') as f:
                 pickle.dump(models_data, f)
             debug_log(3, f"[SpeakerID] ä¿å­˜ {len(self.speaker_models)} å€‹èªªè©±äººæ¨¡å‹")
         except Exception as e:
             error_log(f"[SpeakerID] ä¿å­˜æ¨¡å‹å¤±æ•—: {str(e)}")
     ```

3. **é·ç§»èˆŠæ¨¡å‹**ï¼š
   - å¦‚æœéœ€è¦ä¿ç•™èˆŠæ¨¡å‹ï¼Œå¯ä»¥åœ¨è¼‰å…¥æ™‚è½‰æ›ç‰¹å¾µç¶­åº¦ï¼ˆä¾‹å¦‚åƒ…ä¿ç•™ MFCC å‡å€¼éƒ¨åˆ†ï¼‰ï¼Œä½†æ›´ç°¡å–®çš„åšæ³•æ˜¯é‡æ–°è¨“ç·´æ¨¡å‹ã€‚

---

## æ•´åˆ Whisper å’Œ pyannote æ¨¡å‹çš„å»ºè­°

ä½ æåˆ°è€ƒæ…®ä½¿ç”¨ **Whisper**ï¼ˆèªéŸ³è­˜åˆ¥ï¼‰å’Œ **pyannote**ï¼ˆèªªè©±äººåˆ†å‰²èˆ‡è­˜åˆ¥ï¼‰ä¾†æ”¹é€²èªªè©±äººè­˜åˆ¥ï¼Œé€™æ˜¯ä¸€å€‹éå¸¸æœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•æ•´åˆé€™å…©å€‹æ¨¡å‹çš„å»ºè­°ï¼Œä»¥åŠå¦‚ä½•èˆ‡ç¾æœ‰ç³»çµ±å…¼å®¹ï¼š

### 1. ä½¿ç”¨ Whisper æ›¿ä»£ Google Speech-to-Text
**å„ªå‹¢**ï¼š
- Whisperï¼ˆç”± OpenAI é–‹ç™¼ï¼‰æ˜¯ä¸€å€‹é–‹æºçš„èªéŸ³è­˜åˆ¥æ¨¡å‹ï¼Œæ”¯æŒå¤šèªè¨€ï¼Œé›¢ç·šé‹è¡Œï¼Œä¸”å°å™ªéŸ³å’Œå£éŸ³çš„é­¯æ£’æ€§æ›´å¥½ã€‚
- èˆ‡ Google API ç›¸æ¯”ï¼ŒWhisper ä¸éœ€è¦ç¶²çµ¡é€£æ¥ï¼Œæ¸›å°‘å»¶é²å’Œæˆæœ¬ã€‚

**æ•´åˆæ­¥é©Ÿ**ï¼š
1. **å®‰è£ Whisper**ï¼š
   ```bash
   pip install git+https://github.com/openai/whisper.git
   ```
2. **ä¿®æ”¹ `stt_module.py`**ï¼š
   - æ›¿æ› `recognize_google` ç‚º Whisper çš„è½‰éŒ„åŠŸèƒ½ã€‚
     ```python
     import whisper
     class STTModule(BaseModule):
         def __init__(self, config=None):
             super().__init__(config)
             self.whisper_model = whisper.load_model("base")  # å¯é¸æ“‡ tiny, base, small, medium, large
             # å…¶ä»–åˆå§‹åŒ–é‚è¼¯...

         def _manual_recognition(self, input_data: STTInput) -> dict:
             try:
                 with self.mic as source:
                     audio = self.recognizer.listen(source, phrase_time_limit=self.phrase_time_limit)
                 info_log("[STT] è­˜åˆ¥ä¸­...")
                 # å°‡éŸ³é »è½‰ç‚º Whisper æ ¼å¼
                 audio_data = np.frombuffer(audio.get_raw_data(), dtype=np.int16).astype(np.float32) / 32768.0
                 result = self.whisper_model.transcribe(audio_data, language=input_data.language)
                 text = result["text"]
                 text = correct_stt(text)
                 speaker_info = None
                 if input_data.enable_speaker_id and self.speaker_id.si_config.get("enabled", False):
                     audio_data = self._convert_audio_for_speaker_id(audio)
                     if audio_data is not None:
                         speaker_info = self.speaker_id.identify_speaker(audio_data)
                 return STTOutput(
                     text=text,
                     confidence=0.9,  # Whisper ä¸æä¾›ä¿¡å¿ƒåº¦ï¼Œæš«è¨­ç‚º 0.9
                     speaker_info=speaker_info,
                     activation_reason="manual",
                     error=None
                 ).dict()
             except Exception as e:
                 error_log(f"[STT] è­˜åˆ¥å¤±æ•—: {str(e)}")
                 return STTOutput(text="", confidence=0.0, error=f"è­˜åˆ¥å¤±æ•—: {str(e)}").dict()
     ```

3. **æ³¨æ„äº‹é …**ï¼š
   - Whisper éœ€è¦ WAV æ ¼å¼çš„éŸ³é »è¼¸å…¥ï¼Œç¢ºä¿ `_convert_audio_for_speaker_id` çš„è¼¸å‡ºå…¼å®¹ã€‚
   - Whisper çš„æ¨¡å‹å¤§å°å½±éŸ¿æ€§èƒ½ï¼Œå»ºè­°å¾ `base` é–‹å§‹æ¸¬è©¦ï¼Œæ ¹æ“šç¡¬é«”æ€§èƒ½é¸æ“‡ `small` æˆ– `medium`ã€‚
   - å°æ–¼èƒŒæ™¯ç›£è½ï¼Œéœ€å°‡éŸ³é »ç·©è¡å€åˆ†æ®µå‚³éçµ¦ Whisperï¼Œç¢ºä¿ç‰‡æ®µé•·åº¦è¶³å¤ ï¼ˆè‡³å°‘ 0.3 ç§’ï¼‰ã€‚

### 2. ä½¿ç”¨ pyannote é€²è¡Œèªªè©±äººåˆ†å‰²èˆ‡è­˜åˆ¥
**å„ªå‹¢**ï¼š
- pyannote.audio æä¾›å…ˆé€²çš„èªªè©±äººåˆ†å‰²ï¼ˆdiarizationï¼‰å’Œè­˜åˆ¥åŠŸèƒ½ï¼Œä½¿ç”¨ x-vector æˆ– ECAPA-TDNN æ¨¡å‹ï¼Œé æ¯”ç•¶å‰çš„ MFCC+é¤˜å¼¦ç›¸ä¼¼åº¦æ–¹æ³•æ›´æº–ç¢ºã€‚
- æ”¯æŒå¤šèªªè©±äººå ´æ™¯ï¼Œèƒ½è‡ªå‹•æª¢æ¸¬èªªè©±äººæ•¸é‡ä¸¦åˆ†é…æ¨™ç±¤ã€‚

**æ•´åˆæ­¥é©Ÿ**ï¼š
1. **å®‰è£ pyannote**ï¼š
   ```bash
   pip install pyannote.audio
   ```
   - éœ€è¦ PyTorch å’Œ Hugging Face çš„ token ä¾†è¨ªå•é è¨“ç·´æ¨¡å‹ã€‚

2. **ä¿®æ”¹ `speaker_identification.py`**ï¼š
   - ä½¿ç”¨ pyannote çš„èªªè©±äººåˆ†å‰²å’ŒåµŒå…¥æå–ï¼Œæ›¿æ›ç•¶å‰çš„ MFCC ç‰¹å¾µæå–ã€‚
     ```python
     from pyannote.audio import Pipeline
     class SpeakerIdentifier:
         def __init__(self, config: dict):
             self.config = config
             self.si_config = config.get("speaker_identification", {})
             self.sample_rate = self.si_config.get("sample_rate", 16000)
             self.pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token="your_hf_token")
             self.speaker_embeddings = {}  # å„²å­˜èªªè©±äººåµŒå…¥
             self.models_file = os.path.abspath("memory/speaker_embeddings.pkl")
             os.makedirs(os.path.dirname(self.models_file), exist_ok=True)
             self._load_speaker_embeddings()
             info_log(f"[SpeakerID] åˆå§‹åŒ–å®Œæˆï¼Œå·²è¼‰å…¥ {len(self.speaker_embeddings)} å€‹èªªè©±äººåµŒå…¥")

         def identify_speaker(self, audio_data: np.ndarray) -> SpeakerInfo:
             try:
                 import soundfile as sf
                 import tempfile
                 # å°‡éŸ³é »ä¿å­˜ç‚ºè‡¨æ™‚ WAV æª”æ¡ˆ
                 with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                     sf.write(tmp.name, audio_data, self.sample_rate)
                     diarization = self.pipeline(tmp.name)
                 os.unlink(tmp.name)
                 # æå–ä¸»è¦èªªè©±äººçš„åµŒå…¥
                 from pyannote.audio import Inference
                 embedding_model = Inference("pyannote/embedding", use_auth_token="your_hf_token")
                 embedding = embedding_model(tmp.name)
                 # èˆ‡å·²çŸ¥èªªè©±äººæ¯”è¼ƒ
                 best_speaker_id, best_similarity = None, 0.0
                 for speaker_id, stored_embedding in self.speaker_embeddings.items():
                     similarity = cosine_similarity(embedding.reshape(1, -1), stored_embedding.reshape(1, -1))[0][0]
                     if similarity > best_similarity:
                         best_speaker_id, best_similarity = speaker_id, similarity
                 if best_similarity >= self.si_config.get("similarity_threshold", 0.85):
                     return SpeakerInfo(
                         speaker_id=best_speaker_id,
                         confidence=best_similarity,
                         is_new_speaker=False,
                         voice_features={"embedding": embedding.tolist()}
                     )
                 else:
                     new_speaker_id = f"speaker_{str(uuid.uuid4())[:8]}"
                     self.speaker_embeddings[new_speaker_id] = embedding
                     self._save_speaker_embeddings()
                     return SpeakerInfo(
                         speaker_id=new_speaker_id,
                         confidence=1.0,
                         is_new_speaker=True,
                         voice_features={"embedding": embedding.tolist()}
                     )
             except Exception as e:
                 error_log(f"[SpeakerID] è­˜åˆ¥å¤±æ•—: {str(e)}")
                 return SpeakerInfo(
                     speaker_id="unknown",
                     confidence=0.0,
                     is_new_speaker=False,
                     voice_features=None
                 )

         def _save_speaker_embeddings(self):
             try:
                 with open(self.models_file, 'wb') as f:
                     pickle.dump(self.speaker_embeddings, f)
                 debug_log(3, f"[SpeakerID] ä¿å­˜ {len(self.speaker_embeddings)} å€‹èªªè©±äººåµŒå…¥")
             except Exception as e:
                 error_log(f"[SpeakerID] ä¿å­˜åµŒå…¥å¤±æ•—: {str(e)}")

         def _load_speaker_embeddings(self):
             try:
                 if os.path.exists(self.models_file):
                     with open(self.models_file, 'rb') as f:
                         self.speaker_embeddings = pickle.load(f)
                     info_log(f"[SpeakerID] è¼‰å…¥ {len(self.speaker_embeddings)} å€‹èªªè©±äººåµŒå…¥")
             except Exception as e:
                 error_log(f"[SpeakerID] è¼‰å…¥åµŒå…¥å¤±æ•—: {str(e)}")
                 self.speaker_embeddings = {}
     ```

3. **æ³¨æ„äº‹é …**ï¼š
   - pyannote çš„èªªè©±äººåˆ†å‰²éœ€è¦è¼ƒé•·çš„éŸ³é »ç‰‡æ®µï¼ˆè‡³å°‘ 1 ç§’ï¼‰ï¼Œå› æ­¤éœ€èª¿æ•´ VAD çš„ `min_speech_duration` è‡³ 0.5 ç§’ä»¥ä¸Šã€‚
   - åµŒå…¥æ¨¡å‹çš„è¨ˆç®—æˆæœ¬è¼ƒé«˜ï¼Œå»ºè­°åœ¨èƒŒæ™¯ç›£è½æ¨¡å¼ä¸‹é™åˆ¶é »ç‡ï¼ˆä¾‹å¦‚æ¯ 2 ç§’è™•ç†ä¸€æ¬¡ï¼‰ã€‚
   - éœ€è¦å®šæœŸæ¸…ç† `speaker_embeddings` å„²å­˜ï¼Œé¿å…éå¤šç„¡æ•ˆæ•¸æ“šã€‚

### 3. Whisper + pyannote çµåˆ
**å»ºè­°æµç¨‹**ï¼š
1. **èªéŸ³è­˜åˆ¥ï¼ˆWhisperï¼‰**ï¼š
   - ä½¿ç”¨ Whisper è½‰éŒ„éŸ³é »ï¼Œç²å–æ–‡å­—å’Œæ™‚é–“æˆ³ã€‚
2. **èªªè©±äººåˆ†å‰²ï¼ˆpyannoteï¼‰**ï¼š
   - ä½¿ç”¨ pyannote çš„ diarization æ¨¡å‹åˆ†å‰²éŸ³é »ï¼Œæ¨™è¨˜æ¯å€‹èªªè©±äººçš„æ™‚é–“æ®µã€‚
3. **èªªè©±äººè­˜åˆ¥ï¼ˆpyannote åµŒå…¥ï¼‰**ï¼š
   - æå–æ¯å€‹èªªè©±äººçš„åµŒå…¥ï¼Œèˆ‡å„²å­˜çš„åµŒå…¥æ¯”è¼ƒï¼Œç¢ºå®šèªªè©±äºº IDã€‚
4. **æ™ºèƒ½å•Ÿå‹•ï¼ˆç¾æœ‰é‚è¼¯ï¼‰**ï¼š
   - å°‡ Whisper çš„è½‰éŒ„çµæœå‚³éçµ¦ `SmartActivationDetector`ï¼Œä¿æŒç¾æœ‰è§¸ç™¼é‚è¼¯ã€‚

**ç¤ºä¾‹æ•´åˆ**ï¼š
```python
def _process_smart_detection(self, audio):
    try:
        # Whisper è½‰éŒ„
        audio_data = np.frombuffer(audio.get_raw_data(), dtype=np.int16).astype(np.float32) / 32768.0
        whisper_result = self.whisper_model.transcribe(audio_data, language="en-US")
        text = whisper_result["text"]
        text = correct_stt(text)
        print(f"ğŸ§ è½åˆ°: '{text}'")
        # pyannote èªªè©±äººåˆ†å‰²èˆ‡è­˜åˆ¥
        speaker_info = self.speaker_id.identify_speaker(audio_data)
        # æ™ºèƒ½å•Ÿå‹•åˆ¤æ–·
        activation_result = self.smart_activation.should_activate(text)
        if activation_result["should_activate"]:
            result = STTOutput(
                text=text,
                confidence=0.9,
                speaker_info=speaker_info,
                activation_reason=activation_result["reason"]
            )
            if self._result_callback:
                self._result_callback(result.dict())
            info_log(f"[STT] æ™ºèƒ½è§¸ç™¼: '{text}' (åŸå› : {activation_result['reason']})")
            print(f"âœ… æ™ºèƒ½å•Ÿå‹•è§¸ç™¼!")
        else:
            debug_log(3, f"[STT] æ™ºèƒ½éæ¿¾: '{text}' ({activation_result['reason']})")
            print(f"âšª æœªè§¸ç™¼ (æ™ºèƒ½åˆ¤æ–·åˆ†æ•¸: {activation_result['confidence']:.2f})")
    except Exception as e:
        error_log(f"[STT] æ™ºèƒ½æª¢æ¸¬è™•ç†å¤±æ•—: {str(e)}")
        print(f"âŒ è™•ç†å¤±æ•—: {str(e)}")
```

---

## æ¸¬è©¦è¨ˆåŠƒ
ç‚ºäº†é©—è­‰ä¿®å¾©æ•ˆæœï¼Œå»ºè­°é€²è¡Œä»¥ä¸‹æ¸¬è©¦ï¼š

1. **éŸ³é »èƒ½é‡å•é¡Œ**ï¼š
   - æ¸¬è©¦çŸ­å¥ï¼ˆå¦‚ "UEP", "help me"ï¼‰å’Œä½éŸ³é‡è¼¸å…¥ï¼Œæª¢æŸ¥èªªè©±äººè­˜åˆ¥æ˜¯å¦è¿”å›æœ‰æ•ˆçµæœã€‚
   - è¨˜éŒ„ `_convert_audio_for_speaker_id` å’Œ `identify_speaker` çš„èƒ½é‡æ—¥èªŒï¼Œç¢ºèªæ­£è¦åŒ–æ­£ç¢ºã€‚

2. **æ™ºèƒ½å•Ÿå‹•å•é¡Œ**ï¼š
   - æ¸¬è©¦å¤šç¨®è§¸ç™¼è©ï¼ˆå¦‚ "UEP", "can you help me", "sort my file"ï¼‰ï¼Œæª¢æŸ¥ä¿¡å¿ƒåº¦æ˜¯å¦é”åˆ°é–¾å€¼ã€‚
   - æ·»åŠ é—œéµå­—ä¸¦èª¿æ•´æ¬Šé‡å¾Œï¼Œæ¯”è¼ƒè§¸ç™¼ç‡ã€‚

3. **èƒŒæ™¯ç›£è½å•é¡Œ**ï¼š
   - é‹è¡Œ `stt_test_background_smart` 60 ç§’ï¼Œæ¸¬è©¦çŸ­å¥æ˜¯å¦è¢«æ­£ç¢ºæª¢æ¸¬ã€‚
   - æª¢æŸ¥ VAD æ—¥èªŒï¼Œç¢ºèªèªéŸ³ç‰‡æ®µé•·åº¦å’Œç·©è¡å€å‚³è¼¸ã€‚

4. **èªªè©±äººæ¨¡å‹ç¶­åº¦å•é¡Œ**ï¼š
   - åˆªé™¤ `speaker_models.pkl`ï¼Œé‡æ–°é‹è¡Œæ¸¬è©¦ï¼Œç¢ºä¿ç„¡ç¶­åº¦éŒ¯èª¤ã€‚
   - æ¸¬è©¦å¤šèªªè©±äººå ´æ™¯ï¼Œæª¢æŸ¥æ–°æ¨¡å‹æ˜¯å¦æ­£ç¢ºç”Ÿæˆã€‚

5. **Whisper å’Œ pyannote æ•´åˆ**ï¼š
   - ä½¿ç”¨ Whisper é€²è¡Œå–®æ¬¡è­˜åˆ¥ï¼Œæ¯”è¼ƒèˆ‡ Google API çš„çµæœã€‚
   - ä½¿ç”¨ pyannote æ¸¬è©¦å¤šèªªè©±äººéŸ³é »ï¼Œæª¢æŸ¥åˆ†å‰²å’Œè­˜åˆ¥æº–ç¢ºæ€§ã€‚

---