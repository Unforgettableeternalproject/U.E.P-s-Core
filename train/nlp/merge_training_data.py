"""
NLP è¨“ç·´è³‡æ–™åˆä½µå·¥å…·

åŠŸèƒ½ï¼š
1. æ¯”å°æ–°èˆŠè¨“ç·´è³‡æ–™
2. ç§»é™¤é‡è¤‡é …ç›®ï¼ˆæ‰¾å‡ºåäº¤é›†ï¼‰
3. åˆä½µæ–°è³‡æ–™åˆ°èˆŠè³‡æ–™é›†
4. ç”Ÿæˆçµ±è¨ˆå ±å‘Š
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Set, Tuple
from datetime import datetime

def load_jsonl(filepath: Path) -> List[Dict]:
    """è¼‰å…¥ JSONL æ–‡ä»¶"""
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if line:
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  è­¦å‘Šï¼šç¬¬ {line_num} è¡Œè§£æå¤±æ•—: {e}")
    return data

def save_jsonl(data: List[Dict], filepath: Path):
    """å„²å­˜ç‚º JSONL æ–‡ä»¶"""
    with open(filepath, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def get_item_key(item: Dict) -> str:
    """
    ç”Ÿæˆé …ç›®çš„å”¯ä¸€éµå€¼
    
    ä½¿ç”¨ text å’Œ segments çš„çµ„åˆä½œç‚ºå”¯ä¸€è­˜åˆ¥
    å› ç‚ºç›¸åŒæ–‡å­—å¯èƒ½æœ‰ä¸åŒçš„æ¨™è¨»ï¼ˆé€™ç¨®æƒ…æ³å¾ˆå°‘ä½†å¯èƒ½å­˜åœ¨ï¼‰
    """
    text = item.get('text', '')
    segments = item.get('segments', [])
    
    # è™•ç† segments å¯èƒ½ä¸æ˜¯åˆ—è¡¨çš„æƒ…æ³
    if not isinstance(segments, list):
        segments = []
    
    # å»ºç«‹ segments çš„è¦ç¯„åŒ–è¡¨ç¤º
    segments_repr = tuple(
        (seg.get('text', ''), seg.get('label', ''), seg.get('start', 0), seg.get('end', 0))
        for seg in segments
        if isinstance(seg, dict)  # ç¢ºä¿ seg æ˜¯å­—å…¸
    )
    
    return f"{text}|||{segments_repr}"

def compare_datasets(old_data: List[Dict], new_data: List[Dict]) -> Tuple[Set[str], Set[str], Set[str]]:
    """
    æ¯”å°å…©å€‹è³‡æ–™é›†
    
    Returns:
        (å…±åŒé …, åƒ…åœ¨èˆŠè³‡æ–™, åƒ…åœ¨æ–°è³‡æ–™)
    """
    old_keys = {get_item_key(item): item for item in old_data}
    new_keys = {get_item_key(item): item for item in new_data}
    
    old_key_set = set(old_keys.keys())
    new_key_set = set(new_keys.keys())
    
    common = old_key_set & new_key_set
    only_old = old_key_set - new_key_set
    only_new = new_key_set - old_key_set
    
    return common, only_old, only_new

def print_statistics(old_data: List[Dict], new_data: List[Dict], 
                    common: Set[str], only_old: Set[str], only_new: Set[str]):
    """åˆ—å°çµ±è¨ˆè³‡è¨Š"""
    print("\n" + "="*60)
    print("ğŸ“Š è¨“ç·´è³‡æ–™æ¯”å°çµ±è¨ˆ")
    print("="*60)
    print(f"\nèˆŠè³‡æ–™é›† (nlp_training_data.jsonl):")
    print(f"  ç¸½é …ç›®æ•¸: {len(old_data)}")
    print(f"  å”¯ä¸€é …ç›®: {len(old_data)}")
    
    print(f"\næ–°è³‡æ–™é›† (nlp_training_data2.jsonl):")
    print(f"  ç¸½é …ç›®æ•¸: {len(new_data)}")
    print(f"  å”¯ä¸€é …ç›®: {len(new_data)}")
    
    print(f"\næ¯”å°çµæœ:")
    print(f"  å…±åŒé …ç›®ï¼ˆé‡è¤‡ï¼‰: {len(common)} ({len(common)/max(len(old_data), len(new_data))*100:.1f}%)")
    print(f"  åƒ…åœ¨èˆŠè³‡æ–™: {len(only_old)}")
    print(f"  åƒ…åœ¨æ–°è³‡æ–™: {len(only_new)}")
    
    print(f"\nåˆä½µå¾Œé æœŸï¼ˆè¯é›†ï¼‰:")
    total_union = len(only_old) + len(only_new) + len(common)
    print(f"  ç¸½é …ç›®æ•¸: {total_union}")
    print(f"  = åƒ…èˆŠè³‡æ–™ ({len(only_old)}) + åƒ…æ–°è³‡æ–™ ({len(only_new)}) + å…±åŒé … ({len(common)})")
    print(f"  å°‡ç§»é™¤é‡è¤‡: {len(common)} é …")
    print("="*60 + "\n")

def sample_items(items: List[Dict], keys: Set[str], num_samples: int = 3) -> List[Dict]:
    """æŠ½æ¨£é¡¯ç¤ºé …ç›®"""
    key_to_item = {get_item_key(item): item for item in items}
    sampled_keys = list(keys)[:num_samples]
    return [key_to_item[key] for key in sampled_keys if key in key_to_item]

def print_samples(title: str, items: List[Dict]):
    """åˆ—å°æ¨£æœ¬"""
    print(f"\n{title}:")
    for i, item in enumerate(items, 1):
        text = item.get('text', '')[:50]
        segments_raw = item.get('segments', [])
        # è™•ç† segments å¯èƒ½ä¸æ˜¯åˆ—è¡¨æˆ–åŒ…å«éå­—å…¸å…ƒç´ çš„æƒ…æ³
        if isinstance(segments_raw, list):
            segments = [seg.get('label', '') for seg in segments_raw if isinstance(seg, dict)]
        else:
            segments = []
        print(f"  {i}. \"{text}...\" -> {segments}")

def main():
    """ä¸»å‡½æ•¸"""
    # è¨­å®šè·¯å¾‘
    script_dir = Path(__file__).parent
    old_file = script_dir / "nlp_training_data.jsonl"
    new_file = script_dir / "nlp_training_data_additional.jsonl"
    
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not old_file.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°èˆŠè³‡æ–™é›†: {old_file}")
        sys.exit(1)
    
    if not new_file.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ–°è³‡æ–™é›†: {new_file}")
        sys.exit(1)
    
    print("ğŸ” æ­£åœ¨è¼‰å…¥è¨“ç·´è³‡æ–™...")
    
    # è¼‰å…¥è³‡æ–™
    old_data = load_jsonl(old_file)
    new_data = load_jsonl(new_file)
    
    print(f"âœ… å·²è¼‰å…¥èˆŠè³‡æ–™: {len(old_data)} é …")
    print(f"âœ… å·²è¼‰å…¥æ–°è³‡æ–™: {len(new_data)} é …")
    
    # æ¯”å°è³‡æ–™
    print("\nğŸ”„ æ­£åœ¨æ¯”å°è³‡æ–™...")
    common, only_old, only_new = compare_datasets(old_data, new_data)
    
    # åˆ—å°çµ±è¨ˆ
    print_statistics(old_data, new_data, common, only_old, only_new)
    
    # é¡¯ç¤ºæ¨£æœ¬
    if common:
        samples = sample_items(old_data, common, 3)
        print_samples("ğŸ“ é‡è¤‡é …ç›®æ¨£æœ¬ (å°‡è¢«ç§»é™¤)", samples)
    
    if only_new:
        samples = sample_items(new_data, only_new, 3)
        print_samples("âœ¨ æ–°å¢é …ç›®æ¨£æœ¬", samples)
    
    # è©¢å•æ˜¯å¦åˆä½µ
    print("\n" + "="*60)
    response = input("æ˜¯å¦è¦åˆä½µè³‡æ–™ï¼Ÿ(y/n): ").strip().lower()
    
    if response != 'y':
        print("âŒ å·²å–æ¶ˆåˆä½µ")
        return
    
    # å»ºç«‹è¯é›†ï¼ˆä¿ç•™æ‰€æœ‰å”¯ä¸€é …ç›®ï¼Œé‡è¤‡çš„åªä¿ç•™ä¸€å€‹ï¼‰
    print("\nğŸ”¨ æ­£åœ¨å»ºç«‹åˆä½µè³‡æ–™é›†ï¼ˆè¯é›†ï¼‰...")
    
    old_key_to_item = {get_item_key(item): item for item in old_data}
    new_key_to_item = {get_item_key(item): item for item in new_data}
    
    # è¯é›†ï¼šæ‰€æœ‰å”¯ä¸€é …ç›®ï¼ˆé‡è¤‡çš„å¾æ–°è³‡æ–™å–ï¼‰
    merged_dict = {}
    
    # å…ˆæ·»åŠ æ‰€æœ‰èˆŠè³‡æ–™
    for key in old_key_to_item:
        merged_dict[key] = old_key_to_item[key]
    
    # å†æ·»åŠ æ–°è³‡æ–™ï¼ˆæœƒè¦†è“‹é‡è¤‡çš„ï¼Œä½¿ç”¨æ–°ç‰ˆæœ¬ï¼‰
    for key in new_key_to_item:
        merged_dict[key] = new_key_to_item[key]
    
    merged_items = list(merged_dict.values())
    
    print(f"âœ… åˆä½µå®Œæˆï¼Œå…± {len(merged_items)} é … (ç§»é™¤ {len(common)} å€‹é‡è¤‡é …)")
    
    # å„²å­˜åˆä½µçµæœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. å‚™ä»½åŸå§‹èˆŠè³‡æ–™
    backup_file = script_dir / f"nlp_training_data.jsonl.backup_{timestamp}"
    print(f"\nğŸ’¾ æ­£åœ¨å‚™ä»½åŸå§‹è³‡æ–™åˆ°: {backup_file.name}")
    old_file.rename(backup_file)
    
    # 2. å„²å­˜åˆä½µå¾Œçš„è³‡æ–™ç‚ºæ–°çš„ nlp_training_data.jsonl
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜åˆä½µè³‡æ–™åˆ°: {old_file.name}")
    save_jsonl(merged_items, old_file)
    
    # 3. ç”¢ç”Ÿåˆä½µå¾Œæª”æ¡ˆå‰¯æœ¬ï¼ˆåƒ…ä¾›åƒè€ƒï¼‰
    merged_file = script_dir / f"nlp_training_data_merged_{timestamp}.jsonl"
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜åˆä½µè³‡æ–™å‰¯æœ¬åˆ°: {merged_file.name}")
    save_jsonl(merged_items, merged_file)
    
    # 4. ç”¢ç”Ÿåƒ…æ–°å¢é …ç›®æª”æ¡ˆï¼ˆåƒ…ä¾›åƒè€ƒï¼‰
    new_only_file = None
    if only_new:
        new_only_file = script_dir / f"nlp_training_data_new_only_{timestamp}.jsonl"
        new_only_items = [new_key_to_item[key] for key in only_new]
        print(f"ğŸ’¾ æ­£åœ¨å„²å­˜æ–°å¢é …ç›®åˆ°: {new_only_file.name}")
        save_jsonl(new_only_items, new_only_file)
    
    print("\n" + "="*60)
    print("âœ… åˆä½µå®Œæˆï¼")
    print("="*60)
    print(f"\nç”¢ç”Ÿçš„æª”æ¡ˆ:")
    print(f"  1. {old_file.name} - åˆä½µå¾Œçš„è¨“ç·´è³‡æ–™ ({len(merged_items)} é …)")
    print(f"  2. {backup_file.name} - åŸå§‹è³‡æ–™å‚™ä»½ ({len(old_data)} é …)")
    print(f"  3. {merged_file.name} - åˆä½µè³‡æ–™å‰¯æœ¬ ({len(merged_items)} é …)")
    if new_only_file:
        print(f"  4. {new_only_file.name} - åƒ…æ–°å¢é …ç›® ({len(only_new)} é …)")
    
    print(f"\nçµ±è¨ˆæ‘˜è¦:")
    print(f"  åŸå§‹èˆŠè³‡æ–™: {len(old_data)}")
    print(f"  åŸå§‹æ–°è³‡æ–™: {len(new_data)}")
    print(f"  ç§»é™¤é‡è¤‡é …: {len(common)}")
    print(f"  æœ€çµ‚ç¸½æ•¸: {len(merged_items)} = {len(old_data)} + {len(new_data)} - {len(common)}")
    
    print("\nğŸ’¡ æç¤ºï¼š")
    print("  - åŸå§‹èˆŠè³‡æ–™å·²å‚™ä»½")
    print("  - nlp_training_data.jsonl ç¾åœ¨åŒ…å«åˆä½µå¾Œçš„è³‡æ–™")
    print("  - å¯ä»¥ä½¿ç”¨æ–°è³‡æ–™è¨“ç·´æ¨¡å‹æ¸¬è©¦æº–ç¢ºåº¦")
    print("  - å¦‚éœ€é‚„åŸï¼Œé‡æ–°å‘½åå‚™ä»½æª”æ¡ˆå³å¯")
    print("="*60 + "\n")

if __name__ == "__main__":
    main()
