# modules/llm_module/prompt_manager.py
"""
Prompt Manager - Integrated prompt building functionality

Responsible for dynamically building prompts based on different states and modes:
- CHAT mode: Personalized conversation prompts
- WORK mode: Workflow guidance prompts  
- System values integration
- Identity information integration
- Memory context integration
- Module interface integration (MEM/SYS)
"""

import os
import time
from typing import Dict, Any, Optional, List, Callable
from pathlib import Path
from utils.debug_helper import debug_log, info_log, error_log
from core.status_manager import status_manager
from .module_interfaces import state_aware_interface


class PromptManager:
    """æç¤ºè©ç®¡ç†å™¨ - ä½¿ç”¨éœæ…‹é…ç½®å’Œå‹•æ…‹æ¨¡çµ„è³‡æ–™æ§‹å»ºæç¤ºè©"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # Load static system instructions from config
        self.system_instructions = config.get("system_instructions", {})
        
        # Cache for built prompts
        self._template_cache = {}
        
        debug_log(2, "[PromptManager] æç¤ºè©ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ˆæ”¯æ´éœæ…‹é…ç½®èˆ‡å‹•æ…‹æ¨¡çµ„è³‡æ–™ï¼‰")
    
    def build_chat_prompt(self, user_input: str, identity_context: Optional[Dict] = None,
                         memory_context: Optional[str] = None, 
                         conversation_history: Optional[List] = None,
                         is_internal: bool = False,
                         relevant_memories: Optional[List[Dict]] = None) -> str:
        """æ§‹å»ºå°è©±æ¨¡å¼æç¤ºè© - æ•´åˆéœæ…‹é…ç½®èˆ‡å‹•æ…‹æ¨¡çµ„è³‡æ–™"""
        
        prompt_parts = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ {system_values} ä½”ä½ç¬¦ï¼ˆåœ¨æ›¿æ›å‰æª¢æŸ¥ï¼‰
        base_has_placeholder = "{system_values}" in self.system_instructions.get("base_personality", "")
        chat_has_placeholder = "{system_values}" in self.system_instructions.get("chat_mode", "")
        
        # åŸºç¤äººæ ¼ï¼ˆç¸½æ˜¯åŒ…å«ï¼Œé™¤éæ˜¯å…§éƒ¨èª¿ç”¨ï¼‰
        if not is_internal and "base_personality" in self.system_instructions:
            base_instruction = self.system_instructions["base_personality"]
            base_instruction = self._replace_system_values_placeholder(base_instruction)
            prompt_parts.append(base_instruction)
        
        # å°è©±æ¨¡å¼ç‰¹å®šæŒ‡ä»¤ï¼ˆéå…§éƒ¨èª¿ç”¨æ‰æ·»åŠ ï¼‰
        if not is_internal and "chat_mode" in self.system_instructions:
            chat_instruction = self.system_instructions["chat_mode"]
            chat_instruction = self._replace_system_values_placeholder(chat_instruction)
            prompt_parts.append(chat_instruction)
        
        # ç³»çµ±ç‹€æ…‹è³‡è¨Šï¼ˆå¦‚æœæŒ‡ä»¤ä¸­æ²’æœ‰ {system_values} ä½”ä½ç¬¦æ‰æ·»åŠ ï¼‰
        if not is_internal and not (base_has_placeholder or chat_has_placeholder):
            status_info = self._build_status_context_with_guide()
            if status_info:
                prompt_parts.append(status_info)
        
        # èº«ä»½è³‡è¨Š
        identity_info = self._build_identity_context(identity_context)
        if identity_info:
            prompt_parts.append(identity_info)
        
        # è¨˜æ†¶ä¸Šä¸‹æ–‡ - å¾ MEM æ¨¡çµ„ç²å–æˆ–ä½¿ç”¨å‚³å…¥çš„å…§å®¹ï¼Œä¸¦æ•´åˆæª¢ç´¢åˆ°çš„è¨˜æ†¶
        memory_section = self._build_memory_context(memory_context, relevant_memories)
        if memory_section:
            prompt_parts.append(memory_section)
        
        # å°è©±æ­·å²
        history_section = self._build_conversation_history(conversation_history)
        if history_section:
            prompt_parts.append(history_section)
        
        # ç”¨æˆ¶è¼¸å…¥
        user_section = f"User: {user_input}"
        prompt_parts.append(user_section)
        
        # å›æ‡‰å¼•å°
        if not is_internal:
            prompt_parts.append("Please respond as U.E.P, considering current mood and status.")
        
        return "\n\n".join(prompt_parts)
    
    def build_work_prompt(self, user_input: str, available_functions: Optional[str] = None,
                         workflow_context: Optional[Dict] = None,
                         identity_context: Optional[Dict] = None,
                         workflow_hint: Optional[str] = None,
                         use_mcp_tools: bool = False,
                         suppress_start_workflow_instruction: bool = False) -> str:
        """æ§‹å»ºå·¥ä½œæ¨¡å¼æç¤ºè© - æ•´åˆç³»çµ±åŠŸèƒ½èˆ‡å·¥ä½œæµä¸Šä¸‹æ–‡
        
        Args:
            suppress_start_workflow_instruction: ç•¶å·²æœ‰å·¥ä½œæµé‹è¡Œæ™‚ï¼ŒæŠ‘åˆ¶ã€Œç«‹å³å•Ÿå‹•å·¥ä½œæµã€çš„å¼·åˆ¶æŒ‡ç¤º
        """
        
        prompt_parts = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ {system_values} ä½”ä½ç¬¦ï¼ˆåœ¨æ›¿æ›å‰æª¢æŸ¥ï¼‰
        base_has_placeholder = "{system_values}" in self.system_instructions.get("base_personality", "")
        work_has_placeholder = "{system_values}" in self.system_instructions.get("work_mode", "")
        
        # åŸºç¤äººæ ¼ï¼ˆç¸½æ˜¯åŒ…å«ï¼‰
        if "base_personality" in self.system_instructions:
            base_instruction = self.system_instructions["base_personality"]
            base_instruction = self._replace_system_values_placeholder(base_instruction)
            prompt_parts.append(base_instruction)
        
        # å·¥ä½œæ¨¡å¼ç‰¹å®šæŒ‡ä»¤
        if "work_mode" in self.system_instructions:
            work_instruction = self.system_instructions["work_mode"]
            work_instruction = self._replace_system_values_placeholder(work_instruction)
            prompt_parts.append(work_instruction)
        
        # ç³»çµ±ç‹€æ…‹ï¼ˆå¦‚æœæŒ‡ä»¤ä¸­æ²’æœ‰ {system_values} ä½”ä½ç¬¦æ‰æ·»åŠ ï¼‰
        if not (base_has_placeholder or work_has_placeholder):
            status_info = self._build_status_context_with_guide(focus_on_work=True)
            if status_info:
                prompt_parts.append(status_info)
        
        # èº«ä»½è³‡è¨Šï¼ˆç°¡åŒ–ç‰ˆï¼‰
        identity_info = self._build_identity_context(identity_context, simplified=True)
        if identity_info:
            prompt_parts.append(identity_info)
        
        # å¯ç”¨ç³»çµ±åŠŸèƒ½ - åªåœ¨ä¸ä½¿ç”¨ MCP tools æ™‚æ‰æ·»åŠ æ–‡å­—æè¿°
        if not use_mcp_tools:
            functions_section = self._build_functions_context(available_functions)
            if functions_section:
                prompt_parts.append(functions_section)
        
        # å·¥ä½œæµä¸Šä¸‹æ–‡
        if workflow_context:
            workflow_section = self._build_workflow_context(workflow_context)
            if workflow_section:
                prompt_parts.append(workflow_section)
        
        # ç”¨æˆ¶è«‹æ±‚
        request_section = f"User Request: {user_input}"
        prompt_parts.append(request_section)
        
        # âœ… å·¥ä½œæ¨¡å¼æŒ‡å¼•ï¼ˆæ ¹æ“šæ˜¯å¦ä½¿ç”¨ MCP tools è€Œä¸åŒï¼‰
        if use_mcp_tools:
            work_guidance = (
                "Available Workflows:\n"
                "- drop_and_read: Read file content (for requests like 'read file', 'show content', 'open file')\n"
                "- intelligent_archive: Smart file organization (for 'organize', 'archive', 'sort files')\n"
                "- summarize_tag: Summarize and tag files (for 'summarize', 'tag', 'analyze files')\n"
                "- translate_document: Translate documents to different languages\n"
                "- code_analysis: Analyze code files for issues and improvements\n\n"
            )
            
            # âœ… NLP å·¥ä½œæµæç¤ºï¼ˆå¦‚æœæœ‰ï¼‰- æ”¾åœ¨å¯ç”¨å·¥ä½œæµä¹‹å¾Œ
            if workflow_hint:
                if isinstance(workflow_hint, dict):
                    workflow_name = workflow_hint.get('workflow_name', 'unknown')
                    confidence = workflow_hint.get('confidence', 0)
                    work_guidance += (
                        f"**NLP Analysis Result:**\n"
                        f"The system has analyzed the user's request and identified a matching workflow:\n"
                        f"- Recommended workflow: '{workflow_name}'\n"
                        f"- Match confidence: {confidence:.2f}\n"
                        f"- YOU MUST use this workflow name as the 'workflow_type' parameter\n\n"
                    )
                elif isinstance(workflow_hint, str):
                    work_guidance += (
                        f"**NLP Analysis Result:**\n"
                        f"- Recommended workflow: '{workflow_hint}'\n"
                        f"- YOU MUST use this workflow name as the 'workflow_type' parameter\n\n"
                    )
                else:
                    work_guidance += f"**NLP suggests:** {workflow_hint}\n\n"
            
            # âœ… åªåœ¨æ²’æœ‰æ´»èºå·¥ä½œæµæ™‚æ‰æ·»åŠ ã€Œç«‹å³å•Ÿå‹•ã€æŒ‡ç¤º
            if not suppress_start_workflow_instruction:
                work_guidance += (
                    "**CRITICAL INSTRUCTIONS - DO NOT IGNORE:**\n"
                    "1. YOU MUST IMMEDIATELY call the appropriate workflow tool directly. DO NOT ask for clarification.\n"
                    "2. Use the SPECIFIC workflow tool (e.g., intelligent_archive, drop_and_read, get_weather) NOT the generic 'start_workflow'\n"
                    "3. Each workflow tool has detailed parameter extraction guidance - read it carefully\n"
                    "4. Extract parameters from the user's input as guided by the tool description\n"
                    "5. Required parameters:\n"
                    "   - command: Copy the user's original request exactly as provided\n"
                    "   - initial_data: JSON string with extracted parameters (or empty \"{}\" if none can be extracted)\n"
                    "6. DO NOT respond with plain text asking for more information\n"
                    "7. DO NOT say you need information - the workflow will collect missing information interactively AFTER it starts\n\n"
                    "**REPEAT: You MUST call the specific workflow tool immediately. Do not ask questions first.**\n\n"
                    "Example:\n"
                    "User: 'Archive this file to D drive'\n"
                    "YOU MUST: call intelligent_archive(command='Archive this file to D drive', initial_data='{\"target_dir_input\": \"D:\\\\\"}')"
                )
            else:
                # å·²æœ‰å·¥ä½œæµé‹è¡Œï¼Œæª¢æŸ¥æ˜¯å¦ç‚ºæ­¥é©Ÿå›æ‡‰ä¸Šä¸‹æ–‡
                # âœ… ä¿®å¾©ï¼šç¢ºä¿ workflow_context ä¸ç‚º None
                is_step_response = (workflow_context is not None and 
                                   workflow_context.get('type') == 'workflow_step_response')
                
                if is_step_response:
                    # âœ… æ­¥é©Ÿå·²å®Œæˆï¼ŒLLM æ‡‰è©²ç”Ÿæˆå›æ‡‰ï¼Œä¸è¦å‘¼å«å·¥å…·
                    work_guidance += (
                        "\n**Instructions:**\n"
                        "A workflow step has completed. The step data is provided in the context above.\n"
                        "Your task:\n"
                        "1. Read the workflow data from the context\n"
                        "2. Generate a natural, friendly response in ENGLISH explaining the result to the user\n"
                        "3. DO NOT call any MCP tools (review_step, approve_step, etc.) - just provide a text response\n"
                        "4. The workflow context already contains all the data you need\n"
                    )
                else:
                    # å·¥ä½œæµæ­£åœ¨é€²è¡Œä¸­ï¼Œç­‰å¾…ç”¨æˆ¶è¼¸å…¥æˆ–ç³»çµ±æ“ä½œ
                    work_guidance += (
                        "\n**Instructions:**\n"
                        "The workflow is currently running. Based on the situation:\n"
                        "- If you need to check workflow status: use get_workflow_status\n"
                        "- If the workflow is waiting for user input: provide guidance on what's needed\n"
                        "- DO NOT call start_workflow again - a workflow is already active\n"
                    )
        else:
            work_guidance = (
                "Instructions:\n"
                "1. Analyze the user's request against the Available System Functions above\n"
                "2. If the request matches a system function:\n"
                "   - Set sys_action.action to the appropriate action type (start_workflow, execute_function, or provide_options)\n"
                "   - Set sys_action.target to the exact function name from the list\n"
                "   - Provide sys_action.reason explaining why this function matches\n"
                "   - Include any required parameters in sys_action.parameters\n"
                "3. If the request is unclear or needs more information:\n"
                "   - Ask for specific clarification in your text response\n"
                "   - Set sys_action.action to 'provide_options' and sys_action.target to 'clarification'\n"
                "4. Always provide a helpful text response to the user"
            )
        prompt_parts.append(work_guidance)
        
        return "\n\n".join(prompt_parts)
    
    def build_direct_prompt(self, user_input: str) -> str:
        """æ§‹å»ºç›´æ¥æ¨¡å¼æç¤ºè©ï¼ˆæœ€å°åŒ–ï¼‰"""
        return user_input
    
    def _build_status_context_with_guide(self, focus_on_work: bool = False) -> Optional[str]:
        """æ§‹å»ºç³»çµ±ç‹€æ…‹ä¸Šä¸‹æ–‡ - åŒ…å«ç³»çµ±å€¼èªªæ˜ï¼ˆè‹±æ–‡ç‰ˆï¼‰"""
        try:
            modifiers = status_manager.get_personality_modifiers()
            
            # Convert status to English
            status_text = self._format_system_values_english(modifiers)
            
            parts = []
            
            if focus_on_work:
                # Work mode focuses on efficiency-related states
                parts.append(f"Current Status: {status_text}")
            else:
                # Chat mode includes full status information
                parts.append(f"Current Status: {status_text}")
            
            # Add system values explanations from config
            system_values_guide = self.config.get("system_values_guide", {})
            if system_values_guide:
                explanations = []
                for value_name, explanation_text in system_values_guide.items():
                    if explanation_text and isinstance(explanation_text, str):
                        # Clean up the explanation text
                        clean_explanation = explanation_text.strip()
                        if clean_explanation:
                            explanations.append(f"{value_name.upper()}:\n{clean_explanation}")
                
                if explanations:
                    parts.append("System Values Guide:\n" + "\n\n".join(explanations))
            
            return "\n\n".join(parts)
                
        except Exception as e:
            error_log(f"[PromptManager] æ§‹å»ºç‹€æ…‹ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return None
    
    def _format_system_values_english(self, modifiers: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç³»çµ±å€¼ç‚ºè‹±æ–‡æ ¼å¼ï¼Œä¸¦æä¾›æ•¸å€¼ä»¥ä¾¿å‹•æ…‹èª¿æ•´èªæ°£"""
        status_mapping = {
            "éå¸¸ç©æ¥µ": "very positive", "ç©æ¥µ": "positive", "ä¸­æ€§": "neutral",
            "æ¶ˆæ¥µ": "negative", "éå¸¸æ¶ˆæ¥µ": "very negative",
            "éå¸¸è‡ªä¿¡": "very confident", "è‡ªä¿¡": "confident", "æ™®é€š": "normal", 
            "æ²’æœ‰è‡ªä¿¡": "not confident", "éå¸¸æ²’æœ‰è‡ªä¿¡": "very unconfident",
            "éå¸¸é¡˜æ„å¹«åŠ©": "very helpful", "é¡˜æ„å¹«åŠ©": "helpful", "ä¸­ç­‰": "moderate",
            "ä¸å¤ªé¡˜æ„å¹«åŠ©": "less helpful", "ä¸é¡˜æ„å¹«åŠ©": "unhelpful",
            "éå¸¸ç„¡èŠ": "very bored", "ç„¡èŠ": "bored", "æœ‰é»ç„¡èŠ": "slightly bored",
            "ä¸ç„¡èŠ": "not bored", "æ„Ÿèˆˆè¶£": "interested"
        }
        
        mood_en = status_mapping.get(modifiers['mood_level'], modifiers['mood_level'])
        pride_en = status_mapping.get(modifiers['pride_level'], modifiers['pride_level'])
        helpfulness_en = status_mapping.get(modifiers['helpfulness_level'], modifiers['helpfulness_level'])
        boredom_en = status_mapping.get(modifiers['boredom_level'], modifiers['boredom_level'])
        
        # ç²å–åŸå§‹æ•¸å€¼ï¼ˆç”¨æ–¼å‹•æ…‹èª¿æ•´èªæ°£ï¼‰
        helpfulness_value = modifiers.get('helpfulness', 0.5)  # é è¨­ä¸­ç­‰
        
        # åˆ¤æ–· helpfulness ç´šåˆ¥
        if helpfulness_value > 0.7:
            helpfulness_level = "HIGH"
        elif helpfulness_value >= 0.3:
            helpfulness_level = "MEDIUM"
        else:
            helpfulness_level = "LOW"
        
        return (f"mood={mood_en}, pride={pride_en}, "
                f"helpfulness={helpfulness_en} (level={helpfulness_level}, value={helpfulness_value:.2f}), "
                f"boredom={boredom_en}")
    
    def _build_memory_context(self, memory_context: Optional[str] = None, 
                             relevant_memories: Optional[List[Dict]] = None) -> Optional[str]:
        """æ§‹å»ºè¨˜æ†¶ä¸Šä¸‹æ–‡å€æ®µ - æ•´åˆæª¢ç´¢åˆ°çš„è¨˜æ†¶"""
        context_parts = []
        
        # åŸæœ‰çš„è¨˜æ†¶ä¸Šä¸‹æ–‡
        if memory_context:
            context_parts.append(f"Memory Context:\n{memory_context}")
        
        # æ–°çš„æª¢ç´¢è¨˜æ†¶
        if relevant_memories:
            memory_text_parts = ["Retrieved Relevant Memories:"]
            for i, memory in enumerate(relevant_memories, 1):
                memory_type = memory.get("type", "general")
                content = memory.get("content", "")
                
                if memory_type == "conversation":
                    user_input = memory.get("user_input", "")
                    assistant_response = memory.get("assistant_response", "")
                    memory_text_parts.append(f"{i}. [Conversation] User: {user_input} | Assistant: {assistant_response}")
                elif memory_type == "user_info":
                    memory_text_parts.append(f"{i}. [User Info] {content}")
                else:
                    memory_text_parts.append(f"{i}. [{memory_type}] {content}")
                    
            context_parts.append("\n".join(memory_text_parts))
        
        # å¦‚æœéƒ½æ²’æœ‰ï¼Œå˜—è©¦å¾ MEM æ¨¡çµ„ç²å– (å‘å¾Œå…¼å®¹)
        if not context_parts:
            try:
                mem_data = state_aware_interface.get_chat_mem_data("context")
                if mem_data and isinstance(mem_data, dict):
                    mem_memories = mem_data.get("relevant_memories", "")
                    if mem_memories:
                        context_parts.append(f"Relevant Memory:\n{mem_memories}")
            except Exception as e:
                debug_log(1, f"[PromptManager] ç„¡æ³•å¾ MEM æ¨¡çµ„ç²å–è¨˜æ†¶è³‡æ–™: {e}")
        
        return "\n\n".join(context_parts) if context_parts else None
    
    def _build_functions_context(self, available_functions: Optional[str] = None) -> Optional[str]:
        """æ§‹å»ºç³»çµ±åŠŸèƒ½ä¸Šä¸‹æ–‡ - å„ªå…ˆä½¿ç”¨å‚³å…¥å…§å®¹ï¼Œå¦å‰‡å¾ SYS æ¨¡çµ„ç²å–"""
        if available_functions:
            return f"Available System Functions:\n{available_functions}"
        
        # Try to get functions data from SYS module
        try:
            sys_data = state_aware_interface.get_work_sys_data("function_registry")
            if sys_data:
                if isinstance(sys_data, (list, tuple, set)):
                    return "Available System Functions:\n" + "\n".join(map(str, sys_data))
                if isinstance(sys_data, dict) and sys_data.get("available_functions"):
                    return "Available System Functions:\n" + "\n".join(sys_data["available_functions"])
            return None
        except Exception as e:
            debug_log(1, f"[PromptManager] ç„¡æ³•å¾ SYS æ¨¡çµ„ç²å–åŠŸèƒ½è³‡æ–™: {e}")
        
        return None
    
    def _build_identity_context(self, identity_context: Optional[Dict], 
                               simplified: bool = False) -> Optional[str]:
        """æ§‹å»ºèº«ä»½ä¸Šä¸‹æ–‡"""
        if not identity_context:
            return None
        
        try:
            identity = identity_context.get("identity", {})
            preferences = identity_context.get("preferences", {})
            
            if simplified:
                # Simplified version with basic info only
                name = identity.get("name", "User")
                return f"User: {name}"
            else:
                # Full version with preferences
                parts = []
                
                name = identity.get("name", "User")
                parts.append(f"User: {name}")
                
                # Conversation preferences
                conversation_prefs = preferences.get("conversation", {})
                if conversation_prefs:
                    formality = conversation_prefs.get("formality", "neutral")
                    verbosity = conversation_prefs.get("verbosity", "moderate")
                    parts.append(f"Preferences: {formality} formality, {verbosity} verbosity")
                
                return "; ".join(parts)
                
        except Exception as e:
            error_log(f"[PromptManager] æ§‹å»ºèº«ä»½ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return None
    
    def _build_conversation_history(self, history: Optional[List]) -> Optional[str]:
        """æ§‹å»ºå°è©±æ­·å²ä¸Šä¸‹æ–‡"""
        if not history or len(history) == 0:
            return None
        
        try:
            # Only take recent conversations
            recent_history = history[-3:] if len(history) > 3 else history
            
            history_parts = []
            for entry in recent_history:
                # è™•ç† ConversationEntry å°è±¡æˆ–å­—å…¸
                if hasattr(entry, 'role') and hasattr(entry, 'content'):
                    # é€™æ˜¯ ConversationEntry å°è±¡
                    role = entry.role
                    content = entry.content
                elif isinstance(entry, dict):
                    # é€™æ˜¯å­—å…¸æ ¼å¼
                    role = entry.get("role", "unknown")
                    content = entry.get("content", "")
                else:
                    # æœªçŸ¥æ ¼å¼ï¼Œè·³é
                    continue
                
                if role == "user":
                    history_parts.append(f"User: {content}")
                elif role == "assistant":
                    history_parts.append(f"U.E.P: {content}")
            
            if history_parts:
                return "Recent Conversation:\n" + "\n".join(history_parts)
                
        except Exception as e:
            error_log(f"[PromptManager] æ§‹å»ºå°è©±æ­·å²å¤±æ•—: {e}")
            
        return None
    
    def _build_workflow_context(self, workflow_context: Dict) -> Optional[str]:
        """æ§‹å»ºå·¥ä½œæµä¸Šä¸‹æ–‡"""
        try:
            # ğŸ†• æª¢æŸ¥æ˜¯å¦ç‚ºå·¥ä½œæµæ­¥é©Ÿå›æ‡‰é¡å‹
            context_type = workflow_context.get("type")
            
            if context_type == "workflow_step_response":
                # é€™æ˜¯å·¥ä½œæµæ­¥é©Ÿå®Œæˆï¼Œéœ€è¦ LLM ç”Ÿæˆç”¨æˆ¶å›æ‡‰
                return self._build_workflow_step_response_context(workflow_context)
            
            if context_type == "workflow_input_required":
                # é€™æ˜¯å·¥ä½œæµéœ€è¦ç”¨æˆ¶è¼¸å…¥ï¼Œéœ€è¦ LLM åˆ¤æ–·ä¸¦æä¾›è¼¸å…¥
                return self._build_workflow_input_required_context(workflow_context)
            
            if context_type == "workflow_error":
                # ğŸ†• é€™æ˜¯å·¥ä½œæµéŒ¯èª¤ï¼Œéœ€è¦ LLM ç”ŸæˆéŒ¯èª¤èªªæ˜ä¸¦å–æ¶ˆå·¥ä½œæµ
                return self._build_workflow_error_context(workflow_context)
            
            # åŸæœ‰é‚è¼¯ï¼šå·¥ä½œæµé€²åº¦è¿½è¹¤
            parts = []
            
            # Current step
            current_step = workflow_context.get("current_step")
            if current_step:
                parts.append(f"Current Step: {current_step}")
            
            # Previous result
            previous_result = workflow_context.get("previous_result")
            if previous_result:
                parts.append(f"Previous Result: {previous_result}")
            
            # Remaining steps
            remaining_steps = workflow_context.get("remaining_steps", [])
            if remaining_steps:
                steps_text = ", ".join(remaining_steps[:3])  # Show only first 3
                parts.append(f"Remaining Steps: {steps_text}")
            
            if parts:
                return "Workflow Progress: " + "; ".join(parts)
                
        except Exception as e:
            error_log(f"[PromptManager] æ§‹å»ºå·¥ä½œæµä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            
        return None
    
    def _build_workflow_step_response_context(self, workflow_context: Dict) -> str:
        """
        æ§‹å»ºå·¥ä½œæµæ­¥é©Ÿå›æ‡‰çš„ä¸Šä¸‹æ–‡ï¼ˆé€šç”¨æ¡†æ¶æ¨¡å¼ï¼‰
        
        é€™å€‹æ–¹æ³•ç”Ÿæˆé€šç”¨çš„æŒ‡å¼•ï¼Œè®“ LLM èƒ½å¤ è™•ç†ä»»ä½•é¡å‹çš„å·¥ä½œæµæ­¥é©Ÿæ•¸æ“š
        è€Œä¸æ˜¯é‡å°ç‰¹å®šå·¥ä½œæµç¡¬ç·¨ç¢¼é‚è¼¯
        
        Args:
            workflow_context: å·¥ä½œæµä¸Šä¸‹æ–‡æ•¸æ“š
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        workflow_type = workflow_context.get('workflow_type', 'unknown')
        is_complete = workflow_context.get('is_complete', False)
        should_end_session = workflow_context.get('should_end_session', False)
        review_data = workflow_context.get('review_data', {})
        step_result = workflow_context.get('step_result', {})
        
        context_parts = []
        
        # åŸºç¤è³‡è¨Š
        context_parts.append("=" * 60)
        context_parts.append("WORKFLOW STEP COMPLETED - GENERATE USER RESPONSE")
        context_parts.append("=" * 60)
        
        # ğŸ†• ç²å–ä¸‹ä¸€æ­¥è³‡è¨Š
        next_step_info = workflow_context.get('next_step_info')
        next_step_is_interactive = next_step_info and next_step_info.get('step_type') == 'interactive' if next_step_info else False
        
        # âš ï¸ ä¸é¡¯ç¤ºä»»ä½•æŠ€è¡“ç´°ç¯€çµ¦ LLM
        # ä½¿ç”¨è€…ä¸éœ€è¦çŸ¥é“ workflow_type, task_id, session_id, executed_steps ç­‰
        # LLM åªéœ€è¦çŸ¥é“ï¼šå®Œæˆäº† / é€²è¡Œä¸­ï¼Œä»¥åŠå¦‚ä½•å›æ‡‰
        
        context_parts.append(f"\nStatus: {'All done' if is_complete else 'Still working'}")
        
        # ğŸ”§ é€šç”¨æŒ‡å¼•ï¼ˆæ¡†æ¶æ¨¡å¼ï¼Œä¸é‡å°ç‰¹å®šå·¥ä½œæµï¼‰
        context_parts.append("\n" + "=" * 60)
        context_parts.append("YOUR TASK:")
        context_parts.append("=" * 60)
        
        if is_complete:
            context_parts.append("\nâœ… Done! Everything wrapped up.")
            
            # ğŸ”§ æª¢æ¸¬æ˜¯å¦ç‚ºè‡ªå‹•å®Œæˆçš„ç°¡å–®å·¥ä½œæµï¼ˆç›´æ¥æ¨¡å¼ï¼Œç«‹å³å®Œæˆï¼‰
            # åˆ¤æ–·ä¾æ“šï¼šexecuted_steps æ•¸é‡å°‘ï¼ˆâ‰¤2æ­¥ï¼‰ä¸”æ²’æœ‰å¾©é›œçš„å›å‚³è³‡æ–™
            executed_steps = review_data.get('executed_steps', [])
            is_simple_auto_workflow = (
                len(executed_steps) <= 2 and  # ç°¡å–®å·¥ä½œæµï¼Œæ­¥é©Ÿæ•¸å°‘
                not next_step_is_interactive  # æ²’æœ‰å¾ŒçºŒäº’å‹•
            )
            
            if is_simple_auto_workflow:
                # ç°¡å–®è‡ªå‹•å®Œæˆå·¥ä½œæµ - çµ¦ç°¡çŸ­ç¢ºèªå³å¯
                context_parts.append("\nKeep it super brief - just 1-3 casual words:")
                context_parts.append("'Done!', 'All set!', 'Got it!', 'Yep!', 'Finished!'")
                context_parts.append("\nDon't explain anything or mention technical stuff.")
            else:
                # è¤‡é›œå·¥ä½œæµ - éœ€è¦è©³ç´°èªªæ˜
                context_parts.append("\nLet the user know what happened in a casual, friendly way:")
                context_parts.append("- Mention the key info from above (keep it simple)")
                context_parts.append("- Stay conversational, like telling a friend")
                context_parts.append("- 2-3 sentences max")
                context_parts.append("- Skip the tech talk (no IDs, no formal terms)")
            
            context_parts.append("\nThen call approve_step() to wrap things up.")
            if should_end_session:
                context_parts.append("Also set session_control={'action': 'end_session'} in metadata.")
        
        elif next_step_is_interactive:
            context_parts.append("\nâ­ï¸ The next step requires USER INPUT.")
            if next_step_info:
                context_parts.append(f"\nNext Step Prompt: {next_step_info.get('prompt', '')}")
            
            context_parts.append("\nGenerate a natural response in ENGLISH that:")
            context_parts.append("1. BRIEFLY acknowledges the current step (1 sentence max)")
            context_parts.append("2. Asks the user for the needed input (use the prompt above as guide)")
            context_parts.append("3. Keep it conversational and friendly (2-3 sentences total)")
            context_parts.append("4. Act as a helpful assistant, NOT a system notification")
            context_parts.append("5. âŒ AVOID: Mentioning 'workflow', 'step', 'session' or other tech terms")
            
            context_parts.append("\nğŸ“‹ REQUIRED ACTION:")
            context_parts.append("   Call approve_step() MCP tool AFTER generating your response")
        
        else:
            # é€™å€‹åˆ†æ”¯ç†è«–ä¸Šä¸æ‡‰è©²è¢«åŸ·è¡Œåˆ°ï¼ˆå› ç‚ºå·²è¢« 3 æ™‚åˆ»éæ¿¾ï¼‰
            context_parts.append("\nâ³ Processing step completed, continuing automatically.")
            context_parts.append("\nğŸ“‹ REQUIRED ACTION:")
            context_parts.append("   Call approve_step() MCP tool silently (no text response needed)")
        
        context_parts.append("\n" + "=" * 60)
        context_parts.append("LANGUAGE & PERSONALITY REQUIREMENTS:")
        context_parts.append("=" * 60)
        context_parts.append("âš ï¸ CRITICAL:")
        context_parts.append("   1. Always respond in ENGLISH")
        context_parts.append("   2. You are U.E.P., a personal assistant with personality")
        context_parts.append("   3. Be natural, warm, and conversational")
        context_parts.append("   4. NEVER mention technical details like IDs, session names, etc.")
        context_parts.append("   5. Talk like a helpful friend, not a machine")
        context_parts.append("=" * 60)
        
        return "\n".join(context_parts)
    
    def _build_workflow_input_required_context(self, workflow_context: Dict) -> str:
        """
        æ§‹å»ºå·¥ä½œæµéœ€è¦è¼¸å…¥çš„ä¸Šä¸‹æ–‡
        
        æŒ‡ç¤º LLM ä½¿ç”¨ provide_workflow_input å·¥å…·è™•ç†ç”¨æˆ¶è¼¸å…¥,
        ä¸¦åˆ¤æ–·æ˜¯å¦ç‚ºå§”è¨—æ„åœ–(delegation intent)
        
        Args:
            workflow_context: å·¥ä½œæµä¸Šä¸‹æ–‡æ•¸æ“š
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        workflow_type = workflow_context.get('workflow_type', 'unknown')
        step_id = workflow_context.get('step_id', 'unknown')
        prompt = workflow_context.get('prompt', 'è«‹æä¾›è¼¸å…¥')
        is_optional = workflow_context.get('is_optional', False)
        fallback_value = workflow_context.get('fallback_value', '')
        
        context_parts = []
        
        # åŸºç¤è³‡è¨Š
        context_parts.append("=" * 60)
        context_parts.append("WORKFLOW INPUT REQUIRED - USE provide_workflow_input TOOL")
        context_parts.append("=" * 60)
        
        context_parts.append(f"\nWorkflow Type: {workflow_type}")
        context_parts.append(f"Step ID: {step_id}")
        context_parts.append(f"Step Type: Interactive Input Step")
        context_parts.append(f"Optional: {is_optional}")
        if is_optional and fallback_value:
            context_parts.append(f"Fallback Value: {fallback_value}")
        
        # æç¤ºä¿¡æ¯
        context_parts.append(f"\nPrompt for User: {prompt}")
        
        # ç”¨æˆ¶è¼¸å…¥
        user_input = workflow_context.get('user_input', '')
        context_parts.append(f"\nUser's Response: {user_input}")
        
        # æŒ‡å¼•èªªæ˜
        context_parts.append("\n" + "=" * 60)
        context_parts.append("INSTRUCTIONS")
        context_parts.append("=" * 60)
        
        context_parts.append(
            "\nYou MUST use the provide_workflow_input tool to handle this input."
            "\nDo NOT generate a text response directly."
        )
        
        context_parts.append(
            "\n\nYour task:"
            "\n1. Analyze the user's response semantically and understand their intent"
            "\n2. EXTRACT the key information from natural language:"
            "\n   - For paths: extract the essential description (e.g., 'd drive root', 'documents folder')"
            "\n   - The workflow will handle path resolution and validation internally"
            "\n   - Don't try to construct absolute paths yourself"
            "\n3. Determine if it's DELEGATION INTENT or EXPLICIT VALUE"
        )
        
        # Optional æ­¥é©Ÿçš„ç‰¹æ®Šèªªæ˜
        if is_optional:
            context_parts.append(
                "\n\nâš ï¸  This is an OPTIONAL step with fallback."
                "\n\nDELEGATION INTENT examples (use_fallback=True):"
                "\n  - 'ä½ æ±ºå®š' / 'you decide'"
                "\n  - 'å¹«æˆ‘é¸' / 'help me choose'"
                "\n  - 'éš¨ä¾¿' / 'whatever' / 'anything'"
                "\n  - 'ä¸çŸ¥é“' / 'don't know'"
                "\n  - 'é è¨­' / 'default'"
                "\n  - Empty or very vague responses"
                "\n\nEXPLICIT VALUE examples (use_fallback=False):"
                "\n  - Natural language: 'put it in my d drive root' â†’ extract 'd drive root'"
                "\n  - Natural language: 'save to documents folder' â†’ extract 'documents folder'"
                "\n  - Specific path: 'C:\\\\temp' â†’ pass 'C:\\\\temp' as-is"
                "\n  - Any clear indication of what the user wants"
            )
        else:
            context_parts.append(
                "\n\nâš ï¸  This is a REQUIRED step (not optional)."
                "\n\nYour job is to EXTRACT the key information from user's natural language:"
                "\n  - 'Can you put the file in my d drive root?' â†’ extract 'd drive root'"
                "\n  - 'Save it to the documents folder' â†’ extract 'documents folder'"
                "\n  - 'Use C:\\\\temp' â†’ pass 'C:\\\\temp' as-is"
                "\n\nThe workflow will handle path resolution and validation internally."
                "\n\nSet use_fallback=False for explicit values."
                "\nSet use_fallback=True ONLY if user explicitly delegates the decision to you."
            )
        
        context_parts.append(
            "\n\nHow to respond:"
            "\n  1. Extract the key information from user's response"
            "\n  2. Call provide_workflow_input("
            "\n       session_id: <auto-injected>,"
            "\n       user_input: <extracted key info>,"
            "\n       use_fallback: <True if delegation, False otherwise>"
            "\n     )"
            "\n\nIMPORTANT:"
            "\n- user_input should be the EXTRACTED key information (e.g., 'd drive root'), NOT the full sentence"
            "\n- Let the workflow handle path resolution, validation, and processing"
            "\n- Do NOT try to resolve paths yourself or call other tools first"
        )
        
        context_parts.append("\n" + "=" * 60)
        
        return "\n".join(context_parts)
    
    def _build_workflow_error_context(self, workflow_context: Dict) -> str:
        """
        æ§‹å»ºå·¥ä½œæµéŒ¯èª¤çš„ä¸Šä¸‹æ–‡
        
        ç•¶å·¥ä½œæµåŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤æ™‚ï¼ŒæŒ‡ç¤º LLMï¼š
        1. ç”Ÿæˆè‡ªç„¶èªè¨€çš„éŒ¯èª¤èªªæ˜çµ¦ä½¿ç”¨è€…
        2. èª¿ç”¨ cancel_workflow MCP å·¥å…·å„ªé›…çµ‚æ­¢å·¥ä½œæµ
        
        Args:
            workflow_context: å·¥ä½œæµéŒ¯èª¤ä¸Šä¸‹æ–‡æ•¸æ“š
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        session_id = workflow_context.get('workflow_session_id', 'unknown')
        error_message = workflow_context.get('error_message', 'æœªçŸ¥éŒ¯èª¤')
        current_step = workflow_context.get('current_step', 'æœªçŸ¥æ­¥é©Ÿ')
        
        context_parts = []
        
        # åŸºç¤è³‡è¨Š
        context_parts.append("=" * 60)
        context_parts.append("WORKFLOW ERROR - INFORM USER AND CANCEL")
        context_parts.append("=" * 60)
        
        context_parts.append(f"\nâš ï¸ A workflow has encountered an error and cannot continue.")
        context_parts.append(f"\nError Details:")
        context_parts.append(f"  - Current Step: {current_step}")
        context_parts.append(f"  - Error Message: {error_message}")
        
        # æŒ‡å¼•èªªæ˜
        context_parts.append("\n" + "=" * 60)
        context_parts.append("YOUR TASK:")
        context_parts.append("=" * 60)
        
        context_parts.append("\nYou MUST do the following:")
        context_parts.append("1. Generate a natural, friendly error explanation in ENGLISH")
        context_parts.append("   - Explain what went wrong in simple terms")
        context_parts.append("   - Apologize for the inconvenience")
        context_parts.append("   - Suggest what the user might try instead (if applicable)")
        context_parts.append("   - Keep it conversational (2-3 sentences)")
        context_parts.append("")
        context_parts.append("2. Call the cancel_workflow MCP tool to gracefully terminate:")
        context_parts.append(f"   - session_id: {session_id}")
        context_parts.append(f"   - reason: Brief technical reason (for logs)")
        
        context_parts.append("\nâš ï¸ IMPORTANT:")
        context_parts.append("   - Do NOT mention technical terms like 'session_id', 'workflow', 'step'")
        context_parts.append("   - Be empathetic and helpful, like a personal assistant")
        context_parts.append("   - The cancel_workflow call will handle cleanup automatically")
        
        context_parts.append("\n" + "=" * 60)
        context_parts.append("LANGUAGE REQUIREMENT:")
        context_parts.append("=" * 60)
        context_parts.append("âš ï¸ CRITICAL: Always respond in ENGLISH")
        context_parts.append("=" * 60)
        
        return "\n".join(context_parts)
    
    def _replace_system_values_placeholder(self, instruction: str) -> str:
        """æ›¿æ›ç³»çµ±æŒ‡ä»¤ä¸­çš„ {system_values} ä½”ä½ç¬¦"""
        if "{system_values}" not in instruction:
            return instruction
        
        try:
            # Use the same status context building logic
            status_context = self._build_status_context_with_guide()
            system_values_text = status_context if status_context else ""
            
            return instruction.replace("{system_values}", system_values_text)
            
        except Exception as e:
            error_log(f"[PromptManager] æ›¿æ›ç³»çµ±å€¼ä½”ä½ç¬¦å¤±æ•—: {e}")
            return instruction.replace("{system_values}", "")
    
    def get_system_instruction(self, mode: str = "main") -> str:
        """ç²å–ç³»çµ±æŒ‡ä»¤"""
        return self.system_instructions.get(mode, self.system_instructions.get("main", ""))
    
    def update_system_instruction(self, mode: str, instruction: str):
        """æ›´æ–°ç³»çµ±æŒ‡ä»¤"""
        self.system_instructions[mode] = instruction
        debug_log(2, f"[PromptManager] æ›´æ–°ç³»çµ±æŒ‡ä»¤: {mode}")
    
    def register_mem_provider(self, data_type: str, provider_func: Callable):
        """è¨»å†Š MEM æ¨¡çµ„è³‡æ–™æä¾›è€…"""
        state_aware_interface.register_chat_mem_provider(data_type, provider_func)
        debug_log(2, f"[PromptManager] MEM æ¨¡çµ„è³‡æ–™æä¾›è€…å·²è¨»å†Š: {data_type}")
    
    def register_sys_provider(self, data_type: str, provider_func: Callable):
        """è¨»å†Š SYS æ¨¡çµ„è³‡æ–™æä¾›è€…"""
        state_aware_interface.register_work_sys_provider(data_type, provider_func)
        debug_log(2, f"[PromptManager] SYS æ¨¡çµ„è³‡æ–™æä¾›è€…å·²è¨»å†Š: {data_type}")
    
    def get_template_stats(self) -> Dict[str, Any]:
        """ç²å–æ¨¡æ¿ä½¿ç”¨çµ±è¨ˆ"""
        data_info = state_aware_interface.get_available_data_types()
        return {
            "cached_templates": len(self._template_cache),
            "available_instructions": list(self.system_instructions.keys()),
            "mem_providers_count": len(data_info.get("chat_mem_providers", [])),
            "sys_providers_count": len(data_info.get("work_sys_providers", [])),
            "total_providers": data_info.get("total_providers", 0)
        }