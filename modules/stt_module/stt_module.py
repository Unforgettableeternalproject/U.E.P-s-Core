# modules/stt_module/stt_module.py
# STT Module Phase 3 - æŒçºŒèƒŒæ™¯ç›£è½ + å¯¦æ™‚èªéŸ³è­˜åˆ¥æ•´åˆ + NLPæ¨¡çµ„é€£æ¥

import threading
import queue
import time
import re
import numpy as np
import tempfile
import os
import warnings
from typing import Optional, Dict, Any, cast
warnings.filterwarnings("ignore", category=UserWarning)

# æ–°çš„æ ¸å¿ƒä¾è³´
import torch
import pyaudio
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline

from core.bases.module_base import BaseModule
from utils.debug_helper import debug_log, info_log, error_log
from configs.config_loader import load_module_config
from core.schemas import STTModuleData
from .schemas import STTInput, STTOutput, ActivationMode, SpeakerInfo

# ç¨ç«‹æ¨¡çµ„
from .vad import VoiceActivityDetection
from .speaker_identification import SpeakerIdentification

def correct_stt(text):
    """STT çµæœä¿®æ­£ - ä¸»è¦é‡å°è‹±æ–‡è­˜åˆ¥å„ªåŒ–"""
    corrections = {
        # UEP ç›¸é—œä¿®æ­£
        "you ep": "UEP",
        "youpee": "UEP", 
        "uvp": "UEP",
        "u e p": "UEP",
        "u.e.p": "UEP",
        "uep": "UEP",
        "you e p": "UEP",
        "yu ep": "UEP",
        "yup": "UEP",
        
        # å¸¸è¦‹è‹±æ–‡ä¿®æ­£
        "cant": "can't",
        "wont": "won't",
        "dont": "don't",
        "isnt": "isn't",
        
        # èªéŸ³åŠ©æ‰‹å¸¸è¦‹èª¤è­˜åˆ¥
        "hey you ep": "hey UEP",
        "hello you ep": "hello UEP",
        "hi you ep": "hi UEP"
    }
    
    result = text.lower()
    for wrong, correct in corrections.items():
        result = result.replace(wrong, correct)
    
    # ä¿æŒåŸæœ‰å¤§å°å¯«æ ¼å¼ï¼Œä½†ç¢ºä¿ UEP æ˜¯å¤§å¯«
    if "uep" in result.lower():
        result = re.sub(r'\buep\b', 'UEP', result, flags=re.IGNORECASE)
    
    return result

class STTModule(BaseModule):
    def __init__(self, config=None, working_context_manager=None, result_callback=None):
        self.config = config or load_module_config("stt_module")
        
        # å·¥ä½œä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.working_context_manager = working_context_manager
        
        # çµæœå›èª¿å‡½æ•¸ï¼Œç”¨æ–¼å°‡è­˜åˆ¥çµæœç™¼é€çµ¦NLPæ¨¡çµ„
        self.result_callback = result_callback
        
        # åŸºæœ¬é…ç½®
        self.device_index = self.config.get("device_index", None)  # å…è¨±è‡ªå‹•é¸æ“‡éº¥å…‹é¢¨
        self.phrase_time_limit = self.config.get("phrase_time_limit", 5)
        self.sample_rate = 16000  # Whisper æ¨™æº–æ¡æ¨£ç‡
        
        # Transformers Whisper æ¨¡å‹é…ç½®
        self.whisper_model_id = self.config.get("whisper_model_id", "openai/whisper-large-v3")
        self.whisper_local_path = self.config.get("whisper_local_path", "models/stt/whisper/whisper-large-v3")
        self.use_local_model = self.config.get("use_local_model", True)
        
        # è¨­å‚™é…ç½®
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        
        # æ¨¡å‹çµ„ä»¶
        self.model = None
        self.processor = None
        self.pipe = None
        
        # èªªè©±äººè­˜åˆ¥æ¨¡å¼é…ç½® (ç¾åœ¨ä½¿ç”¨çµ±ä¸€ç³»çµ±ï¼Œä½†ä¿ç•™æ­¤è®Šæ•¸ä»¥å…¼å®¹ç¾æœ‰é…ç½®)
        self.speaker_recognition_mode = self.config.get("speaker_recognition_mode", "unified")
        
        # ç¨ç«‹æ¨¡çµ„
        self.vad_module = VoiceActivityDetection(self.sample_rate)
        self.speaker_module = SpeakerIdentification(config=self.config)  # å¢å¼·ç‰ˆèªè€…è­˜åˆ¥ç³»çµ±
        
        # ç›£è½æ§åˆ¶
        self.should_stop_listening = False
        
        # PyAudio é…ç½®
        self.pyaudio_instance = None
        self.audio_stream = None
        self.pa_config = {
            "format": pyaudio.paInt16,
            "channels": 1,
            "rate": self.sample_rate,
            "frames_per_buffer": 1024,
        }
        
        # ç•¶å‰ç‹€æ…‹
        self._current_mode = ActivationMode.MANUAL
        self._listening_active = False

        self.is_initialized = False
        info_log("[STT] Transformers Whisper + pyannote æ¶æ§‹æ¨¡çµ„åˆå§‹åŒ–å®Œæˆ")

    def debug(self):
        debug_log(1, "[STT] Debug æ¨¡å¼å•Ÿç”¨")
        debug_log(2, f"[STT] åŸºæœ¬è¨­å®š: è¨­å‚™={self.device_index}, æ¡æ¨£ç‡={self.sample_rate}")
        debug_log(2, f"[STT] æ¨¡å‹ ID: {self.whisper_model_id}")
        debug_log(2, f"[STT] æœ¬åœ°è·¯å¾‘: {self.whisper_local_path}")
        debug_log(3, f"[STT] ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {self.use_local_model}")
        debug_log(3, f"[STT] è¨ˆç®—è¨­å‚™: {self.device}, æ•¸æ“šé¡å‹: {self.torch_dtype}")
        debug_log(3, f"[STT] PyAudio é…ç½®: {self.pa_config}")
        debug_log(3, f"[STT] æ¨¡å¼: æŒçºŒèƒŒæ™¯ç›£è½ï¼Œå¯¦æ™‚å‚³é€çµæœçµ¦NLPæ¨¡çµ„")
        debug_log(4, f"[STT] å®Œæ•´æ¨¡çµ„è¨­å®š: {self.config}")

    def initialize(self):
        debug_log(1, "[STT] åˆå§‹åŒ–ä¸­...")
        self.debug()

        try:
            # åˆå§‹åŒ– Transformers Whisper æ¨¡å‹
            model_path = None
            if self.use_local_model and os.path.exists(self.whisper_local_path):
                model_path = self.whisper_local_path
                info_log(f"[STT] ä½¿ç”¨æœ¬åœ° Transformers æ¨¡å‹: {model_path}")
            else:
                model_path = self.whisper_model_id
                info_log(f"[STT] ä½¿ç”¨é ç«¯ Transformers æ¨¡å‹: {model_path}")
            
            # è¼‰å…¥æ¨¡å‹
            info_log("[STT] è¼‰å…¥ Whisper æ¨¡å‹...")
            self.model = AutoModelForSpeechSeq2Seq.from_pretrained(
                model_path,
                torch_dtype=self.torch_dtype,
                low_cpu_mem_usage=True,
                use_safetensors=True
            )
            self.model.to(self.device)
            
            # è¼‰å…¥è™•ç†å™¨
            info_log("[STT] è¼‰å…¥è™•ç†å™¨...")
            if self.use_local_model and os.path.exists(self.whisper_local_path):
                self.processor = AutoProcessor.from_pretrained(self.whisper_local_path)
            else:
                self.processor = AutoProcessor.from_pretrained(self.whisper_model_id)
            
            # å‰µå»º pipeline
            info_log("[STT] å‰µå»ºèªéŸ³è­˜åˆ¥ pipeline...")
            self.pipe = pipeline(
                "automatic-speech-recognition",
                model=self.model,
                tokenizer=self.processor.tokenizer,
                feature_extractor=self.processor.feature_extractor,
                torch_dtype=self.torch_dtype,
                device=self.device,
            )
            
            info_log(f"[STT] Transformers Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ (è¨­å‚™: {self.device})")
            
            # åˆå§‹åŒ– PyAudio
            self.pyaudio_instance = pyaudio.PyAudio()
            info_log("[STT] PyAudio åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–æ–°çš„ç¨ç«‹æ¨¡çµ„
            info_log("[STT] åˆå§‹åŒ– VAD æ¨¡çµ„...")
            if not self.vad_module.initialize():
                error_log("[STT] VAD æ¨¡çµ„åˆå§‹åŒ–å¤±æ•—ï¼Œä½†ä¸å½±éŸ¿åŸºæœ¬ STT åŠŸèƒ½")
            
            info_log("[STT] åˆå§‹åŒ–èªªè©±äººè­˜åˆ¥æ¨¡çµ„...")
            if not self.speaker_module.initialize():
                info_log("[STT] èªªè©±äººè­˜åˆ¥æ¨¡çµ„ä½¿ç”¨ fallback æ¨¡å¼ï¼ŒåŸºæœ¬åŠŸèƒ½ä»å¯ä½¿ç”¨")
            else:
                info_log("[STT] èªªè©±äººè­˜åˆ¥æ¨¡çµ„åˆå§‹åŒ–æˆåŠŸ")
            
            # èªè€…è­˜åˆ¥å·²ç¶“åˆå§‹åŒ–å®Œç•¢
            
            # åˆ—å‡ºå¯ç”¨çš„éŸ³é »è¨­å‚™
            debug_log(3, "[STT] å¯ç”¨éŸ³é »è¨­å‚™ï¼š")
            for i in range(self.pyaudio_instance.get_device_count()):
                device_info = self.pyaudio_instance.get_device_info_by_index(i)
                max_input = device_info.get('maxInputChannels', 0)
                if isinstance(max_input, int) and max_input > 0:
                    device_name = device_info.get('name', 'Unknown')
                    debug_log(3, f"  è¨­å‚™ {i}: {device_name}")
            
            # è¨­ç½®åˆå§‹åŒ–å®Œæˆæ¨™èªŒ
            self.is_initialized = True
            
            return True
            
        except Exception as e:
            error_log(f"[STT] åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
            return False

    def handle(self, data: dict = {}) -> dict:
        """è™•ç† STT è«‹æ±‚"""
        try:
            # ç›´æ¥è½‰æ›ç‚ºæ¨¡çµ„å…§éƒ¨ä½¿ç”¨çš„æ ¼å¼
            validated = STTInput(**data)
            debug_log(1, f"[STT] è™•ç†è«‹æ±‚: {validated.mode}")
            
            start_time = time.time()
            
            if validated.mode == ActivationMode.MANUAL:
                # æ‰‹å‹•æ¨¡å¼ï¼šç«‹å³éŒ„éŸ³è­˜åˆ¥
                result = self._manual_recognition(validated)
            elif validated.mode == ActivationMode.CONTINUOUS:
                # æŒçºŒèƒŒæ™¯ç›£è½æ¨¡å¼ï¼šæŒçºŒéŒ„éŸ³ä¸¦å¯¦æ™‚å‚³é€çµæœçµ¦NLP
                result = self._continuous_recognition(validated)
            else:
                # ä¸æ”¯æŒçš„æ¨¡å¼
                return STTOutput(
                    text="", 
                    confidence=0.0, 
                    error="ä¸æ”¯æŒçš„æ¨¡å¼",
                    activation_reason="ä¸æ”¯æŒçš„æ¨¡å¼"
                ).model_dump()
                
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            
            # å°‡çµæœè½‰æ›ç‚º STTOutput ç‰©ä»¶
            stt_output = STTOutput(**result)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è­˜åˆ¥å‡ºæ–‡æœ¬ï¼ˆä½†æŒçºŒç›£è½å®Œæˆä¸ç®—éŒ¯èª¤ï¼‰
            is_listening_completed = stt_output.activation_reason == "continuous_listening_completed"
            if not stt_output.text or not stt_output.text.strip():
                if is_listening_completed:
                    # æŒçºŒç›£è½æ­£å¸¸çµæŸï¼Œä¸æ˜¯éŒ¯èª¤
                    stt_output.error = None
                else:
                    # å…¶ä»–æƒ…æ³ä¸‹ï¼Œç©ºæ–‡æœ¬æ˜¯éŒ¯èª¤
                    info_log("[STT] ğŸ”‡ æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹")
                    stt_output.error = "æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹"
            else:
                stt_output.error = None
            
            # è¿”å›å­—å…¸æ ¼å¼
            return stt_output.model_dump()
            
        except Exception as e:
            error_log(f"[STT] è™•ç†å¤±æ•—: {str(e)}")
            return STTOutput(
                text="",
                confidence=0.0,
                error=f"è™•ç†å¤±æ•—: {str(e)}"
            ).model_dump()
    
    def stop_listening(self):
        """åœæ­¢æŒçºŒç›£è½"""
        self.should_stop_listening = True
        debug_log(2, "[STT] è¨­ç½®åœæ­¢ç›£è½æ¨™èªŒ")
    
    def resume_listening(self):
        """æ¢å¾©ç›£è½èƒ½åŠ›"""
        self.should_stop_listening = False
        debug_log(2, "[STT] æ¸…é™¤åœæ­¢ç›£è½æ¨™èªŒ")
    
    def handle_text_input(self, text: str) -> dict:
        """
        è™•ç†æ–‡å­—è¼¸å…¥ - ç¹éèªéŸ³è­˜åˆ¥å’Œèªªè©±äººè¾¨è­˜
        
        é€™æ˜¯ä¸€å€‹ç‰¹æ®Šçš„å…¥å£é»,ç”¨æ–¼:
        1. ä¸éœ€è¦èªéŸ³æ´»å‹•æª¢æ¸¬çš„æƒ…æ³
        2. ç”¨æˆ¶é¸æ“‡é—œé–‰ STT åŠŸèƒ½ä½†ä»æƒ³ä½¿ç”¨ç³»çµ±çš„æƒ…æ³
        3. æ¸¬è©¦å’Œé–‹ç™¼ç›®çš„
        
        ç‰¹æ€§:
        - ä¸é€²è¡ŒèªéŸ³è­˜åˆ¥ (ç›´æ¥ä½¿ç”¨æ–‡å­—)
        - ä¸é€²è¡Œèªªè©±äººè¾¨è­˜ (speaker_info=None)
        - ä¸å‰µå»º speaker_accumulation ä¸Šä¸‹æ–‡
        - NLP å°‡ä½¿ç”¨é è¨­èº«ä»½è™•ç†
        
        Args:
            text: ç”¨æˆ¶è¼¸å…¥çš„æ–‡å­—å…§å®¹
            
        Returns:
            dict: çµ±ä¸€æ ¼å¼çš„è¼¸å‡ºçµæœ
        """
        try:
            if not text or text.isspace():
                debug_log(2, "[STT] æ–‡å­—è¼¸å…¥ç‚ºç©ºï¼Œå¿½ç•¥")
                return STTOutput(
                    text="",
                    confidence=0.0,
                    speaker_info=None,
                    activation_reason="text_input_empty",
                    error="æ–‡å­—è¼¸å…¥ç‚ºç©º"
                ).model_dump()
            
            # ğŸ†• æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½• cycle æ­£åœ¨è™•ç†è¼¸å…¥
            # å¦‚æœæœ‰ï¼Œç­‰å¾…ç•¶å‰æ‰€æœ‰ cycle å®Œæˆï¼ˆæ¨¡æ“¬ VAD åœ¨ cycle æœªçµæŸæ™‚ä¸æ¥å—æ–°è¼¸å…¥çš„è¡Œç‚ºï¼‰
            from core.sessions.session_manager import unified_session_manager
            from core.system_loop import system_loop
            import time
            
            active_cs = unified_session_manager.get_active_chatting_session_ids()
            active_ws = unified_session_manager.get_active_workflow_session_ids()
            
            debug_log(2, f"[STT] æ–‡å­—è¼¸å…¥ç­‰å¾…æª¢æŸ¥: active_cs={len(active_cs) if active_cs else 0}, active_ws={len(active_ws) if active_ws else 0}")
            
            if active_cs or active_ws:
                debug_log(2, f"[STT] æª¢æ¸¬åˆ°æ´»èºæœƒè©±ï¼Œæª¢æŸ¥ cycle tracking")
                # æœ‰æ´»èºæœƒè©±ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½• cycle æ­£åœ¨è™•ç†
                if hasattr(system_loop, '_cycle_layer_tracking'):
                    max_wait_time = 30.0  # æœ€å¤šç­‰å¾… 30 ç§’
                    wait_start = time.time()
                    
                    with system_loop._cycle_tracking_lock:
                        tracking_count = len(system_loop._cycle_layer_tracking)
                        debug_log(2, f"[STT] ç•¶å‰ cycle tracking æ•¸é‡: {tracking_count}")
                    
                    if tracking_count > 0:
                        info_log(f"[STT] â³ ç­‰å¾…ç•¶å‰ cycle å®Œæˆï¼ˆæ¨¡æ“¬ VAD è¡Œç‚ºï¼‰...")
                    
                    while time.time() - wait_start < max_wait_time:
                        with system_loop._cycle_tracking_lock:
                            # å¦‚æœæ²’æœ‰ä»»ä½• cycle æ­£åœ¨è¿½è¹¤ï¼Œè¡¨ç¤ºå¯ä»¥æ¥å—æ–°è¼¸å…¥
                            if len(system_loop._cycle_layer_tracking) == 0:
                                debug_log(2, f"[STT] âœ“ æ‰€æœ‰ cycle å·²å®Œæˆï¼Œæ¥å—æ–°è¼¸å…¥")
                                break
                            
                            # è¨˜éŒ„ç­‰å¾…çš„ cycle
                            tracking_keys = list(system_loop._cycle_layer_tracking.keys())
                            debug_log(3, f"[STT] ç­‰å¾… cycle å®Œæˆ: {tracking_keys}")
                        
                        time.sleep(0.1)
                    else:
                        # ç­‰å¾…è¶…æ™‚
                        debug_log(1, f"[STT] âš ï¸ ç­‰å¾… cycle å®Œæˆè¶…æ™‚ï¼Œå¼·åˆ¶æ¥å—è¼¸å…¥")
                else:
                    debug_log(2, f"[STT] system_loop æ²’æœ‰ _cycle_layer_tracking å±¬æ€§")
            else:
                debug_log(2, f"[STT] æ²’æœ‰æ´»èºæœƒè©±ï¼Œç›´æ¥æ¥å—è¼¸å…¥")
            
            info_log(f"[STT] æ–‡å­—è¼¸å…¥æ¨¡å¼: '{text}'")
            
            # å‰µå»ºè¼¸å‡ºç‰©ä»¶ - ä¸åŒ…å«èªªè©±äººè³‡è¨Š
            output = STTOutput(
                text=text.strip(),
                confidence=1.0,  # æ–‡å­—è¼¸å…¥è¦–ç‚º 100% ä¿¡å¿ƒåº¦
                speaker_info=None,  # æ˜ç¢ºè¨­ç‚º None,è¡¨ç¤ºç¹éèªªè©±äººè­˜åˆ¥
                activation_reason="text_input",
                error=None
            )
            
            # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
            unified_data = output.to_unified_format()
            
            # æ·»åŠ ç‰¹æ®Šæ¨™è¨˜åˆ° metadata
            unified_data.metadata["input_mode"] = "text"
            unified_data.metadata["bypass_speaker_id"] = True
            
            # é€šéå›èª¿å°‡çµæœç™¼é€çµ¦ NLP æ¨¡çµ„
            if self.result_callback:
                try:
                    self.result_callback(unified_data)
                    info_log(f"[STT] æ–‡å­—è¼¸å…¥å·²ç™¼é€çµ¦ NLP æ¨¡çµ„: '{text}'")
                except Exception as e:
                    error_log(f"[STT] ç™¼é€æ–‡å­—è¼¸å…¥çµæœå¤±æ•—: {e}")
            else:
                debug_log(2, "[STT] æœªè¨­å®šçµæœå›èª¿å‡½æ•¸")
            
            return output.model_dump()
            
        except Exception as e:
            error_log(f"[STT] è™•ç†æ–‡å­—è¼¸å…¥å¤±æ•—: {str(e)}")
            return STTOutput(
                text="",
                confidence=0.0,
                speaker_info=None,
                activation_reason="text_input_error",
                error=f"è™•ç†æ–‡å­—è¼¸å…¥å¤±æ•—: {str(e)}"
            ).model_dump()

    def _manual_recognition(self, input_data: STTInput) -> dict:
        """æ‰‹å‹•èªéŸ³è­˜åˆ¥ - ä½¿ç”¨ Transformers Whisper"""
        try:
            info_log("[STT] é–‹å§‹éŒ„éŸ³...")
            
            # ä½¿ç”¨ PyAudio ç›´æ¥éŒ„éŸ³
            duration = input_data.duration if input_data.duration else self.phrase_time_limit
            audio_data = self._record_audio(duration)
            
            if audio_data is None or len(audio_data) == 0:
                return STTOutput(
                    text="", 
                    confidence=0.0, 
                    error="éŒ„éŸ³å¤±æ•—æˆ–éŸ³é »ç‚ºç©º",
                    activation_reason="éŒ„éŸ³å¤±æ•—"
                ).model_dump()
            
            info_log("[STT] ä½¿ç”¨ Transformers Whisper é€²è¡ŒèªéŸ³è­˜åˆ¥...")
            
            # æ­£è¦åŒ–éŸ³é »æ•¸æ“šåˆ° [-1, 1] ç¯„åœ
            audio_float = audio_data.astype(np.float32) / 32768.0
            
            # ç”Ÿæˆåƒæ•¸é…ç½®
            generate_kwargs = {
                "max_new_tokens": 128,  # é™ä½åˆ°å®‰å…¨ç¯„åœ
                "num_beams": 1,
                "condition_on_prev_tokens": False,
                "compression_ratio_threshold": 1.35,
                "temperature": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
                "logprob_threshold": -1.0,
                "no_speech_threshold": 0.4,  # é™ä½é–¾å€¼ä»¥æé«˜æ•æ„Ÿåº¦
                "return_timestamps": True,
                "task": "translate",  # ç¿»è­¯ä»»å‹™ï¼šå°‡æ‰€æœ‰èªè¨€ç¿»è­¯æˆè‹±æ–‡
            }
            
            # æª¢æŸ¥éŸ³é »æ•¸æ“šæ˜¯å¦æœ‰èªéŸ³å…§å®¹
            if not self.vad_module.has_sufficient_speech(audio_data):
                info_log("[STT] VAD æª¢æ¸¬ï¼šæœªæª¢æ¸¬åˆ°è¶³å¤ èªéŸ³å…§å®¹ï¼Œä½†ä»å˜—è©¦è­˜åˆ¥")
            
            # ä½¿ç”¨ Transformers pipeline é€²è¡ŒèªéŸ³è­˜åˆ¥
            if self.pipe is None:
                error_log("[STT] Pipeline æœªåˆå§‹åŒ–")
                return STTOutput(text="", confidence=0.0, error="Pipeline æœªåˆå§‹åŒ–").model_dump()
            
            result = self.pipe(
                audio_float,
                generate_kwargs=generate_kwargs
            )
            
            # é¡å‹è½‰æ› - Transformers pipeline è¿”å› dict
            result_dict = cast(Dict[str, Any], result)
            text = str(result_dict.get("text", "")).strip()
            text = correct_stt(text)
            confidence = self._calculate_transformers_confidence(result_dict)
            
            # æª¢æŸ¥çµæœæ˜¯å¦ç‚ºç©º
            if not text or text.isspace():
                info_log("[STT] ğŸ”‡ æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹")
                return STTOutput(
                    text="",
                    confidence=0.0,
                    speaker_info=None,
                    activation_reason="manual",
                    error="æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹"
                ).model_dump()
            
            # èªªè©±äººè­˜åˆ¥ - æ ¹æ“šé…ç½®é¸æ“‡æ¨¡å¼
            speaker_info = None
            if input_data.enable_speaker_id:
                speaker_info = self._identify_speaker_with_mode(audio_data)
                
                # å°‡éŸ³é »æ¨£æœ¬æ·»åŠ åˆ° Speaker_Accumulation ä¸Šä¸‹æ–‡
                self._add_audio_sample_to_accumulation(audio_data, speaker_info)
            
            # é¡¯ç¤ºè­˜åˆ¥çµæœ
            info_log(f"[STT] è­˜åˆ¥çµæœ: '{text}' (ä¿¡å¿ƒåº¦: {confidence:.2f})")
            
            return STTOutput(
                text=text,
                confidence=confidence,
                speaker_info=speaker_info,
                activation_reason="manual",
                error=None
            ).model_dump()
            
        except Exception as e:
            error_log(f"[STT] è­˜åˆ¥å¤±æ•—: {str(e)}")
            return STTOutput(
                text="", 
                confidence=0.0, 
                error=f"è­˜åˆ¥å¤±æ•—: {str(e)}",
                activation_reason="è­˜åˆ¥å¤±æ•—ï¼šæœªçŸ¥éŒ¯èª¤"
            ).model_dump()

    def _record_audio(self, duration: float) -> np.ndarray:
        """ä½¿ç”¨ PyAudio éŒ„è£½éŸ³é »"""
        try:
            # å‰µå»ºéŸ³é »æµ
            stream_params = {
                "format": self.pa_config["format"],
                "channels": self.pa_config["channels"],
                "rate": self.pa_config["rate"],
                "input": True,
                "frames_per_buffer": self.pa_config["frames_per_buffer"]
            }
            
            # åªæœ‰ç•¶è¨­å‚™ç´¢å¼•è¢«æ˜ç¢ºæŒ‡å®šæ™‚æ‰æ·»åŠ 
            if self.device_index is not None:
                stream_params["input_device_index"] = self.device_index
            
            if self.pyaudio_instance is None:
                error_log("[STT] PyAudio æœªåˆå§‹åŒ–")
                return np.array([])
                
            stream = self.pyaudio_instance.open(**stream_params)
            
            frames = []
            frames_to_record = int(self.sample_rate * duration / self.pa_config["frames_per_buffer"])
            
            info_log(f"[STT] é–‹å§‹éŒ„éŸ³ {duration} ç§’...")
            for _ in range(frames_to_record):
                data = stream.read(self.pa_config["frames_per_buffer"])
                frames.append(data)
            
            stream.stop_stream()
            stream.close()
            
            # è½‰æ›ç‚º numpy æ•¸çµ„
            audio_data = b''.join(frames)
            audio_array = np.frombuffer(audio_data, dtype=np.int16)
            
            # ç°¡å–®çš„éŸ³é »å‰è™•ç†ï¼šæ­¸ä¸€åŒ–ä¸¦å¢å¼·
            if len(audio_array) > 0:
                # æª¢æŸ¥éŸ³é »æ˜¯å¦å…¨ç‚ºéœéŸ³
                if np.max(np.abs(audio_array)) > 0:
                    # æ­¸ä¸€åŒ–åˆ°æœ€å¤§æŒ¯å¹…çš„ 90%
                    norm_factor = 0.9 * 32767 / np.max(np.abs(audio_array))
                    audio_array = (audio_array * norm_factor).astype(np.int16)
            
            info_log(f"[STT] éŒ„éŸ³å®Œæˆï¼Œé•·åº¦: {len(audio_array) / self.sample_rate:.2f} ç§’")
            return audio_array
            
        except Exception as e:
            error_log(f"[STT] éŒ„éŸ³å¤±æ•—: {str(e)}")
            return np.array([])

    def _calculate_transformers_confidence(self, result: dict) -> float:
        """è¨ˆç®— Transformers Whisper çµæœçš„ä¿¡å¿ƒåº¦"""
        try:
            # åŸºæ–¼æ–‡æœ¬å…§å®¹å’Œæ™‚é–“æˆ³è³‡è¨Šä¼°ç®—ä¿¡å¿ƒåº¦
            text = result.get("text", "").strip()
            
            if not text:
                return 0.0
            
            base_confidence = 0.8  # Transformers æ¨¡å‹é€šå¸¸æœ‰æ›´é«˜çš„åŸºç¤ä¿¡å¿ƒåº¦
            
            # åŸºæ–¼æ–‡æœ¬é•·åº¦èª¿æ•´
            text_length = len(text.split())
            if 1 <= text_length <= 30:
                length_bonus = 0.15
            elif text_length > 30:
                length_bonus = 0.1
            else:
                length_bonus = 0.0
            
            # åŸºæ–¼æ˜¯å¦åŒ…å«å¸¸è¦‹è©å½™
            common_words = ["UEP", "help", "please", "can", "you", "the", "a", "an", "is", "are"]
            common_word_count = sum(1 for word in text.lower().split() if word in common_words)
            common_word_bonus = min(common_word_count * 0.03, 0.1)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ™‚é–“æˆ³è³‡è¨Šï¼ˆè¡¨ç¤ºæ¨¡å‹å°çµæœæœ‰ä¿¡å¿ƒï¼‰
            if "chunks" in result and result["chunks"]:
                timestamp_bonus = 0.05
            else:
                timestamp_bonus = 0.0
            
            confidence = min(base_confidence + length_bonus + common_word_bonus + timestamp_bonus, 1.0)
            return confidence
            
        except Exception as e:
            debug_log(3, f"[STT] è¨ˆç®—ä¿¡å¿ƒåº¦å¤±æ•—: {str(e)}")
            return 0.8  # é»˜èªä¿¡å¿ƒåº¦

    def shutdown(self):
        """é—œé–‰æ¨¡çµ„"""
        # æ¸…ç† PyAudio
        if self.pyaudio_instance:
            self.pyaudio_instance.terminate()
        
        # æ¸…ç† GPU è¨˜æ†¶é«”
        if self.model is not None:
            del self.model
        if self.pipe is not None:
            del self.pipe
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # é—œé–‰ç¨ç«‹æ¨¡çµ„
        if hasattr(self, 'vad_module'):
            self.vad_module.shutdown()
        if hasattr(self, 'speaker_module'):
            self.speaker_module.shutdown()
        
        info_log("[STT] æ¨¡çµ„å·²é—œé–‰")

    def _continuous_recognition(self, input_data: STTInput) -> dict:
        """æŒçºŒèƒŒæ™¯ç›£è½ - æŒçºŒéŒ„éŸ³ï¼Œæª¢æ¸¬åˆ°å®Œæ•´èªéŸ³ç‰‡æ®µå¾Œåˆä½µç™¼é€çµ¦NLPæ¨¡çµ„"""
        try:
            info_log("[STT] é–‹å§‹æŒçºŒèƒŒæ™¯ç›£è½æ¨¡å¼ï¼ˆæ™ºèƒ½èªéŸ³ç‰‡æ®µåˆä½µï¼‰...")
            
            # è¨­å®šç›£è½æ™‚é•·ï¼Œå¦‚æœæœªæŒ‡å®šå‰‡ä½¿ç”¨é»˜èªå€¼
            duration = input_data.duration or 30.0
            start_time = time.time()
            
            # VAD åƒæ•¸é…ç½®
            chunk_duration = 2.0  # æ¯æ¬¡éŒ„éŸ³çš„ç‰‡æ®µé•·åº¦ï¼ˆç§’ï¼‰
            silence_threshold = 1.5  # éœéŸ³æŒçºŒæ™‚é–“é–¾å€¼ï¼ˆç§’ï¼‰ï¼Œè¶…éæ­¤æ™‚é–“è¦–ç‚ºèªéŸ³çµæŸ
            max_speech_duration = 30.0  # å–®æ¬¡èªéŸ³æœ€å¤§é•·åº¦ï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢ç„¡é™ç´¯ç©
            
            info_log(f"[STT] æŒçºŒç›£è½é…ç½®: ç¸½æ™‚é•·={duration}s, ç‰‡æ®µé•·åº¦={chunk_duration}s, "
                    f"éœéŸ³é–¾å€¼={silence_threshold}s, æœ€å¤§èªéŸ³é•·åº¦={max_speech_duration}s")
            
            # å‰µå»ºèªè€…ä¸Šä¸‹æ–‡ï¼Œç”¨æ–¼ç´¯ç©èªè€…è³‡è¨Š
            context_id = None
            if self.working_context_manager:
                from core.working_context import ContextType
                context_id = self.working_context_manager.create_context(
                    ContextType.SPEAKER_ACCUMULATION, 
                    threshold=15,  # æ¨£æœ¬é–¾å€¼
                    timeout=300.0  # 5åˆ†é˜éæœŸ
                )
                debug_log(2, f"[STT] å·²å»ºç«‹æŒçºŒç›£è½çš„èªéŸ³ç´¯ç©ä¸Šä¸‹æ–‡: {context_id}")
            
            # èªéŸ³ç‰‡æ®µç·©è¡å€
            audio_buffer = []  # å­˜å„²å¾…åˆä½µçš„éŸ³é »ç‰‡æ®µ
            last_speech_time = None  # æœ€å¾Œæª¢æ¸¬åˆ°èªéŸ³çš„æ™‚é–“
            speech_start_time = None  # ç•¶å‰èªéŸ³ç‰‡æ®µé–‹å§‹æ™‚é–“
            
            # æŒçºŒç›£è½ç›´åˆ°é”åˆ°æŒ‡å®šæ™‚é–“æˆ–æ”¶åˆ°åœæ­¢ä¿¡è™Ÿ
            while time.time() - start_time < duration and not self.should_stop_listening:
                current_time = time.time()
                
                # éŒ„è£½éŸ³é »ç‰‡æ®µ
                audio_chunk = self._record_audio(chunk_duration)
                
                if audio_chunk is None or len(audio_chunk) == 0:
                    debug_log(3, "[STT] éŒ„éŸ³å¤±æ•—æˆ–éŸ³é »ç‚ºç©ºï¼Œç¹¼çºŒç›£è½")
                    continue
                
                # ä½¿ç”¨VADæª¢æŸ¥æ˜¯å¦æœ‰èªéŸ³å…§å®¹
                has_speech = self.vad_module.has_sufficient_speech(audio_chunk, min_duration=0.05)
                
                if has_speech:
                    # æª¢æ¸¬åˆ°èªéŸ³
                    debug_log(3, "[STT] æª¢æ¸¬åˆ°èªéŸ³å…§å®¹ï¼ŒåŠ å…¥ç·©è¡å€")
                    
                    # å¦‚æœé€™æ˜¯æ–°çš„èªéŸ³ç‰‡æ®µé–‹å§‹
                    if speech_start_time is None:
                        speech_start_time = current_time
                        info_log("[STT] ğŸ¤ èªéŸ³é–‹å§‹...")
                    
                    # å°‡éŸ³é »æ·»åŠ åˆ°ç·©è¡å€
                    audio_buffer.append(audio_chunk)
                    last_speech_time = current_time
                    
                    # æª¢æŸ¥æ˜¯å¦è¶…éæœ€å¤§èªéŸ³é•·åº¦
                    if current_time - speech_start_time > max_speech_duration:
                        info_log(f"[STT] èªéŸ³ç‰‡æ®µé”åˆ°æœ€å¤§é•·åº¦ ({max_speech_duration}s)ï¼Œå¼·åˆ¶è™•ç†")
                        self._process_audio_buffer(
                            audio_buffer, 
                            context_id, 
                            input_data.enable_speaker_id
                        )
                        # é‡ç½®ç·©è¡å€
                        audio_buffer = []
                        last_speech_time = None
                        speech_start_time = None
                    
                else:
                    # æœªæª¢æ¸¬åˆ°èªéŸ³ï¼ˆéœéŸ³ï¼‰
                    if audio_buffer:
                        # è¨ˆç®—éœéŸ³æŒçºŒæ™‚é–“
                        silence_duration = current_time - last_speech_time if last_speech_time else 0
                        debug_log(3, f"[STT] éœéŸ³æŒçºŒ: {silence_duration:.2f}s / {silence_threshold}s")
                        
                        # å¦‚æœéœéŸ³æ™‚é–“è¶…éé–¾å€¼ï¼Œè™•ç†ç·©è¡å€ä¸­çš„éŸ³é »
                        if silence_duration >= silence_threshold:
                            speech_duration = (last_speech_time - speech_start_time) if (last_speech_time and speech_start_time) else 0
                            info_log(f"[STT] ğŸ“ èªéŸ³çµæŸ (æ™‚é•·: {speech_duration:.2f}s)ï¼Œé–‹å§‹è™•ç†...")
                            
                            self._process_audio_buffer(
                                audio_buffer, 
                                context_id, 
                                input_data.enable_speaker_id
                            )
                            
                            # é‡ç½®ç·©è¡å€
                            audio_buffer = []
                            last_speech_time = None
                            speech_start_time = None
                    else:
                        debug_log(3, "[STT] éœéŸ³ç‹€æ…‹ï¼Œç­‰å¾…èªéŸ³...")
                
                # çŸ­æš«ä¼‘æ¯é¿å…éåº¦ä½”ç”¨CPU
                time.sleep(0.05)
            
            # ç›£è½çµæŸæ™‚ï¼Œå¦‚æœç·©è¡å€é‚„æœ‰æœªè™•ç†çš„éŸ³é »ï¼Œè™•ç†å®ƒ
            if audio_buffer:
                info_log("[STT] ç›£è½çµæŸï¼Œè™•ç†å‰©é¤˜éŸ³é »ç·©è¡å€...")
                self._process_audio_buffer(
                    audio_buffer, 
                    context_id, 
                    input_data.enable_speaker_id
                )
            
            # ç›£è½çµæŸ
            if context_id and self.working_context_manager:
                # ä¸è¦æ¨™è¨˜ç‚ºå®Œæˆï¼Œå› ç‚ºæ˜¯æŒçºŒç›£è½
                pass
                
            info_log("[STT] æŒçºŒç›£è½æ¨¡å¼æ­£å¸¸çµæŸ")
            return STTOutput(
                text="",
                confidence=1.0,
                speaker_info=None,
                activation_reason="continuous_listening_completed",
                error=None
            ).model_dump()
            
        except Exception as e:
            error_log(f"[STT] æŒçºŒç›£è½å¤±æ•—: {str(e)}")
            return STTOutput(
                text="",
                confidence=0.0,
                error=f"æŒçºŒç›£è½å¤±æ•—: {str(e)}",
                activation_reason="continuous_listening_failed"
            ).model_dump()
    
    def _process_audio_buffer(self, audio_buffer: list, context_id: Optional[str], 
                             enable_speaker_id: bool) -> None:
        """è™•ç†éŸ³é »ç·©è¡å€ - åˆä½µéŸ³é »ä¸¦é€²è¡Œè­˜åˆ¥"""
        try:
            if not audio_buffer:
                debug_log(2, "[STT] éŸ³é »ç·©è¡å€ç‚ºç©ºï¼Œè·³éè™•ç†")
                return
            
            # åˆä½µæ‰€æœ‰éŸ³é »ç‰‡æ®µ
            info_log(f"[STT] åˆä½µ {len(audio_buffer)} å€‹éŸ³é »ç‰‡æ®µ...")
            merged_audio = np.concatenate(audio_buffer)
            total_duration = len(merged_audio) / self.sample_rate
            info_log(f"[STT] åˆä½µå¾ŒéŸ³é »é•·åº¦: {total_duration:.2f} ç§’")
            
            # ç™¼å¸ƒ INTERACTION_STARTED äº‹ä»¶ï¼Œé€šçŸ¥å‰ç«¯ä½¿ç”¨è€…é–‹å§‹äº’å‹•
            try:
                from core.event_bus import event_bus, SystemEvent
                event_bus.publish(
                    SystemEvent.INTERACTION_STARTED,
                    {
                        "module": "stt",
                        "input_type": "voice",
                        "audio_duration": total_duration,
                        "num_chunks": len(audio_buffer)
                    },
                    source="stt_module"
                )
                debug_log(2, "[STT] å·²ç™¼å¸ƒ INTERACTION_STARTED äº‹ä»¶")
            except Exception as e:
                debug_log(2, f"[STT] ç„¡æ³•ç™¼å¸ƒ INTERACTION_STARTED äº‹ä»¶: {e}")
            
            # å°‡éŸ³é »æ•¸æ“šæ·»åŠ åˆ°èªè€…ä¸Šä¸‹æ–‡
            if context_id and self.working_context_manager:
                self.working_context_manager.add_data_to_context(
                    context_id, 
                    merged_audio,
                    metadata={"timestamp": time.time(), "type": "merged_audio_sample"}
                )
            
            # ä½¿ç”¨Whisperé€²è¡ŒèªéŸ³è­˜åˆ¥
            info_log("[STT] å°åˆä½µéŸ³é »é€²è¡ŒèªéŸ³è­˜åˆ¥...")
            audio_float = merged_audio.astype(np.float32) / 32768.0
            
            # è­˜åˆ¥åƒæ•¸
            recognition_kwargs = {
                "max_new_tokens": 128,
                "num_beams": 1,
                "condition_on_prev_tokens": False,
                "compression_ratio_threshold": 1.35,
                "temperature": 0.0,
                "logprob_threshold": -1.0,
                "no_speech_threshold": 0.4,
                "return_timestamps": True,
                "task": "translate",  # ç¿»è­¯ä»»å‹™ï¼šå°‡æ‰€æœ‰èªè¨€ç¿»è­¯æˆè‹±æ–‡
            }
            
            if self.pipe is None:
                error_log("[STT] Pipeline æœªåˆå§‹åŒ–ï¼Œç„¡æ³•è­˜åˆ¥")
                return
            
            result = self.pipe(audio_float, generate_kwargs=recognition_kwargs)  # type: ignore
            result_dict = cast(Dict[str, Any], result)
            text = str(result_dict.get("text", "")).strip()
            text = correct_stt(text)  # æ‡‰ç”¨STTä¿®æ­£
            
            # è¨ˆç®—ä¿¡å¿ƒåº¦
            confidence = self._calculate_transformers_confidence(result_dict)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è­˜åˆ¥å‡ºæ–‡æœ¬
            if not text or text.isspace():
                debug_log(2, "[STT] æœªè­˜åˆ¥åˆ°æœ‰æ•ˆèªéŸ³å…§å®¹")
                return
            
            info_log(f"[STT] âœ… è­˜åˆ¥çµæœ: '{text}' (ä¿¡å¿ƒåº¦: {confidence:.2f})")
            
            # é€²è¡Œèªªè©±äººè­˜åˆ¥
            speaker_info = None
            if enable_speaker_id:
                speaker_info = self._identify_speaker_with_mode(merged_audio)
                if speaker_info:
                    speaker_id = speaker_info.speaker_id
                    speaker_confidence = speaker_info.confidence
                    debug_log(2, f"[STT] è­˜åˆ¥èªè€…: {speaker_id} (ä¿¡å¿ƒåº¦: {speaker_confidence:.2f})")
            
            # å‰µå»ºè¼¸å‡ºç‰©ä»¶
            output = STTOutput(
                text=text,
                confidence=confidence,
                speaker_info=speaker_info,
                activation_reason="continuous_listening",
                error=None
            )
            
            # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
            unified_data = output.to_unified_format()
            
            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡IDï¼Œæ·»åŠ åˆ°metadata
            if context_id:
                unified_data.metadata["context_id"] = context_id
            unified_data.metadata["audio_duration"] = total_duration
            unified_data.metadata["num_chunks_merged"] = len(audio_buffer)
            
            # é€šéå›èª¿å°‡çµæœç™¼é€çµ¦NLPæ¨¡çµ„
            if self.result_callback:
                try:
                    self.result_callback(unified_data)
                    speaker_id = unified_data.speaker_info.get('speaker_id', 'unknown') if unified_data.speaker_info else 'unknown'
                    info_log(f"[STT] ğŸ“¤ è­˜åˆ¥çµæœå·²ç™¼é€çµ¦NLPæ¨¡çµ„: '{text}' (èªè€…: {speaker_id})")
                except Exception as e:
                    error_log(f"[STT] ç™¼é€è­˜åˆ¥çµæœå¤±æ•—: {e}")
            else:
                debug_log(2, "[STT] æœªè¨­å®šçµæœå›èª¿å‡½æ•¸")
            
        except Exception as e:
            error_log(f"[STT] è™•ç†éŸ³é »ç·©è¡å€å¤±æ•—: {str(e)}")
            
    def _identify_speaker_with_mode(self, audio_data: np.ndarray) -> SpeakerInfo:
        """æ ¹æ“šé…ç½®çš„æ¨¡å¼é€²è¡Œèªªè©±äººè­˜åˆ¥"""
        try:
            # æˆ‘å€‘ç¾åœ¨åªæœ‰ä¸€ç¨®èªè€…è­˜åˆ¥ç³»çµ±ï¼Œç„¡éœ€å†æ ¹æ“šæ¨¡å¼é¸æ“‡
            debug_log(2, f"[STT] ä½¿ç”¨çµ±ä¸€çš„èªè€…è­˜åˆ¥ç³»çµ±")
            return self.speaker_module.identify_speaker(audio_data)
                
        except Exception as e:
            error_log(f"[STT] èªªè©±äººè­˜åˆ¥å®Œå…¨å¤±æ•—: {e}")
            # è¿”å›é»˜èªçµæœ
            return SpeakerInfo(
                speaker_id="unknown",
                confidence=0.0,
                is_new_speaker=False,
                voice_features={"error": str(e)}
            )

    def _add_audio_sample_to_accumulation(self, audio_data: np.ndarray, speaker_info: Optional[SpeakerInfo] = None):
        """å°‡éŸ³é »æ¨£æœ¬æ·»åŠ åˆ° Speaker_Accumulation ä¸Šä¸‹æ–‡ä¸­
        
        âš ï¸ é‡è¦ï¼šåªæœ‰åœ¨å·²æŒ‡å®šä½¿ç”¨è€…èº«åˆ†æ™‚æ‰æœƒç´¯ç©æ¨£æœ¬
        æª¢æŸ¥ Working Context ä¸­æ˜¯å¦æœ‰ declared_identity ä¾†åˆ¤æ–·
        """
        try:
            if not self.working_context_manager:
                debug_log(3, "[STT] Working Context ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè·³éæ¨£æœ¬ç´¯ç©")
                return
            
            # ğŸ†• æª¢æŸ¥æ˜¯å¦æœ‰å·²è²æ˜çš„ Identity
            # åªæœ‰åœ¨ä½¿ç”¨è€…å·²æ˜ç¢ºæŒ‡å®šèº«åˆ†æ™‚æ‰ç´¯ç©æ¨£æœ¬
            has_declared_identity = self._check_has_declared_identity()
            if not has_declared_identity:
                debug_log(3, "[STT] ç„¡å·²è²æ˜çš„ Identityï¼Œè·³é Speaker æ¨£æœ¬ç´¯ç©")
                return
            
            debug_log(3, "[STT] æª¢æ¸¬åˆ°å·²è²æ˜çš„ Identityï¼Œé–‹å§‹ç´¯ç© Speaker æ¨£æœ¬")
            
            from core.working_context import ContextType
            import time
            
            # æŸ¥æ‰¾æˆ–å‰µå»º SPEAKER_ACCUMULATION ä¸Šä¸‹æ–‡
            contexts = self.working_context_manager.get_contexts_by_type("speaker_accumulation")
            
            context_id = None
            if contexts:
                # ä½¿ç”¨æœ€æ–°çš„ä¸Šä¸‹æ–‡
                latest_context = max(contexts, key=lambda c: c.get('created_at', 0))
                context_id = latest_context['id']
                debug_log(3, f"[STT] ä½¿ç”¨ç¾æœ‰ Speaker_Accumulation ä¸Šä¸‹æ–‡: {context_id}")
            else:
                # å‰µå»ºæ–°çš„ä¸Šä¸‹æ–‡
                context_id = self.working_context_manager.create_context(
                    ContextType.SPEAKER_ACCUMULATION,
                    threshold=15,  # æ¨£æœ¬é–¾å€¼
                    timeout=300.0  # 5åˆ†é˜éæœŸ
                )
                debug_log(2, f"[STT] å‰µå»ºæ–°çš„ Speaker_Accumulation ä¸Šä¸‹æ–‡: {context_id}")
            
            if context_id:
                # æ·»åŠ éŸ³é »æ¨£æœ¬åˆ°ä¸Šä¸‹æ–‡
                # å®‰å…¨åœ°ç²å–èªªè©±äººè³‡è¨Š
                if speaker_info:
                    speaker_id = speaker_info.speaker_id
                    confidence = speaker_info.confidence
                else:
                    speaker_id = "unknown"
                    confidence = 0.0
                
                sample_metadata = {
                    "timestamp": time.time(),
                    "type": "audio_sample",
                    "speaker_id": speaker_id,
                    "confidence": confidence,
                    "audio_length": len(audio_data)
                }
                
                self.working_context_manager.add_data_to_context(
                    context_id,
                    audio_data,
                    metadata=sample_metadata
                )
                
                debug_log(3, f"[STT] éŸ³é »æ¨£æœ¬å·²æ·»åŠ åˆ°ç´¯ç©ä¸Šä¸‹æ–‡: {context_id} "
                         f"(èªªè©±äºº: {sample_metadata['speaker_id']}, "
                         f"é•·åº¦: {sample_metadata['audio_length']})")
            
        except Exception as e:
            error_log(f"[STT] æ·»åŠ éŸ³é »æ¨£æœ¬åˆ°ç´¯ç©ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
    
    def _check_has_declared_identity(self) -> bool:
        """æª¢æŸ¥ Working Context ä¸­æ˜¯å¦æœ‰å·²è²æ˜çš„ Identity
        
        Returns:
            bool: æ˜¯å¦æœ‰å·²è²æ˜çš„ Identity
        """
        try:
            if not self.working_context_manager:
                return False
            
            # æª¢æŸ¥å…¨å±€ä¸Šä¸‹æ–‡æ•¸æ“šä¸­çš„ declared_identity æ¨™è¨˜
            global_data = self.working_context_manager.global_context_data
            
            # æ–¹æ³•1: æª¢æŸ¥ declared_identity æ¨™è¨˜
            if global_data.get('declared_identity'):
                debug_log(3, "[STT] æª¢æ¸¬åˆ° declared_identity æ¨™è¨˜")
                return True
            
            # æ–¹æ³•2: æª¢æŸ¥ current_identity_id
            current_identity_id = global_data.get('current_identity_id')
            if current_identity_id and current_identity_id != 'unknown':
                debug_log(3, f"[STT] æª¢æ¸¬åˆ° current_identity_id: {current_identity_id}")
                return True
            
            # æ–¹æ³•3: æª¢æŸ¥ StatusManager çš„ç•¶å‰ Identity
            try:
                from core.status_manager import status_manager
                if status_manager.current_identity_id and status_manager.current_identity_id != 'unknown':
                    debug_log(3, f"[STT] StatusManager æœ‰ç•¶å‰ Identity: {status_manager.current_identity_id}")
                    return True
            except Exception:
                pass
            
            return False
            
        except Exception as e:
            error_log(f"[STT] æª¢æŸ¥ declared_identity å¤±æ•—: {e}")
            return False
