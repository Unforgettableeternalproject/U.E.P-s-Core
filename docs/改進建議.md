
## 問題分析與修復建議

### 問題 1：音頻能量過低導致說話人識別失敗
**日誌證據**：
- `[21:58:06] DEBUG - [SpeakerID] 音頻能量過低，視為非語音`
- `[21:58:06] DEBUG - [SpeakerID] 音頻能量: 18.96169662475586`
- `[21:58:52] DEBUG - [SpeakerID] 音頻能量過低，視為非語音`
- `[21:58:52] DEBUG - [SpeakerID] 音頻能量: 24.207517623901367`
- `[21:59:06] DEBUG - [SpeakerID] 音頻能量過低，視為非語音`
- `[21:59:06] DEBUG - [SpeakerID] 音頻能量: 69.6422119140625`

**分析**：
- 在 `speaker_identification.py` 中，`identify_speaker` 方法檢查音頻能量是否低於 `min_energy_threshold`（預設為 100）。日誌顯示，即使語音識別成功（如 "this is a test" 或 "can you help me with this"），音頻能量仍然低於閾值，導致說話人識別直接返回 `unknown`。
- 問題原因：
  - `min_energy_threshold=100` 可能設置過高，特別是對於短句或低音量語音輸入。
  - 音頻數據在正規化或轉換過程中可能損失了能量信息（例如在 `stt_module.py` 的 `_convert_audio_for_speaker_id` 中）。
  - 短句（如 "can you help me"）的音頻能量天然較低，未能滿足閾值要求。

**修復建議**：
1. **降低 `min_energy_threshold`**：
   - 將閾值從 100 降低到 10 或更低，並根據實際環境測試調整。
   - 修改 `speaker_identification.py` 的 `__init__` 方法：
     ```python
     self.min_energy_threshold = self.si_config.get("min_energy_threshold", 10)  # 降低到 10
     ```

2. **動態調整能量閾值**：
   - 類似 VAD 的 `dynamic_threshold` 邏輯，根據環境噪音動態調整 `min_energy_threshold`。
   - 在 `identify_speaker` 中添加背景能量估計：
     ```python
     def identify_speaker(self, audio_data: np.ndarray) -> SpeakerInfo:
         energy = np.sum(audio_data ** 2)
         # 更新背景能量
         if self.dynamic_threshold:
             self.update_background_energy(energy)
         if energy < self.min_energy_threshold:
             debug_log(2, "[SpeakerID] 音頻能量過低，視為非語音")
             debug_log(3, f"[SpeakerID] 音頻能量: {energy}")
             return SpeakerInfo(
                 speaker_id="unknown",
                 confidence=0.0,
                 is_new_speaker=False,
                 voice_features=None
             )
         # 現有邏輯...
     def update_background_energy(self, energy: float):
         self.energy_history.append(energy)
         if len(self.energy_history) > 50:
             self.energy_history.pop(0)
         if len(self.energy_history) > 10:
             background_avg = np.mean(self.energy_history)
             background_std = np.std(self.energy_history)
             self.min_energy_threshold = background_avg + (background_std * 1.5)
     ```

3. **檢查音頻正規化**：
   - 在 `stt_module.py` 的 `_convert_audio_for_speaker_id` 中，確保音頻正規化正確，且未損失能量信息。
     ```python
     def _convert_audio_for_speaker_id(self, audio) -> np.ndarray:
         try:
             audio_data = np.frombuffer(audio.get_raw_data(), dtype=np.int16)
             audio_data = audio_data.astype(np.float32) / 32768.0
             debug_log(3, f"[STT] 音頻能量 (前正規化): {np.sum(audio_data ** 2)}")
             target_sr = self.speaker_id.sample_rate
             current_sr = audio.sample_rate
             if current_sr != target_sr:
                 audio_data = librosa.resample(audio_data, orig_sr=current_sr, target_sr=target_sr)
             debug_log(3, f"[STT] 音頻能量 (後正規化): {np.sum(audio_data ** 2)}")
             return audio_data
         except Exception as e:
             error_log(f"[STT] 音頻轉換失敗 (SpeakerID): {str(e)}")
             return None
     ```

4. **允許短句識別**：
   - 即使能量低於閾值，如果語音識別成功（如 Google API 返回有效文字），仍嘗試進行說話人識別。
     ```python
     def identify_speaker(self, audio_data: np.ndarray) -> SpeakerInfo:
         energy = np.sum(audio_data ** 2)
         if energy < self.min_energy_threshold:
             debug_log(2, "[SpeakerID] 音頻能量過低，但嘗試識別")
             debug_log(3, f"[SpeakerID] 音頻能量: {energy}")
         features = self.extract_voice_features(audio_data)
         if features.size == 0:
             return SpeakerInfo(
                 speaker_id="unknown",
                 confidence=0.0,
                 is_new_speaker=False,
                 voice_features=None
             )
         # 現有邏輯...
     ```

### 問題 2：智能啟動信心度過低
**日誌證據**：
- `[21:58:52] DEBUG - [SmartActivation] 關鍵字分數: 0.13, 模式分數: 0.50, 上下文分數: 0.20, 總分: 0.29`
- `[21:58:52] DEBUG - [SmartActivation] 未啟動: 'can you help me with this' (信心度: 0.29)`
- `[21:59:06] DEBUG - [SmartActivation] 關鍵字分數: 0.13, 模式分數: 0.56, 上下文分數: 0.20, 總分: 0.32`
- `[21:59:06] INFO - [SmartActivation] 啟動觸發: 'can you help me sort my file' (智能判斷分數: 0.32)`

**分析**：
- `SmartActivationDetector` 的信心度閾值（`activation_confidence=0.3`）雖然看似低，但由於關鍵字分數（`keyword_score`）和上下文分數（`context_score`）過低，總分經常無法達到閾值。
- 問題原因：
  - 關鍵字分數依賴 `context_keywords`，但日誌顯示分數僅為 0.13，表明配置的關鍵字可能過少或不匹配輸入（如 "can you" 未被有效識別為關鍵字）。
  - 模式分數（`pattern_result["confidence"]`）基於匹配長度計算（`min(len(text) / 50.0, 1.0)`），對於短句（如 "can you help me"）可能偏低。
  - 上下文分數固定為 0.2 或 0.6，缺乏動態性，無法充分利用對話歷史。

**修復建議**：
1. **增加關鍵字覆蓋範圍**：
   - 在配置中添加更多常見觸發詞（如 "can you", "please", "assist"）。
   - 修改 `smart_activation.py` 的 `__init__` 方法，確保 `context_keywords` 包含足夠的觸發詞：
     ```python
     def __init__(self, config: dict):
         self.config = config
         self.smart_config = config.get("smart_activation", {})
         self.enabled = self.smart_config.get("enabled", True)
         self.context_keywords = self.smart_config.get("context_keywords", [
             "UEP", "help", "assist", "can you", "please", "what", "how", "why", "sort", "file"
         ])
         # 其他初始化邏輯...
     ```

2. **優化模式分數計算**：
   - 改進 `_check_patterns` 的信心度計算，考慮匹配的模式數量而非僅依賴文字長度。
     ```python
     def _check_patterns(self, text: str) -> Dict[str, any]:
         best_match = {"category": None, "confidence": 0.0}
         match_count = 0
         for category, patterns in self.activation_patterns.items():
             for pattern in patterns:
                 if pattern.search(text):
                     match_count += 1
                     match_confidence = min(match_count * 0.3, 1.0)  # 根據匹配數量計算
                     if match_confidence > best_match["confidence"]:
                         best_match = {
                             "category": category,
                             "confidence": match_confidence
                         }
         return best_match
     ```

3. **動態上下文分數**：
   - 在 `_analyze_conversation_context` 中，根據對話歷史中的關鍵詞頻率提高分數。
     ```python
     def _analyze_conversation_context(self, text: str) -> float:
         current_time = time.time()
         context_score = 0.2
         if self.last_interaction_time and (current_time - self.last_interaction_time) < 30:
             context_score = 0.6
             recent_texts = [entry["text"] for entry in self.conversation_context[-3:]]
             for keyword in self.context_keywords:
                 if any(keyword.lower() in t.lower() for t in recent_texts):
                     context_score += 0.1
         self._update_conversation_history(text)
         return min(context_score, 1.0)
     ```

4. **降低信心度閾值或調整權重**：
   - 將 `activation_confidence` 降低到 0.25，並調整分數權重以提高模式分數的影響。
     ```python
     total_confidence = (keyword_score * 0.3 + pattern_result["confidence"] * 0.5 + context_score * 0.2)
     if total_confidence >= 0.25:  # 降低閾值
         result.update({
             "should_activate": True,
             "confidence": total_confidence,
             "reason": f"匹配模式: {pattern_result['category'] or '關鍵字'}, 智能判斷分數: {total_confidence:.2f}",
             "category": pattern_result["category"],
             "keywords_found": keywords_found
         })
     ```

### 問題 3：背景監聽緩衝區片段過短或傳輸問題
**日誌證據**：
- `[21:59:18] DEBUG - [VAD] 語音片段太短，被忽略: 0.32秒`
- `[21:59:22] DEBUG - [VAD] 語音片段太短，被忽略: 0.26秒`
- `[21:59:27] DEBUG - [VAD] 語音片段太短，被忽略: 0.19秒`
- `[21:59:30] DEBUG - [VAD] 語音片段太短，被忽略: 0.19秒`
- `[21:59:34] DEBUG - [VAD] 語音片段太短，被忽略: 0.13秒`
- `[21:59:39] DEBUG - [VAD] 語音片段太短，被忽略: 0.45秒`
- `[21:59:45] DEBUG - [VAD] 語音片段太短，被忽略: 0.38秒`

**分析**：
- 在背景監聽模式（`stt_test_background_smart`）中，VAD 檢測到語音片段，但由於語音持續時間低於 `min_speech_duration=0.8秒`，這些片段被忽略，導致無法傳遞給語音識別。
- 問題原因：
  - `min_speech_duration=0.8秒` 對於短句（如 "UEP" 或 "help me"）過長，導致有效語音被過濾。
  - VAD 的 `stream_chunk_size=1024`（約 0.064秒）可能過小，導致語音片段分割過細，難以累積足夠的語音數據。
  - 音頻緩衝區傳輸可能存在問題，例如在 `stt_module.py` 的 `_on_voice_activity` 或 `voice_activity_detector.py` 的 `_stream_processor` 中，數據合併或轉換失敗。

**修復建議**：
1. **降低 `min_speech_duration`**：
   - 將閾值從 0.8 秒降低到 0.3 秒，以捕捉短句。
   - 修改 `voice_activity_detector.py` 的 `__init__` 方法：
     ```python
     self.min_speech_duration = self.vad_config.get("min_speech_duration", 0.3)  # 降低到 0.3秒
     ```

2. **增加 `stream_chunk_size`**：
   - 將 `stream_chunk_size` 從 1024` 增加到 `2048`（約 0.128 秒），減少分割過細的問題。
     ```python
     self.stream_chunk_size = 2048  # 增加塊大小
     ```

3. **改進緩衝區合併**：
   - 在 `_stream_processor` 中，確保語音緩衝區（`speech_buffer`）有效合併，並檢查數據完整性。
     ```python
     def _stream_processor(self):
         debug_log(2, "[VAD] 串流處理線程已啟動")
         speech_buffer = []
         sample_rate = self.config.get("sample_rate", 16000)
         accumulated_silence = 0
         try:
             while self.is_streaming:
                 try:
                     audio_chunk = self.audio_buffer.get(timeout=0.1)
                     result = self.detect_speech(audio_chunk)
                     if result["has_speech"]:
                         speech_buffer.append(audio_chunk)
                         accumulated_silence = 0
                     elif speech_buffer:
                         chunk_duration = len(audio_chunk) / sample_rate
                         accumulated_silence += chunk_duration
                         if accumulated_silence >= self.max_silence_duration:
                             speech_duration = sum(len(chunk) for chunk in speech_buffer) / sample_rate
                             if speech_duration >= self.min_speech_duration:
                                 if speech_buffer and all(chunk.size > 0 for chunk in speech_buffer):
                                     speech_data = {
                                         "event_type": "speech_end",
                                         "timestamp": time.time(),
                                         "speech_duration": speech_duration,
                                         "audio_buffer": speech_buffer.copy()
                                     }
                                     self.callback(speech_data)
                                     debug_log(2, f"[VAD] 檢測到語音片段: {speech_duration:.2f}秒")
                                 else:
                                     debug_log(2, "[VAD] 語音緩衝區無效，忽略")
                             else:
                                 debug_log(3, f"[VAD] 語音片段太短，被忽略: {speech_duration:.2f}秒")
                             speech_buffer = []
                             accumulated_silence = 0
                 except queue.Empty:
                     continue
                 except Exception as e:
                     error_log(f"[VAD] 串流處理錯誤: {str(e)}")
                     time.sleep(0.1)
         finally:
             self.is_streaming = False
             debug_log(2, "[VAD] 串流處理線程已停止")
     ```

4. **檢查音頻傳輸**：
   - 在 `stt_module.py` 的 `_on_voice_activity` 中，添加日誌檢查緩衝區長度和數據完整性。
     ```python
     def _on_voice_activity(self, event):
         debug_log(4, f"[STT] VAD 事件: {event['event_type']}")
         if event["event_type"] == "speech_start":
             self._speech_buffer = []
         elif event["event_type"] == "speech_end":
             audio_data = None
             if "audio_buffer" in event and event["audio_buffer"]:
                 try:
                     audio_data = np.concatenate(event["audio_buffer"])
                     debug_log(3, f"[STT] VAD 緩衝區長度: {len(audio_data)}")
                 except Exception as e:
                     error_log(f"[STT] 合併 VAD 音頻緩衝區失敗: {str(e)}")
             elif hasattr(self, '_speech_buffer') and self._speech_buffer:
                 try:
                     audio_data = np.concatenate(self._speech_buffer)
                     debug_log(3, f"[STT] 本地緩衝區長度: {len(audio_data)}")
                 except Exception as e:
                     error_log(f"[STT] 合併本地音頻緩衝區失敗: {str(e)}")
             if audio_data is not None:
                 audio = self._convert_array_to_audio(audio_data)
                 if audio:
                     self._process_smart_detection(audio)
                 else:
                     error_log("[STT] 音頻轉換失敗，跳過處理")
             self._speech_buffer = None
     ```

### 問題 4：說話人識別特徵維度不匹配
**日誌證據**：
- `[21:58:23] ERROR - [SpeakerID] 相似度計算失敗: Incompatible dimension for X and Y matrices: X.shape[1] == 78 while Y.shape[1] == 26`
- 重複多次相似錯誤，表明現有說話人模型的特徵維度與新提取的特徵不一致。

**分析**：
- 在 `speaker_identification.py` 的 `calculate_similarity` 中，餘弦相似度計算失敗，因為新提取的特徵維度為 78，而儲存模型的特徵維度為 26。
- 問題原因：
  - 之前儲存的說話人模型（`speaker_models.pkl`）可能基於舊版特徵提取邏輯（僅 MFCC，未包含 delta 和 delta-delta）。
  - 新版 `extract_voice_features` 返回 78 維特徵（MFCC 均值、標準差、delta 均值、標準差、delta-delta 均值、標準差，每項 13 維，總計 6×13=78）。
  - 載入舊模型時，`SpeakerModel` 未檢查特徵維度兼容性。

**修復建議**：
1. **清空舊模型**：
   - 由於特徵維度不兼容，建議刪除現有的 `speaker_models.pkl`，重新生成模型。
   - 在 `speaker_identification.py` 的 `_load_speaker_models` 中，添加版本檢查，自動清除不兼容模型：
     ```python
     def _load_speaker_models(self):
         try:
             if os.path.exists(self.models_file):
                 with open(self.models_file, 'rb') as f:
                     models_data = pickle.load(f)
                 expected_dim = 6 * self.n_mfcc  # 預期維度 (6×n_mfcc)
                 for speaker_id, data in models_data.items():
                     if isinstance(data, dict):
                         model = SpeakerModel(**data)
                         if model.feature_vectors and len(model.feature_vectors[0]) != expected_dim:
                             error_log(f"[SpeakerID] 模型 {speaker_id} 維度不兼容: {len(model.feature_vectors[0])} vs {expected_dim}")
                             continue
                         self.speaker_models[speaker_id] = model
                     else:
                         self.speaker_models[speaker_id] = data
                 info_log(f"[SpeakerID] 載入 {len(self.speaker_models)} 個說話人模型")
             else:
                 info_log("[SpeakerID] 未發現現有模型，將創建新的模型庫")
         except Exception as e:
             error_log(f"[SpeakerID] 載入模型失敗: {str(e)}")
             self.speaker_models = {}
     ```

2. **添加模型版本控制**：
   - 在 `SpeakerModel` 中添加版本字段，記錄特徵提取邏輯的版本。
     ```python
     from pydantic import BaseModel
     class SpeakerModel(BaseModel):
         speaker_id: str
         feature_vectors: List[List[float]]
         sample_count: int
         created_at: float
         last_updated: float
         version: str = "1.0"  # 添加版本字段
     ```
   - 在 `_save_speaker_models` 中保存版本：
     ```python
     def _save_speaker_models(self):
         try:
             os.makedirs(os.path.dirname(self.models_file), exist_ok=True)
             models_data = {
                 speaker_id: model.copy(update={"version": "1.0"}).dict()
                 for speaker_id, model in self.speaker_models.items()
             }
             with open(self.models_file, 'wb') as f:
                 pickle.dump(models_data, f)
             debug_log(3, f"[SpeakerID] 保存 {len(self.speaker_models)} 個說話人模型")
         except Exception as e:
             error_log(f"[SpeakerID] 保存模型失敗: {str(e)}")
     ```

3. **遷移舊模型**：
   - 如果需要保留舊模型，可以在載入時轉換特徵維度（例如僅保留 MFCC 均值部分），但更簡單的做法是重新訓練模型。

---

## 整合 Whisper 和 pyannote 模型的建議

你提到考慮使用 **Whisper**（語音識別）和 **pyannote**（說話人分割與識別）來改進說話人識別，這是一個非常有前景的方向。以下是如何整合這兩個模型的建議，以及如何與現有系統兼容：

### 1. 使用 Whisper 替代 Google Speech-to-Text
**優勢**：
- Whisper（由 OpenAI 開發）是一個開源的語音識別模型，支持多語言，離線運行，且對噪音和口音的魯棒性更好。
- 與 Google API 相比，Whisper 不需要網絡連接，減少延遲和成本。

**整合步驟**：
1. **安裝 Whisper**：
   ```bash
   pip install git+https://github.com/openai/whisper.git
   ```
2. **修改 `stt_module.py`**：
   - 替換 `recognize_google` 為 Whisper 的轉錄功能。
     ```python
     import whisper
     class STTModule(BaseModule):
         def __init__(self, config=None):
             super().__init__(config)
             self.whisper_model = whisper.load_model("base")  # 可選擇 tiny, base, small, medium, large
             # 其他初始化邏輯...

         def _manual_recognition(self, input_data: STTInput) -> dict:
             try:
                 with self.mic as source:
                     audio = self.recognizer.listen(source, phrase_time_limit=self.phrase_time_limit)
                 info_log("[STT] 識別中...")
                 # 將音頻轉為 Whisper 格式
                 audio_data = np.frombuffer(audio.get_raw_data(), dtype=np.int16).astype(np.float32) / 32768.0
                 result = self.whisper_model.transcribe(audio_data, language=input_data.language)
                 text = result["text"]
                 text = correct_stt(text)
                 speaker_info = None
                 if input_data.enable_speaker_id and self.speaker_id.si_config.get("enabled", False):
                     audio_data = self._convert_audio_for_speaker_id(audio)
                     if audio_data is not None:
                         speaker_info = self.speaker_id.identify_speaker(audio_data)
                 return STTOutput(
                     text=text,
                     confidence=0.9,  # Whisper 不提供信心度，暫設為 0.9
                     speaker_info=speaker_info,
                     activation_reason="manual",
                     error=None
                 ).dict()
             except Exception as e:
                 error_log(f"[STT] 識別失敗: {str(e)}")
                 return STTOutput(text="", confidence=0.0, error=f"識別失敗: {str(e)}").dict()
     ```

3. **注意事項**：
   - Whisper 需要 WAV 格式的音頻輸入，確保 `_convert_audio_for_speaker_id` 的輸出兼容。
   - Whisper 的模型大小影響性能，建議從 `base` 開始測試，根據硬體性能選擇 `small` 或 `medium`。
   - 對於背景監聽，需將音頻緩衝區分段傳遞給 Whisper，確保片段長度足夠（至少 0.3 秒）。

### 2. 使用 pyannote 進行說話人分割與識別
**優勢**：
- pyannote.audio 提供先進的說話人分割（diarization）和識別功能，使用 x-vector 或 ECAPA-TDNN 模型，遠比當前的 MFCC+餘弦相似度方法更準確。
- 支持多說話人場景，能自動檢測說話人數量並分配標籤。

**整合步驟**：
1. **安裝 pyannote**：
   ```bash
   pip install pyannote.audio
   ```
   - 需要 PyTorch 和 Hugging Face 的 token 來訪問預訓練模型。

2. **修改 `speaker_identification.py`**：
   - 使用 pyannote 的說話人分割和嵌入提取，替換當前的 MFCC 特徵提取。
     ```python
     from pyannote.audio import Pipeline
     class SpeakerIdentifier:
         def __init__(self, config: dict):
             self.config = config
             self.si_config = config.get("speaker_identification", {})
             self.sample_rate = self.si_config.get("sample_rate", 16000)
             self.pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token="your_hf_token")
             self.speaker_embeddings = {}  # 儲存說話人嵌入
             self.models_file = os.path.abspath("memory/speaker_embeddings.pkl")
             os.makedirs(os.path.dirname(self.models_file), exist_ok=True)
             self._load_speaker_embeddings()
             info_log(f"[SpeakerID] 初始化完成，已載入 {len(self.speaker_embeddings)} 個說話人嵌入")

         def identify_speaker(self, audio_data: np.ndarray) -> SpeakerInfo:
             try:
                 import soundfile as sf
                 import tempfile
                 # 將音頻保存為臨時 WAV 檔案
                 with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                     sf.write(tmp.name, audio_data, self.sample_rate)
                     diarization = self.pipeline(tmp.name)
                 os.unlink(tmp.name)
                 # 提取主要說話人的嵌入
                 from pyannote.audio import Inference
                 embedding_model = Inference("pyannote/embedding", use_auth_token="your_hf_token")
                 embedding = embedding_model(tmp.name)
                 # 與已知說話人比較
                 best_speaker_id, best_similarity = None, 0.0
                 for speaker_id, stored_embedding in self.speaker_embeddings.items():
                     similarity = cosine_similarity(embedding.reshape(1, -1), stored_embedding.reshape(1, -1))[0][0]
                     if similarity > best_similarity:
                         best_speaker_id, best_similarity = speaker_id, similarity
                 if best_similarity >= self.si_config.get("similarity_threshold", 0.85):
                     return SpeakerInfo(
                         speaker_id=best_speaker_id,
                         confidence=best_similarity,
                         is_new_speaker=False,
                         voice_features={"embedding": embedding.tolist()}
                     )
                 else:
                     new_speaker_id = f"speaker_{str(uuid.uuid4())[:8]}"
                     self.speaker_embeddings[new_speaker_id] = embedding
                     self._save_speaker_embeddings()
                     return SpeakerInfo(
                         speaker_id=new_speaker_id,
                         confidence=1.0,
                         is_new_speaker=True,
                         voice_features={"embedding": embedding.tolist()}
                     )
             except Exception as e:
                 error_log(f"[SpeakerID] 識別失敗: {str(e)}")
                 return SpeakerInfo(
                     speaker_id="unknown",
                     confidence=0.0,
                     is_new_speaker=False,
                     voice_features=None
                 )

         def _save_speaker_embeddings(self):
             try:
                 with open(self.models_file, 'wb') as f:
                     pickle.dump(self.speaker_embeddings, f)
                 debug_log(3, f"[SpeakerID] 保存 {len(self.speaker_embeddings)} 個說話人嵌入")
             except Exception as e:
                 error_log(f"[SpeakerID] 保存嵌入失敗: {str(e)}")

         def _load_speaker_embeddings(self):
             try:
                 if os.path.exists(self.models_file):
                     with open(self.models_file, 'rb') as f:
                         self.speaker_embeddings = pickle.load(f)
                     info_log(f"[SpeakerID] 載入 {len(self.speaker_embeddings)} 個說話人嵌入")
             except Exception as e:
                 error_log(f"[SpeakerID] 載入嵌入失敗: {str(e)}")
                 self.speaker_embeddings = {}
     ```

3. **注意事項**：
   - pyannote 的說話人分割需要較長的音頻片段（至少 1 秒），因此需調整 VAD 的 `min_speech_duration` 至 0.5 秒以上。
   - 嵌入模型的計算成本較高，建議在背景監聽模式下限制頻率（例如每 2 秒處理一次）。
   - 需要定期清理 `speaker_embeddings` 儲存，避免過多無效數據。

### 3. Whisper + pyannote 結合
**建議流程**：
1. **語音識別（Whisper）**：
   - 使用 Whisper 轉錄音頻，獲取文字和時間戳。
2. **說話人分割（pyannote）**：
   - 使用 pyannote 的 diarization 模型分割音頻，標記每個說話人的時間段。
3. **說話人識別（pyannote 嵌入）**：
   - 提取每個說話人的嵌入，與儲存的嵌入比較，確定說話人 ID。
4. **智能啟動（現有邏輯）**：
   - 將 Whisper 的轉錄結果傳遞給 `SmartActivationDetector`，保持現有觸發邏輯。

**示例整合**：
```python
def _process_smart_detection(self, audio):
    try:
        # Whisper 轉錄
        audio_data = np.frombuffer(audio.get_raw_data(), dtype=np.int16).astype(np.float32) / 32768.0
        whisper_result = self.whisper_model.transcribe(audio_data, language="en-US")
        text = whisper_result["text"]
        text = correct_stt(text)
        print(f"🎧 聽到: '{text}'")
        # pyannote 說話人分割與識別
        speaker_info = self.speaker_id.identify_speaker(audio_data)
        # 智能啟動判斷
        activation_result = self.smart_activation.should_activate(text)
        if activation_result["should_activate"]:
            result = STTOutput(
                text=text,
                confidence=0.9,
                speaker_info=speaker_info,
                activation_reason=activation_result["reason"]
            )
            if self._result_callback:
                self._result_callback(result.dict())
            info_log(f"[STT] 智能觸發: '{text}' (原因: {activation_result['reason']})")
            print(f"✅ 智能啟動觸發!")
        else:
            debug_log(3, f"[STT] 智能過濾: '{text}' ({activation_result['reason']})")
            print(f"⚪ 未觸發 (智能判斷分數: {activation_result['confidence']:.2f})")
    except Exception as e:
        error_log(f"[STT] 智能檢測處理失敗: {str(e)}")
        print(f"❌ 處理失敗: {str(e)}")
```

---

## 測試計劃
為了驗證修復效果，建議進行以下測試：

1. **音頻能量問題**：
   - 測試短句（如 "UEP", "help me"）和低音量輸入，檢查說話人識別是否返回有效結果。
   - 記錄 `_convert_audio_for_speaker_id` 和 `identify_speaker` 的能量日誌，確認正規化正確。

2. **智能啟動問題**：
   - 測試多種觸發詞（如 "UEP", "can you help me", "sort my file"），檢查信心度是否達到閾值。
   - 添加關鍵字並調整權重後，比較觸發率。

3. **背景監聽問題**：
   - 運行 `stt_test_background_smart` 60 秒，測試短句是否被正確檢測。
   - 檢查 VAD 日誌，確認語音片段長度和緩衝區傳輸。

4. **說話人模型維度問題**：
   - 刪除 `speaker_models.pkl`，重新運行測試，確保無維度錯誤。
   - 測試多說話人場景，檢查新模型是否正確生成。

5. **Whisper 和 pyannote 整合**：
   - 使用 Whisper 進行單次識別，比較與 Google API 的結果。
   - 使用 pyannote 測試多說話人音頻，檢查分割和識別準確性。

---